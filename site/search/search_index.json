{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs why its lie 1 2 3 4 5 while True : print ( \"hello world\" ) this is example of that For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"","title":"Welcome to MkDocs"},{"location":"#why-its-lie","text":"1 2 3 4 5 while True : print ( \"hello world\" ) this is example of that For full documentation visit mkdocs.org .","title":"why its lie"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"#welcome-to-mkdocs_1","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands_1","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout_1","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"#welcome-to-mkdocs_2","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands_2","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout_2","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"gitlab/install/","text":"GitLab Server Installation and Configuration Follow these steps to install and configure a GitLab server: Install Debian server. Install Docker CE: 1 2 apt install docker.io systemctl start docker Install Portainer CE. Ports 9000 is for HTTP and 9443 is for HTTPS: 1 docker run -d -p 8000:8000 -p 9000:9000 -p 9443:9443 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:latest Open ports: 1 2 iptables -t nat -A PREROUTING -i vmbr0 -p tcp --dport 9980 -j DNAT --to 192.168.1.7:9000 iptables -t nat -A PREROUTING -i vmbr0 -p tcp --dport 9981 -j DNAT --to 192.168.1.7:9443 Install GitLab CE in Docker with Portainer. Create a docker-compose.yml file with the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 version : '3.8' services : gitlab : image : 'gitlab/gitlab-ce:latest' restart : 'unless-stopped' hostname : 'gitlab.gitlab' environment : GITLAB_OMNIBUS_CONFIG : | external_url 'https://gitlab.gtdi.ge' gitlab_rails['gitlab_ssh_host'] = 'gtdi.ge' gitlab_rails['gitlab_shell_ssh_port'] = 9982 gitlab_rails['gitlab_port'] = 9983 nginx['listen_port'] = 9983 nginx['listen_https'] = false gitlab_rails['registry_enabled'] = true ports : - '9983:9983' - '9982:22' volumes : - 'gitlab_config:/etc/gitlab' - 'gitlab_logs:/var/log/gitlab' - 'gitlab_data:/var/opt/gitlab' shm_size : '1gb' networks : default : aliases : - 'gitlab.gitlab' gitlab-runner : image : 'gitlab/gitlab-runner:latest' restart : 'unless-stopped' container_name : 'gitlab-runner' volumes : - 'gitlab_runner_config:/etc/gitlab-runner' - '/var/run/docker.sock:/var/run/docker.sock' extra_hosts : - \"gitlab.gtdi.ge:192.168.1.5\" networks : - 'default' networks : default : driver : 'bridge' volumes : gitlab_config : gitlab_logs : gitlab_data : Gitlab_runner_config : Replace external_url with your Git repository clone HTTPS address, and gitlab_ssh_host and gitlab_shell_ssh_port with your Clone with SSH address. Make sure the IP in extra_hosts for gitlab_runner matches the GitLab server's IP since they are on the same server. Open ports from the outside: 1 iptables -t nat -A PREROUTING -i vmbr0 -p tcp --dport 9982 -j DNAT --to 192.168.1.7:9982 Create an Nginx configuration file, gitlab.conf , with the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 server { listen 80; listen [::]:80; server_name gitlab.gtdi.ge; server_name www.gitlab.gtdi.ge; location / { return 301 https://$server_name$request_uri; } } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name gitlab.gtdi.ge www.gitlab.gtdi.ge; ssl_certificate /etc/letsencrypt/live/www.gitlab.gtdi.ge/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/www.gitlab.gtdi.ge/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; location / { proxy_pass http://192.168.1.7:9983; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Proto https; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_redirect off; } } Note: Let's Encrypt does not work on non-standard ports for GitLab server. Generate the certificate: 1 certbot --nginx -d www.gitlab.gtdi.ge -d gitlab.gtdi.ge Create a symlink: 1 ln -sf /etc/nginx/sites-available/gitlab.conf /etc/nginx/sites-enabled/gitlab Restart Nginx: 1 systemctl restart nginx In GitLab, create a group, user, and repository. Go to the repository settings -> CI/CD -> Runners -> Expand -> Copy the registration token, which is required to register the runner. In Portainer, go to the runner terminal and register the runner: 1 gitlab-runner register --non-interactive --executor \"docker\" --docker-image docker:20.10.24-git --url \"https://gitlab.gtdi.ge/\" --registration-token \"TOKEN\" --description \"local-runner\" --docker-network-mode gitlab-ce_default --docker-privileged 1 Ensure that the `docker-network-mode` value is the same as the network used in the `docker-compose.yml` file. Here is a sample .gitlab-ci.yml pipeline configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 image : docker:20.10.24-git services : - name : docker:20.10.24-dind alias : docker stages : - build - test variables : APP_NAME : my-app DOCKER_HOST : tcp://docker:2375 DOCKER_DRIVER : overlay2 DOCKER_TLS_CERTDIR : \"\" DOCKER_IMAGE_TAG : latest DOCKER_REGISTRY_URL : gitlab.gtdi.ge DOCKER_REGISTRY_USERNAME : root DOCKER_REGISTRY_PASSWORD : build : stage : build script : - echo $DOCKER_HOST - docker build -t $APP_NAME:$(git rev-parse --short HEAD) . test : stage : test script : - echo \"Running tests...\"","title":"Install"},{"location":"gitlab/install/#gitlab-server-installation-and-configuration","text":"Follow these steps to install and configure a GitLab server: Install Debian server. Install Docker CE: 1 2 apt install docker.io systemctl start docker Install Portainer CE. Ports 9000 is for HTTP and 9443 is for HTTPS: 1 docker run -d -p 8000:8000 -p 9000:9000 -p 9443:9443 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:latest Open ports: 1 2 iptables -t nat -A PREROUTING -i vmbr0 -p tcp --dport 9980 -j DNAT --to 192.168.1.7:9000 iptables -t nat -A PREROUTING -i vmbr0 -p tcp --dport 9981 -j DNAT --to 192.168.1.7:9443 Install GitLab CE in Docker with Portainer. Create a docker-compose.yml file with the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 version : '3.8' services : gitlab : image : 'gitlab/gitlab-ce:latest' restart : 'unless-stopped' hostname : 'gitlab.gitlab' environment : GITLAB_OMNIBUS_CONFIG : | external_url 'https://gitlab.gtdi.ge' gitlab_rails['gitlab_ssh_host'] = 'gtdi.ge' gitlab_rails['gitlab_shell_ssh_port'] = 9982 gitlab_rails['gitlab_port'] = 9983 nginx['listen_port'] = 9983 nginx['listen_https'] = false gitlab_rails['registry_enabled'] = true ports : - '9983:9983' - '9982:22' volumes : - 'gitlab_config:/etc/gitlab' - 'gitlab_logs:/var/log/gitlab' - 'gitlab_data:/var/opt/gitlab' shm_size : '1gb' networks : default : aliases : - 'gitlab.gitlab' gitlab-runner : image : 'gitlab/gitlab-runner:latest' restart : 'unless-stopped' container_name : 'gitlab-runner' volumes : - 'gitlab_runner_config:/etc/gitlab-runner' - '/var/run/docker.sock:/var/run/docker.sock' extra_hosts : - \"gitlab.gtdi.ge:192.168.1.5\" networks : - 'default' networks : default : driver : 'bridge' volumes : gitlab_config : gitlab_logs : gitlab_data : Gitlab_runner_config : Replace external_url with your Git repository clone HTTPS address, and gitlab_ssh_host and gitlab_shell_ssh_port with your Clone with SSH address. Make sure the IP in extra_hosts for gitlab_runner matches the GitLab server's IP since they are on the same server. Open ports from the outside: 1 iptables -t nat -A PREROUTING -i vmbr0 -p tcp --dport 9982 -j DNAT --to 192.168.1.7:9982 Create an Nginx configuration file, gitlab.conf , with the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 server { listen 80; listen [::]:80; server_name gitlab.gtdi.ge; server_name www.gitlab.gtdi.ge; location / { return 301 https://$server_name$request_uri; } } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name gitlab.gtdi.ge www.gitlab.gtdi.ge; ssl_certificate /etc/letsencrypt/live/www.gitlab.gtdi.ge/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/www.gitlab.gtdi.ge/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; location / { proxy_pass http://192.168.1.7:9983; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Proto https; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_redirect off; } } Note: Let's Encrypt does not work on non-standard ports for GitLab server. Generate the certificate: 1 certbot --nginx -d www.gitlab.gtdi.ge -d gitlab.gtdi.ge Create a symlink: 1 ln -sf /etc/nginx/sites-available/gitlab.conf /etc/nginx/sites-enabled/gitlab Restart Nginx: 1 systemctl restart nginx In GitLab, create a group, user, and repository. Go to the repository settings -> CI/CD -> Runners -> Expand -> Copy the registration token, which is required to register the runner. In Portainer, go to the runner terminal and register the runner: 1 gitlab-runner register --non-interactive --executor \"docker\" --docker-image docker:20.10.24-git --url \"https://gitlab.gtdi.ge/\" --registration-token \"TOKEN\" --description \"local-runner\" --docker-network-mode gitlab-ce_default --docker-privileged 1 Ensure that the `docker-network-mode` value is the same as the network used in the `docker-compose.yml` file. Here is a sample .gitlab-ci.yml pipeline configuration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 image : docker:20.10.24-git services : - name : docker:20.10.24-dind alias : docker stages : - build - test variables : APP_NAME : my-app DOCKER_HOST : tcp://docker:2375 DOCKER_DRIVER : overlay2 DOCKER_TLS_CERTDIR : \"\" DOCKER_IMAGE_TAG : latest DOCKER_REGISTRY_URL : gitlab.gtdi.ge DOCKER_REGISTRY_USERNAME : root DOCKER_REGISTRY_PASSWORD : build : stage : build script : - echo $DOCKER_HOST - docker build -t $APP_NAME:$(git rev-parse --short HEAD) . test : stage : test script : - echo \"Running tests...\"","title":"GitLab Server Installation and Configuration"},{"location":"kubernetes/architecture/","text":"Master Node The master node is responsible for managing, planning, scheduling, and monitoring nodes in the cluster. Kube-apiserver The kube-apiserver is responsible for orchestrating all actions in the cluster. It is what is behind the kubectl command. It uses HTTP POST requests and can be installed separately as a service at /etc/systemd/system/kube-apiserver.service , or it can be installed automatically as a pod at /etc/kubernetes/manifests/Kube-apiserver.yaml . The kube-apiserver does the following: Authenticates the user Validates requests Retrieves data Updates the ETCD cluster Assigns a node to the request using the scheduler Sends the assigned node to the kubelet Updates the ETCD cluster with the status of the kubelet ETCD Cluster The ETCD cluster is a key-value store that is installed on the master node. It stores information about the cluster, including nodes, pods, configs, secrets, accounts, roles, bindings, and other information. It can be configured for high availability by setting up multiple instances. It is a standalone store that is not tied to any specific service. Kube-scheduler The kube-scheduler is responsible for managing the scheduling of containers on nodes. It determines which pods should be run on which nodes. It can be run as a service and uses algorithms to prioritize nodes based on available resources (such as CPU). For example, it may do the following: Filter nodes based on available resources Rank nodes using a priority algorithm on a scale of 0-10 Controllers Controllers are responsible for monitoring the system and ensuring that desired state is maintained. They can be downloaded as a service and run on the master node. Node Controllers Node controllers are responsible for monitoring the status of nodes and ensuring that they are running. They check the status of nodes every 5 seconds, and if a node becomes unreachable, they wait 40 seconds before marking it as unreachable. Replication Controllers Replication controllers are responsible for ensuring that the desired number of pods are running. If there are not enough pods, they will create new ones to meet the desired count. Worker Nodes Worker nodes host applications as containers. Container Runtime Engine The container runtime engine is responsible for running and managing containers on the node. An example of a container runtime engine is Docker. Kubelet The kubelet is an agent that runs or creates pods on the node. It is responsible for registering the node with the cluster. Kube-proxy The kube-proxy can be run as a service and is installed on each node in the cluster. It creates iptables rules to facilitate communication between worker nodes. Pods A pod is the basic execution unit in Kubernetes and is where a container lives. It is recommended to have one container per pod, but helper containers can also be deployed with the main container.","title":"Architecture"},{"location":"kubernetes/architecture/#master-node","text":"The master node is responsible for managing, planning, scheduling, and monitoring nodes in the cluster.","title":"Master Node"},{"location":"kubernetes/architecture/#kube-apiserver","text":"The kube-apiserver is responsible for orchestrating all actions in the cluster. It is what is behind the kubectl command. It uses HTTP POST requests and can be installed separately as a service at /etc/systemd/system/kube-apiserver.service , or it can be installed automatically as a pod at /etc/kubernetes/manifests/Kube-apiserver.yaml . The kube-apiserver does the following: Authenticates the user Validates requests Retrieves data Updates the ETCD cluster Assigns a node to the request using the scheduler Sends the assigned node to the kubelet Updates the ETCD cluster with the status of the kubelet","title":"Kube-apiserver"},{"location":"kubernetes/architecture/#etcd-cluster","text":"The ETCD cluster is a key-value store that is installed on the master node. It stores information about the cluster, including nodes, pods, configs, secrets, accounts, roles, bindings, and other information. It can be configured for high availability by setting up multiple instances. It is a standalone store that is not tied to any specific service.","title":"ETCD Cluster"},{"location":"kubernetes/architecture/#kube-scheduler","text":"The kube-scheduler is responsible for managing the scheduling of containers on nodes. It determines which pods should be run on which nodes. It can be run as a service and uses algorithms to prioritize nodes based on available resources (such as CPU). For example, it may do the following: Filter nodes based on available resources Rank nodes using a priority algorithm on a scale of 0-10","title":"Kube-scheduler"},{"location":"kubernetes/architecture/#controllers","text":"Controllers are responsible for monitoring the system and ensuring that desired state is maintained. They can be downloaded as a service and run on the master node.","title":"Controllers"},{"location":"kubernetes/architecture/#node-controllers","text":"Node controllers are responsible for monitoring the status of nodes and ensuring that they are running. They check the status of nodes every 5 seconds, and if a node becomes unreachable, they wait 40 seconds before marking it as unreachable.","title":"Node Controllers"},{"location":"kubernetes/architecture/#replication-controllers","text":"Replication controllers are responsible for ensuring that the desired number of pods are running. If there are not enough pods, they will create new ones to meet the desired count.","title":"Replication Controllers"},{"location":"kubernetes/architecture/#worker-nodes","text":"Worker nodes host applications as containers.","title":"Worker Nodes"},{"location":"kubernetes/architecture/#container-runtime-engine","text":"The container runtime engine is responsible for running and managing containers on the node. An example of a container runtime engine is Docker.","title":"Container Runtime Engine"},{"location":"kubernetes/architecture/#kubelet","text":"The kubelet is an agent that runs or creates pods on the node. It is responsible for registering the node with the cluster.","title":"Kubelet"},{"location":"kubernetes/architecture/#kube-proxy","text":"The kube-proxy can be run as a service and is installed on each node in the cluster. It creates iptables rules to facilitate communication between worker nodes.","title":"Kube-proxy"},{"location":"kubernetes/architecture/#pods","text":"A pod is the basic execution unit in Kubernetes and is where a container lives. It is recommended to have one container per pod, but helper containers can also be deployed with the main container.","title":"Pods"},{"location":"kubernetes/backup/","text":"get all services kubectl get all --all-namespaces -o yaml > all-deploy.yaml Tools VELERO ETCD cluster backup --data-dir /var/lib/etcd etcdctl snapshot save snapshot.db service kube-apiserver stop etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup systemctl daemon-reload systemctl etcd restart etcd need keys for that command","title":"Backup"},{"location":"kubernetes/backup/#get-all-services","text":"kubectl get all --all-namespaces -o yaml > all-deploy.yaml","title":"get all services"},{"location":"kubernetes/backup/#tools","text":"VELERO","title":"Tools"},{"location":"kubernetes/backup/#etcd-cluster-backup","text":"--data-dir /var/lib/etcd etcdctl snapshot save snapshot.db service kube-apiserver stop etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup systemctl daemon-reload systemctl etcd restart","title":"ETCD cluster backup"},{"location":"kubernetes/backup/#etcd-need-keys-for-that-command","text":"","title":"etcd need keys for that command"},{"location":"kubernetes/deployments/","text":"Deployment upgrade pods 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: apps/v1 kind: Deployment metadata: name: myapp-deployment labels: app: myapp type: front-end spec: template: metadata: name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-container image: nginx replicas: 3 selector: matchLabels: type: front-end kubectl create -f deployment.yml kubectl get deployments because it automaticly creates replicasets kubectl get replicas kubectl get pods kubectl get all Describe kubectl describe deployment name create deployment manually kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3","title":"Deployments"},{"location":"kubernetes/deployments/#deployment","text":"upgrade pods 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: apps/v1 kind: Deployment metadata: name: myapp-deployment labels: app: myapp type: front-end spec: template: metadata: name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-container image: nginx replicas: 3 selector: matchLabels: type: front-end kubectl create -f deployment.yml kubectl get deployments because it automaticly creates replicasets kubectl get replicas kubectl get pods kubectl get all","title":"Deployment"},{"location":"kubernetes/deployments/#describe","text":"kubectl describe deployment name","title":"Describe"},{"location":"kubernetes/deployments/#create-deployment-manually","text":"kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3","title":"create deployment manually"},{"location":"kubernetes/image-security/","text":"IMAGE security nginx is the same as nginx/nginx The default registry is docker.io Google's registry is gcr.io To login to a private registry: 1 docker login private-registry To create a secret for a private registry: 1 2 3 4 5 kubectl create secret docker-registry regcred \\ --docker-server=private-registry.io \\ --docker-username=registry-user \\ --docker-password=registry-password \\ --docker-email=registry-user@org.com To use the secret in a pod: 1 2 3 4 5 6 7 8 9 10 apiVersion : v1 kind : Pod metadata : name : nginx-pod spec : containers : - name : nginx image : private-registry.io/apps/internal-app imagePullSecrets : - name : regcred To run a container as a different user: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : nginx-pod spec : containers : - name : nginx image : private-registry.io/apps/internal-app imagePullSecrets : - name : regcred securityContext : runAsUser : 1000 capabilities : add : [ \"MAC_ADMIN\" ]","title":"Image Security"},{"location":"kubernetes/image-security/#image-security","text":"nginx is the same as nginx/nginx The default registry is docker.io Google's registry is gcr.io To login to a private registry: 1 docker login private-registry To create a secret for a private registry: 1 2 3 4 5 kubectl create secret docker-registry regcred \\ --docker-server=private-registry.io \\ --docker-username=registry-user \\ --docker-password=registry-password \\ --docker-email=registry-user@org.com To use the secret in a pod: 1 2 3 4 5 6 7 8 9 10 apiVersion : v1 kind : Pod metadata : name : nginx-pod spec : containers : - name : nginx image : private-registry.io/apps/internal-app imagePullSecrets : - name : regcred To run a container as a different user: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : nginx-pod spec : containers : - name : nginx image : private-registry.io/apps/internal-app imagePullSecrets : - name : regcred securityContext : runAsUser : 1000 capabilities : add : [ \"MAC_ADMIN\" ]","title":"IMAGE security"},{"location":"kubernetes/lifecycle/","text":"lifecycle rollout and versioning when create deploiment it triggers rollout and it creates revision when container updaited new revision is created. this helps tracking of changes end gives ability to rollback kubectl rollout status deployment/myapp-deployment kubectl rollout history deployment/myapp-deployment deploiment strategies recreate delete all and crate news rolling update replace one by one new replacasets will be created. update using kubectl apply kubectl set image == but not good idea Rollback kubectl rollout undo deployment/myapp-deployment","title":"Lifecycle"},{"location":"kubernetes/lifecycle/#lifecycle","text":"","title":"lifecycle"},{"location":"kubernetes/lifecycle/#rollout-and-versioning","text":"when create deploiment it triggers rollout and it creates revision when container updaited new revision is created. this helps tracking of changes end gives ability to rollback kubectl rollout status deployment/myapp-deployment kubectl rollout history deployment/myapp-deployment","title":"rollout and versioning"},{"location":"kubernetes/lifecycle/#deploiment-strategies","text":"","title":"deploiment strategies"},{"location":"kubernetes/lifecycle/#recreate","text":"delete all and crate news","title":"recreate"},{"location":"kubernetes/lifecycle/#rolling-update","text":"replace one by one new replacasets will be created.","title":"rolling update"},{"location":"kubernetes/lifecycle/#update-using-kubectl-apply","text":"","title":"update using kubectl apply"},{"location":"kubernetes/lifecycle/#kubectl-set-image-but-not-good-idea","text":"","title":"kubectl set image == but not good idea"},{"location":"kubernetes/lifecycle/#rollback","text":"kubectl rollout undo deployment/myapp-deployment","title":"Rollback"},{"location":"kubernetes/lifecycle/#_1","text":"","title":""},{"location":"kubernetes/maintanence/","text":"if node is down 5 minute, it considered as dead if it will be replicated to another node pod eviction is 5 minute Drain node kubectl node drain-1 moves nodes node becomes unshedulable reboot kubectl uncordon node-1 kubectl cordon node-1 -make unshedulable but not move pods Vesionin v1.1.1 major,minor,patch upgrade versions of kubernetes kubeadm , only cluseter kubectl upgrade plan kubectl upgrade apply v1.12.0 upgdare master first upgrade strategies all nodes together upgrade one node at time move pords to another nodes create new node with new version move pods to that and delete old take back not for maintenance kubectl drain node01 --ignore-daemonsets moved pods to another node now we update that node kubectl uncordon node01 noschedule but keep apps kubectl cordon node01 Cluster version kubectl get nodes update version in cluster drain nodes upate 3.systemctl restart daemon and kubelet kubectl uncordon in node we need kubeadm upgrade node too ETCD backup ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot save /opt/snapshot-pre-boot.db ETCD restore ETCDCTL_API=3 etcdctl --data-dir /var/lib/etcd-from-backup snapshot restore /opt/snapshot-pre-boot.db update /etc/kubernetes/manifests/etcd.yaml and update volume:hostapath and VolimeMount check membrs from external etcd ETCDCTL_API=3 etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/pki/ca.pem \\ --cert=/etc/etcd/pki/etcd.pem \\ --key=/etc/etcd/pki/etcd-key.pem \\ member list get link for snapshot kubectl describe pods -n kube-system etcd-cluster1-controlplane | grep advertise-client-urls get all keys kubectl describe pods -n kube-system etcd-cluster1-controlplane | grep pki","title":"Maintenance"},{"location":"kubernetes/maintanence/#_1","text":"if node is down 5 minute, it considered as dead if it will be replicated to another node pod eviction is 5 minute","title":""},{"location":"kubernetes/maintanence/#drain-node","text":"kubectl node drain-1 moves nodes node becomes unshedulable reboot kubectl uncordon node-1 kubectl cordon node-1 -make unshedulable but not move pods","title":"Drain node"},{"location":"kubernetes/maintanence/#vesionin","text":"v1.1.1 major,minor,patch","title":"Vesionin"},{"location":"kubernetes/maintanence/#upgrade-versions-of-kubernetes","text":"","title":"upgrade versions of kubernetes"},{"location":"kubernetes/maintanence/#kubeadm-only-cluseter","text":"kubectl upgrade plan kubectl upgrade apply v1.12.0","title":"kubeadm , only cluseter"},{"location":"kubernetes/maintanence/#upgdare-master-first","text":"","title":"upgdare master first"},{"location":"kubernetes/maintanence/#upgrade-strategies","text":"","title":"upgrade strategies"},{"location":"kubernetes/maintanence/#all-nodes-together","text":"","title":"all nodes together"},{"location":"kubernetes/maintanence/#upgrade-one-node-at-time","text":"move pords to another nodes","title":"upgrade one node at time"},{"location":"kubernetes/maintanence/#create-new-node-with-new-version","text":"move pods to that and delete old","title":"create new node with new version"},{"location":"kubernetes/maintanence/#take-back-not-for-maintenance","text":"kubectl drain node01 --ignore-daemonsets moved pods to another node now we update that node kubectl uncordon node01","title":"take back not for maintenance"},{"location":"kubernetes/maintanence/#noschedule-but-keep-apps","text":"kubectl cordon node01","title":"noschedule but keep apps"},{"location":"kubernetes/maintanence/#cluster-version","text":"kubectl get nodes","title":"Cluster version"},{"location":"kubernetes/maintanence/#update-version-in-cluster","text":"drain nodes upate 3.systemctl restart daemon and kubelet kubectl uncordon","title":"update version in cluster"},{"location":"kubernetes/maintanence/#in-node","text":"we need kubeadm upgrade node too","title":"in node"},{"location":"kubernetes/maintanence/#etcd-backup","text":"ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot save /opt/snapshot-pre-boot.db","title":"ETCD backup"},{"location":"kubernetes/maintanence/#etcd-restore","text":"ETCDCTL_API=3 etcdctl --data-dir /var/lib/etcd-from-backup snapshot restore /opt/snapshot-pre-boot.db update /etc/kubernetes/manifests/etcd.yaml and update volume:hostapath and VolimeMount","title":"ETCD restore"},{"location":"kubernetes/maintanence/#check-membrs-from-external-etcd","text":"ETCDCTL_API=3 etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/pki/ca.pem \\ --cert=/etc/etcd/pki/etcd.pem \\ --key=/etc/etcd/pki/etcd-key.pem \\ member list","title":"check membrs from external etcd"},{"location":"kubernetes/maintanence/#get-link-for-snapshot","text":"kubectl describe pods -n kube-system etcd-cluster1-controlplane | grep advertise-client-urls","title":"get link for snapshot"},{"location":"kubernetes/maintanence/#get-all-keys","text":"kubectl describe pods -n kube-system etcd-cluster1-controlplane | grep pki","title":"get all keys"},{"location":"kubernetes/monitoring/","text":"logging and monitoring kubelet contains anther tool named Cadvisor whhich monitors perfomance enable with minikube minikube addons enable metrics-server other git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git show stats kubectl top node kubectl top pod docker logs docker log -f dockername kubectl logs -f podname if in pods are miltiple container u have to specify name","title":"Monitoring"},{"location":"kubernetes/monitoring/#logging-and-monitoring","text":"kubelet contains anther tool named Cadvisor whhich monitors perfomance","title":"logging and monitoring"},{"location":"kubernetes/monitoring/#enable-with-minikube","text":"minikube addons enable metrics-server","title":"enable with minikube"},{"location":"kubernetes/monitoring/#other","text":"git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git","title":"other"},{"location":"kubernetes/monitoring/#show-stats","text":"kubectl top node kubectl top pod","title":"show stats"},{"location":"kubernetes/monitoring/#docker-logs","text":"docker log -f dockername kubectl logs -f podname","title":"docker  logs"},{"location":"kubernetes/monitoring/#if-in-pods-are-miltiple-container-u-have-to-specify-name","text":"","title":"if in pods are miltiple container u have to specify name"},{"location":"kubernetes/namespaces/","text":"default namespace kubernetes default uses namespace named default , kubesystem and kubepublic when service is created dns name automaticly assigned resources in namespace can refer by its name .connect(\"db-service\") to connect database in another namspace .connect(\"db-service.namespace.service.domain.domainlocal\") kubectl get pods --namespace=anothername creating pods in namespace kubectl create -f pod.yml --namespace=dev or add namespace under metadata section in yml creating namespaces 1 2 3 4 apiVersion: v1 kind: Namespace metadata: name: dev kubectl create -f namespace-dev.yaml kubectl create namespace dev set default namespace for command kubectl config set-context $(kubectl config current-context) --namespace=dev kubectl get pods show pods in all namespaces kubectl get pods --all-namespaces limit resources in namespace using ResourceQuota `yml `apiVersion: v1 kind: ResourceQuota metadata: name: compute-quota namespace: dev spec: hard: pods: \"10\" requests.cpu: \"4\" requests.memory: 5Gi limits.cpu: \"10\" limits.memory: 10Gi kubectl create -f quota.yml","title":"Namespaces"},{"location":"kubernetes/namespaces/#default-namespace","text":"kubernetes default uses namespace named default , kubesystem and kubepublic when service is created dns name automaticly assigned resources in namespace can refer by its name .connect(\"db-service\")","title":"default namespace"},{"location":"kubernetes/namespaces/#to-connect-database-in-another-namspace","text":".connect(\"db-service.namespace.service.domain.domainlocal\") kubectl get pods --namespace=anothername","title":"to connect database in another namspace"},{"location":"kubernetes/namespaces/#creating-pods-in-namespace","text":"kubectl create -f pod.yml --namespace=dev or add namespace under metadata section in yml","title":"creating pods in namespace"},{"location":"kubernetes/namespaces/#creating-namespaces","text":"1 2 3 4 apiVersion: v1 kind: Namespace metadata: name: dev kubectl create -f namespace-dev.yaml kubectl create namespace dev","title":"creating namespaces"},{"location":"kubernetes/namespaces/#set-default-namespace-for-command","text":"kubectl config set-context $(kubectl config current-context) --namespace=dev kubectl get pods","title":"set default namespace for command"},{"location":"kubernetes/namespaces/#show-pods-in-all-namespaces","text":"kubectl get pods --all-namespaces","title":"show pods in all namespaces"},{"location":"kubernetes/namespaces/#limit-resources-in-namespace-using-resourcequota","text":"`yml `apiVersion: v1 kind: ResourceQuota metadata: name: compute-quota namespace: dev spec: hard: pods: \"10\" requests.cpu: \"4\" requests.memory: 5Gi limits.cpu: \"10\" limits.memory: 10Gi kubectl create -f quota.yml","title":"limit resources in namespace using ResourceQuota"},{"location":"kubernetes/newtwork/","text":"Ingress ingress: from: podSelector: matchLabels: role: api-pod ports: protocol: TCP port: 3306","title":"Network"},{"location":"kubernetes/pods/","text":"pods yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : apps/v1 kind : Pod metadata : name : myapp-pod labels : app : myapp type : front-end spec : containers : - name : nginx-container image : nginx kubectl create -f pod.yml show pods kubectl get pods show detail info create simple yml file kubectl run redis --image=redis123 --dry-run=client -o yaml > redis-definition.yaml apply changes kubectl apply -f redis-definition.yaml add tolerration to pods 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : apps/v1 kind : Pod metadata : name : myapp-pod labels : app : myapp type : front-end spec : containers : - name : nginx-container image : nginx tolerration : -key:app operator : \"equal\" value : blue effect : Noschedle | PreferNoSchedule|NoExecute limit resource 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : v1 kind : Pod metadata : name : simple-webapp-color labels : name : simple-webapp-color spec : containers : - name : simple-webapp-color image : simple-webapp-color ports : - containerPort : 8080 resources : requests : memory : \"1Gi\" cpu : \"1\" default pods VCPU value is 1 and 512 Mi memory pods can use more memory that needed but it is permanently it will be terminated Static pods if we have only kubelet on server ,but kubelet can create pods. we can provide kubelet to read pod definition files. we can add yml files /etc/kubernetes/manifests/ folder and kubelet automaticly create this pods. if we delete that file. pod will be removed. it works only with Pods. we can add service parameter to change path --pod-manifest-path=/home/davit/kubemanifest or kubeconfig.yaml if it is not service. use docker ps to see pods use custom sheduler under spec: schedulerName: custom-scheduler view events kubectl get events get all pods kubectl get pods --all-namespaces pod sample kubectl run redis --image=redis:alpine --dry-run=client -o yaml > redis.yaml exmpose pod directly NodePort kubect run custom-nginx --image=ngin --port=8080 expose pod ClusterIp kubectl run httpd --image=httpd:alpine --port=80 --expose get system cluster pods kubectl get pods --namespace kube-system filter pods by selector kubectl get pods --selector env=dev get all objects using selector kubectl get all --selector env=prod multiple selectors kubectl get pods --selector env=prod,bu=finance,tier=frontend create yaml from running pod kubectl get pod elephant -o yaml > elep.yaml replace pod by force kubectl replace -f elephant.yaml --force detect static pods controlplane at the end of pods name get wide info with pods kubectl get pods -o wide add arguments to pods 1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Pod metadata: name: ubuntu-sleeper-pod spec: containers: - name: ubuntu-sleeper image: ubuntu-sleeper command: [\"sleep2.0\"] args: [\"10\"] add env variables in yml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 8080 env: - name: APP_COLOR value: pink add env with configmaps kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod kubectl create configmap app-config --from-file=app_config.properties (Another way) or yaml apiVersion: v1 kind: ConfigMap metadata: name: app-config data: APP_COLOR: blue APP_MODE: prod kubectl get configmaps map configmap 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 8080 envFrom: - configMapRef: name: app-config we can inject using volumes too use Secrets for passwords kubectl create secret generic app-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root --from-literal=DB_Password=paswrd kubectl create secret generic app-secret --from-file=app_secret.properties echo -n \"mysql\" | base64 1 2 3 4 5 6 7 8 apiVersion: v1 kind: Secret metadata: name: app-secret data: DB_Host: bX1zcWw= DB_User: cm9vdA== DB_Password: cGFzd3Jk kubectl create -f secret-data.yaml kubectl get secrets get data kubectl get secret app-secret -o yaml if it is mounted as Volume each password will we as file like /opt/passwords multiple container pods 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : simple-webapp labels : name : simple-webapp spec : containers : - name : simple-webapp image : simple-webapp ports : - ContainerPort : 8080 - name : log-agent image : log-agent","title":"Pods"},{"location":"kubernetes/pods/#pods-yaml","text":"1 2 3 4 5 6 7 8 9 10 11 apiVersion : apps/v1 kind : Pod metadata : name : myapp-pod labels : app : myapp type : front-end spec : containers : - name : nginx-container image : nginx kubectl create -f pod.yml show pods kubectl get pods show detail info","title":"pods yaml"},{"location":"kubernetes/pods/#create-simple-yml-file","text":"kubectl run redis --image=redis123 --dry-run=client -o yaml > redis-definition.yaml","title":"create simple yml file"},{"location":"kubernetes/pods/#apply-changes","text":"kubectl apply -f redis-definition.yaml","title":"apply changes"},{"location":"kubernetes/pods/#add-tolerration-to-pods","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : apps/v1 kind : Pod metadata : name : myapp-pod labels : app : myapp type : front-end spec : containers : - name : nginx-container image : nginx tolerration : -key:app operator : \"equal\" value : blue effect : Noschedle | PreferNoSchedule|NoExecute","title":"add tolerration to pods"},{"location":"kubernetes/pods/#limit-resource","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : v1 kind : Pod metadata : name : simple-webapp-color labels : name : simple-webapp-color spec : containers : - name : simple-webapp-color image : simple-webapp-color ports : - containerPort : 8080 resources : requests : memory : \"1Gi\" cpu : \"1\"","title":"limit resource"},{"location":"kubernetes/pods/#default-pods-vcpu-value-is-1-and-512-mi-memory","text":"pods can use more memory that needed but it is permanently it will be terminated","title":"default pods VCPU value is 1 and 512 Mi memory"},{"location":"kubernetes/pods/#static-pods","text":"if we have only kubelet on server ,but kubelet can create pods. we can provide kubelet to read pod definition files. we can add yml files /etc/kubernetes/manifests/ folder and kubelet automaticly create this pods. if we delete that file. pod will be removed. it works only with Pods. we can add service parameter to change path --pod-manifest-path=/home/davit/kubemanifest or kubeconfig.yaml if it is not service. use docker ps to see pods","title":"Static pods"},{"location":"kubernetes/pods/#use-custom-sheduler","text":"under spec: schedulerName: custom-scheduler","title":"use custom sheduler"},{"location":"kubernetes/pods/#view-events","text":"kubectl get events","title":"view events"},{"location":"kubernetes/pods/#get-all-pods","text":"kubectl get pods --all-namespaces","title":"get all pods"},{"location":"kubernetes/pods/#pod-sample","text":"kubectl run redis --image=redis:alpine --dry-run=client -o yaml > redis.yaml","title":"pod sample"},{"location":"kubernetes/pods/#exmpose-pod-directly-nodeport","text":"kubect run custom-nginx --image=ngin --port=8080","title":"exmpose pod directly NodePort"},{"location":"kubernetes/pods/#expose-pod-clusterip","text":"kubectl run httpd --image=httpd:alpine --port=80 --expose","title":"expose pod ClusterIp"},{"location":"kubernetes/pods/#get-system-cluster-pods","text":"kubectl get pods --namespace kube-system","title":"get system cluster pods"},{"location":"kubernetes/pods/#filter-pods-by-selector","text":"kubectl get pods --selector env=dev","title":"filter pods by selector"},{"location":"kubernetes/pods/#get-all-objects-using-selector","text":"kubectl get all --selector env=prod","title":"get all objects using selector"},{"location":"kubernetes/pods/#multiple-selectors","text":"kubectl get pods --selector env=prod,bu=finance,tier=frontend","title":"multiple selectors"},{"location":"kubernetes/pods/#create-yaml-from-running-pod","text":"kubectl get pod elephant -o yaml > elep.yaml","title":"create yaml from running pod"},{"location":"kubernetes/pods/#replace-pod-by-force","text":"kubectl replace -f elephant.yaml --force","title":"replace pod by force"},{"location":"kubernetes/pods/#detect-static-pods","text":"controlplane at the end of pods name","title":"detect static pods"},{"location":"kubernetes/pods/#get-wide-info-with-pods","text":"kubectl get pods -o wide","title":"get wide info with pods"},{"location":"kubernetes/pods/#add-arguments-to-pods","text":"1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Pod metadata: name: ubuntu-sleeper-pod spec: containers: - name: ubuntu-sleeper image: ubuntu-sleeper command: [\"sleep2.0\"] args: [\"10\"]","title":"add arguments to pods"},{"location":"kubernetes/pods/#add-env-variables-in-yml","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 8080 env: - name: APP_COLOR value: pink","title":"add env variables in yml"},{"location":"kubernetes/pods/#add-env-with-configmaps","text":"kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod kubectl create configmap app-config --from-file=app_config.properties (Another way) or yaml apiVersion: v1 kind: ConfigMap metadata: name: app-config data: APP_COLOR: blue APP_MODE: prod kubectl get configmaps","title":"add env with configmaps"},{"location":"kubernetes/pods/#map-configmap","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 8080 envFrom: - configMapRef: name: app-config","title":"map configmap"},{"location":"kubernetes/pods/#we-can-inject-using-volumes-too","text":"","title":"we can inject using volumes too"},{"location":"kubernetes/pods/#use-secrets-for-passwords","text":"kubectl create secret generic app-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root --from-literal=DB_Password=paswrd kubectl create secret generic app-secret --from-file=app_secret.properties echo -n \"mysql\" | base64 1 2 3 4 5 6 7 8 apiVersion: v1 kind: Secret metadata: name: app-secret data: DB_Host: bX1zcWw= DB_User: cm9vdA== DB_Password: cGFzd3Jk kubectl create -f secret-data.yaml kubectl get secrets","title":"use Secrets for passwords"},{"location":"kubernetes/pods/#get-data","text":"kubectl get secret app-secret -o yaml","title":"get data"},{"location":"kubernetes/pods/#if-it-is-mounted-as-volume","text":"each password will we as file like /opt/passwords","title":"if it is mounted as Volume"},{"location":"kubernetes/pods/#multiple-container-pods","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : simple-webapp labels : name : simple-webapp spec : containers : - name : simple-webapp image : simple-webapp ports : - ContainerPort : 8080 - name : log-agent image : log-agent","title":"multiple container pods"},{"location":"kubernetes/replicasets/","text":"replication controller Replication Controller is the older technology that is being replaced by a ReplicaSet. ReplicaSet is the new way to setup replication. automaticaly bring new pods if needed Load balancing & scaling 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: ReplicationController metadata: name: myapp-rc labels: app: myapp type: front-end spec: template: metadata: name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-container image: nginx replicas: 3 kubectl create -f replica.yml kubectl get replicationcontroller 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: apps/v1 kind: ReplicaSet metadata: name: myapp-replicaset labels: app: myapp type: front-end spec: template: metadata: name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-container image: nginx replicas: 3 selector: matchLabels: type: front-end replica set needs selector definition scale replica sets update file kubectl replace -f replica.yml kubectl scale --replicas=6 -f replica.yml kubectl scale --replicas=6 replicaset myapp-replicaset delete replicaset kubectl delete replicaset myapp-replicaset kubectl delete --all namespaces get replicasetso kubectl get replicasets.apps describe kubectl describe replicasets.apps new-replica-set get version of replicaset kubectl explain replicaset | grep VERSION edit replica, uses editor automaticaly kubectl edit replicaset new-replica-set","title":"ReplicaSets"},{"location":"kubernetes/replicasets/#replication-controller","text":"Replication Controller is the older technology that is being replaced by a ReplicaSet. ReplicaSet is the new way to setup replication. automaticaly bring new pods if needed","title":"replication controller"},{"location":"kubernetes/replicasets/#load-balancing-scaling","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: ReplicationController metadata: name: myapp-rc labels: app: myapp type: front-end spec: template: metadata: name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-container image: nginx replicas: 3 kubectl create -f replica.yml kubectl get replicationcontroller 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: apps/v1 kind: ReplicaSet metadata: name: myapp-replicaset labels: app: myapp type: front-end spec: template: metadata: name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-container image: nginx replicas: 3 selector: matchLabels: type: front-end replica set needs selector definition","title":"Load balancing &amp; scaling"},{"location":"kubernetes/replicasets/#scale-replica-sets","text":"update file kubectl replace -f replica.yml kubectl scale --replicas=6 -f replica.yml kubectl scale --replicas=6 replicaset myapp-replicaset","title":"scale replica sets"},{"location":"kubernetes/replicasets/#delete-replicaset","text":"kubectl delete replicaset myapp-replicaset kubectl delete --all namespaces","title":"delete replicaset"},{"location":"kubernetes/replicasets/#get-replicasetso","text":"kubectl get replicasets.apps","title":"get replicasetso"},{"location":"kubernetes/replicasets/#describe","text":"kubectl describe replicasets.apps new-replica-set","title":"describe"},{"location":"kubernetes/replicasets/#get-version-of-replicaset","text":"kubectl explain replicaset | grep VERSION","title":"get version of replicaset"},{"location":"kubernetes/replicasets/#edit-replica-uses-editor-automaticaly","text":"kubectl edit replicaset new-replica-set","title":"edit replica, uses editor automaticaly"},{"location":"kubernetes/roles/","text":"CREATE ROLE 1 2 3 4 5 6 7 8 apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : cluster-administrator rules : - apiGroups : [ \"\" ] # \"\" indicates the core API group resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" , \"delete\" , \"create\" ] 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : cluster-admin-role-binding subjects : - kind : User name : cluster-admin apiGroup : rbac.authorization.k8s.io roleRef : kind : ClusterRole name : cluster-administrator apiGroup : rbac.authorization.k8s.io SERVICE ACCOUNT AS BOT ACCOUNT kubectl create serviceaccount dashboard-sa kubectl get serviceaccounts kubectl describe serviceaccount dashboiard-sa get secreet kubectl describe secret dashboard-sa-token-kbbdm secrets are mounter /var/run/secret/kubernetis.io/serviceaccount in Pod xml: serviceAccount:dashboard-sa automountServiceAccountToken: false get roles kubectl get roles describe role kubectl describe role kube-proxy -n kube-system describe rolebinding kubectl describe rolebinding kube-proxy -n kube-system use command as different role --as dev-user create role 1 2 3 4 5 6 7 8 9 kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : namespace : default name : developer rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"list\" , \"create\" , \"delete\" ] create rolebind 1 2 3 4 5 6 7 8 9 10 11 12 kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : dev-user-binding subjects : - kind : User name : dev-user apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : developer apiGroup : rbac.authorization.k8s.io","title":"Roles"},{"location":"kubernetes/roles/#create-role","text":"1 2 3 4 5 6 7 8 apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : cluster-administrator rules : - apiGroups : [ \"\" ] # \"\" indicates the core API group resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" , \"delete\" , \"create\" ] 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : cluster-admin-role-binding subjects : - kind : User name : cluster-admin apiGroup : rbac.authorization.k8s.io roleRef : kind : ClusterRole name : cluster-administrator apiGroup : rbac.authorization.k8s.io","title":"CREATE ROLE"},{"location":"kubernetes/roles/#service-account-as-bot-account","text":"kubectl create serviceaccount dashboard-sa kubectl get serviceaccounts kubectl describe serviceaccount dashboiard-sa","title":"SERVICE ACCOUNT AS BOT ACCOUNT"},{"location":"kubernetes/roles/#get-secreet","text":"kubectl describe secret dashboard-sa-token-kbbdm","title":"get secreet"},{"location":"kubernetes/roles/#secrets-are-mounter-varrunsecretkubernetisioserviceaccount","text":"in Pod xml: serviceAccount:dashboard-sa automountServiceAccountToken: false","title":"secrets are mounter /var/run/secret/kubernetis.io/serviceaccount"},{"location":"kubernetes/roles/#get-roles","text":"kubectl get roles","title":"get roles"},{"location":"kubernetes/roles/#describe-role","text":"kubectl describe role kube-proxy -n kube-system","title":"describe role"},{"location":"kubernetes/roles/#describe-rolebinding","text":"kubectl describe rolebinding kube-proxy -n kube-system","title":"describe rolebinding"},{"location":"kubernetes/roles/#use-command-as-different-role","text":"--as dev-user","title":"use command as different role"},{"location":"kubernetes/roles/#create-role_1","text":"1 2 3 4 5 6 7 8 9 kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : namespace : default name : developer rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"list\" , \"create\" , \"delete\" ]","title":"create role"},{"location":"kubernetes/roles/#create-rolebind","text":"1 2 3 4 5 6 7 8 9 10 11 12 kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : dev-user-binding subjects : - kind : User name : dev-user apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : developer apiGroup : rbac.authorization.k8s.io","title":"create rolebind"},{"location":"kubernetes/security/","text":"cluster cluster who can acess? how: username and password username and tokens certificates externale auth providers - ldap service accounts authorization RBAC - role based ABAC NODE auth webhook mode TLS sertificates between applications all can access each other but it can be restricted with network policies Users we can not create users in kubernetes but we can create service accounts kubectl create service account accounts managed by kube-apiserver authenticate user static file auth password,user,user_id,group --basic-auth-file = details.csv static token file token,user,uid, group --token-auth-file = token.csv SSL TLS sertificates creting certificates private key openssl genrsa -out ca.key 2048 specify name of what is for. this is signing openssl req -new -key ca.key -subj \"/CN=KUBERNETES-CA\" -out ca.csr sign request. this is self signed openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt generate client certificeates openssl genrsa -out admin.key 2048 this name is for logs mostly openssl req -new -key ca.key -subj \"/CN=kube-admin\" -out admin.csr we must mention group details in signing request openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt why we need certs we cen auth to kluster apiserver using this key or in cluster definiton we can add this keys CA root certificates needed for client if there is more dns names we have create openssl conf user kubelet certificates by its node names to dermined which node is requested name: system:node:node01 system:node:node02 view certificates kubeadm automaitlcy deploys certs openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout what if new admin comes CA server - pair of certificates files certificates key is on CA server. certificates API create CertificateeSigningRequest object and can be reviewd approved how it is done openssl genrsa -out jane.key 204 openssl req -new -key jane.key -subj =\"/CN=jane\" out -jane.csr 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: jane spec: groups: - system:authenticated usages: - digital signature - key encipherment - server auth request: <certificate-goes-here> cat jane.csr |base64 > and paster in requet above show request kubectl get csr aprove requests kubectl certificate approve jane it automaticly generates client certificates kubectl get csr jane -o yaml echo cert| base --decode and share to user all the certificates are managed by controller manager kubeconfigtes default config .kube/config clusters development production google contexts admin@production google@production Users admin dev view config kubectl config view kubectl config use-context prod-user@production kubectl config -h config namespaces add namespace in config to switch automaticly api group curl http://localhost:6443 -k and add certs or kubectl proxy it is not kube proxy authorization - what they can do different types of authorization node abac can view pod can delete pod needs policie definiton file it is bad practice RBAC we define role associate role to users webhook we need auth to be managed to another tool for example open policy agent AlwaysAllow AllwaysDeny systemctl - > --authorization-mode= authorization abac dev-user - access using policy file in json format. every time we need change we have to change file and restart server RBAC create roles and add users to this role webhook openpolycyagent third party auth AlwaysAllow and AllwaysDeny by default is AlwaysAllow --authorization-mode=None,RBAC,Webhook certificate apprval example cat akshay.csr | base64 -w 0 -w - wrap 0 `--- apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: akshay spec: groups: - system:authenticated request: CCCCCCC signerName: kubernetes.io/kube-apiserver-client usages: - client auth`` ``` check status kubectl get csr approve csr kubectl certificate approve akshay get detals of cst kubectl get csr agent-smith -o yaml deny reject kubectl certificate deny agent-smith delete csr kubectl delete csr agent-smith","title":"Security"},{"location":"kubernetes/security/#cluster-cluster","text":"who can acess? how: username and password username and tokens certificates externale auth providers - ldap service accounts","title":"cluster cluster"},{"location":"kubernetes/security/#authorization","text":"RBAC - role based ABAC NODE auth webhook mode","title":"authorization"},{"location":"kubernetes/security/#tls-sertificates","text":"","title":"TLS sertificates"},{"location":"kubernetes/security/#between-applications","text":"all can access each other but it can be restricted with network policies","title":"between applications"},{"location":"kubernetes/security/#users","text":"we can not create users in kubernetes but we can create service accounts kubectl create service account","title":"Users"},{"location":"kubernetes/security/#accounts","text":"managed by kube-apiserver authenticate user","title":"accounts"},{"location":"kubernetes/security/#static-file-auth","text":"password,user,user_id,group --basic-auth-file = details.csv","title":"static file auth"},{"location":"kubernetes/security/#static-token-file","text":"token,user,uid, group --token-auth-file = token.csv","title":"static token file"},{"location":"kubernetes/security/#ssl-tls-sertificates","text":"","title":"SSL TLS sertificates"},{"location":"kubernetes/security/#creting-certificates","text":"private key openssl genrsa -out ca.key 2048 specify name of what is for. this is signing openssl req -new -key ca.key -subj \"/CN=KUBERNETES-CA\" -out ca.csr sign request. this is self signed openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt","title":"creting certificates"},{"location":"kubernetes/security/#generate-client-certificeates","text":"openssl genrsa -out admin.key 2048 this name is for logs mostly openssl req -new -key ca.key -subj \"/CN=kube-admin\" -out admin.csr we must mention group details in signing request openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt","title":"generate client certificeates"},{"location":"kubernetes/security/#why-we-need-certs","text":"we cen auth to kluster apiserver using this key or in cluster definiton we can add this keys","title":"why we need certs"},{"location":"kubernetes/security/#ca-root-certificates-needed-for-client","text":"if there is more dns names we have create openssl conf","title":"CA root certificates needed for client"},{"location":"kubernetes/security/#user-kubelet-certificates-by-its-node-names","text":"to dermined which node is requested name: system:node:node01 system:node:node02","title":"user kubelet certificates by its node names"},{"location":"kubernetes/security/#view-certificates","text":"kubeadm automaitlcy deploys certs openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout","title":"view certificates"},{"location":"kubernetes/security/#what-if-new-admin-comes","text":"CA server - pair of certificates files certificates key is on CA server.","title":"what if new admin comes"},{"location":"kubernetes/security/#certificates-api","text":"create CertificateeSigningRequest object and can be reviewd approved","title":"certificates API"},{"location":"kubernetes/security/#how-it-is-done","text":"openssl genrsa -out jane.key 204 openssl req -new -key jane.key -subj =\"/CN=jane\" out -jane.csr 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: jane spec: groups: - system:authenticated usages: - digital signature - key encipherment - server auth request: <certificate-goes-here> cat jane.csr |base64 > and paster in requet above","title":"how it is done"},{"location":"kubernetes/security/#show-request","text":"kubectl get csr","title":"show request"},{"location":"kubernetes/security/#aprove-requests","text":"kubectl certificate approve jane it automaticly generates client certificates kubectl get csr jane -o yaml echo cert| base --decode and share to user","title":"aprove requests"},{"location":"kubernetes/security/#all-the-certificates-are-managed-by-controller-manager","text":"","title":"all the certificates are managed by controller manager"},{"location":"kubernetes/security/#kubeconfigtes","text":"default config .kube/config clusters development production google contexts admin@production google@production Users admin dev","title":"kubeconfigtes"},{"location":"kubernetes/security/#view-config","text":"kubectl config view kubectl config use-context prod-user@production kubectl config -h","title":"view config"},{"location":"kubernetes/security/#config-namespaces","text":"add namespace in config to switch automaticly","title":"config namespaces"},{"location":"kubernetes/security/#api-group","text":"curl http://localhost:6443 -k and add certs or kubectl proxy it is not kube proxy","title":"api group"},{"location":"kubernetes/security/#authorization-what-they-can-do","text":"","title":"authorization - what they can do"},{"location":"kubernetes/security/#different-types-of-authorization","text":"node abac can view pod can delete pod needs policie definiton file it is bad practice RBAC we define role associate role to users webhook we need auth to be managed to another tool for example open policy agent AlwaysAllow AllwaysDeny systemctl - > --authorization-mode=","title":"different types of authorization"},{"location":"kubernetes/security/#authorization_1","text":"","title":"authorization"},{"location":"kubernetes/security/#abac","text":"dev-user - access using policy file in json format. every time we need change we have to change file and restart server","title":"abac"},{"location":"kubernetes/security/#rbac","text":"create roles and add users to this role","title":"RBAC"},{"location":"kubernetes/security/#webhook","text":"openpolycyagent third party auth","title":"webhook"},{"location":"kubernetes/security/#alwaysallow-and-allwaysdeny","text":"by default is AlwaysAllow --authorization-mode=None,RBAC,Webhook","title":"AlwaysAllow and AllwaysDeny"},{"location":"kubernetes/security/#certificate-apprval-example","text":"cat akshay.csr | base64 -w 0 -w - wrap 0 `--- apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: akshay spec: groups: - system:authenticated request: CCCCCCC signerName: kubernetes.io/kube-apiserver-client usages: - client auth`` ```","title":"certificate apprval example"},{"location":"kubernetes/security/#check-status","text":"kubectl get csr","title":"check status"},{"location":"kubernetes/security/#approve-csr","text":"kubectl certificate approve akshay","title":"approve csr"},{"location":"kubernetes/security/#get-detals-of-cst","text":"kubectl get csr agent-smith -o yaml","title":"get  detals of cst"},{"location":"kubernetes/security/#deny-reject","text":"kubectl certificate deny agent-smith","title":"deny reject"},{"location":"kubernetes/security/#delete-csr","text":"kubectl delete csr agent-smith","title":"delete csr"},{"location":"kubernetes/services/","text":"Services Services enables communication between various components within and outside of the application. Service types NodePort Where the service makes an internal POD accessible on a POD on the NODE. Where the service makes an internal POD accessible on a POD on the NODE. 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Service metadata: name: myapp-service spec: types: NodePort ports: - targetPort: 80 port: 80 nodePort: 30008 selector: app:myapp type:front-end if there is multiple pods in same node it will load balance else we can access it like ip:port kubectl create -f service-definition.yaml kubectl get services CluserIP in nodes if there is multiple applications like fronted backend , db they need to communicate to each other. but communicate with ip is not opation they will change CluserIP gives ability to group this pods under one name and give other pods to access with name. 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: back-end spec: types: ClusterIP ports: - targetPort: 80 port: 80 selector: app: myapp type: back-end kubectl get services LoadBalacer we can instlal load balancer like HA or nginx and add node ports . we can user native cloud balancer useing LoadBalacer service. 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: back-end spec: types: LoadBalacer ports: - targetPort: 80 port: 80 selector: app: myapp type: back-end create sample yaml file kubectl run redis --image=redis:alpine --dry-run=client -o yaml > redis.yaml create service from command line kubectl expose pod redis --name redis-service --port=3839","title":"Services"},{"location":"kubernetes/services/#services","text":"Services enables communication between various components within and outside of the application.","title":"Services"},{"location":"kubernetes/services/#service-types","text":"","title":"Service types"},{"location":"kubernetes/services/#nodeport","text":"Where the service makes an internal POD accessible on a POD on the NODE. Where the service makes an internal POD accessible on a POD on the NODE. 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Service metadata: name: myapp-service spec: types: NodePort ports: - targetPort: 80 port: 80 nodePort: 30008 selector: app:myapp type:front-end if there is multiple pods in same node it will load balance else we can access it like ip:port kubectl create -f service-definition.yaml kubectl get services","title":"NodePort"},{"location":"kubernetes/services/#cluserip","text":"in nodes if there is multiple applications like fronted backend , db they need to communicate to each other. but communicate with ip is not opation they will change CluserIP gives ability to group this pods under one name and give other pods to access with name. 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: back-end spec: types: ClusterIP ports: - targetPort: 80 port: 80 selector: app: myapp type: back-end kubectl get services","title":"CluserIP"},{"location":"kubernetes/services/#loadbalacer","text":"we can instlal load balancer like HA or nginx and add node ports . we can user native cloud balancer useing LoadBalacer service. 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: back-end spec: types: LoadBalacer ports: - targetPort: 80 port: 80 selector: app: myapp type: back-end","title":"LoadBalacer"},{"location":"kubernetes/services/#create-sample-yaml-file","text":"kubectl run redis --image=redis:alpine --dry-run=client -o yaml > redis.yaml","title":"create sample yaml file"},{"location":"kubernetes/services/#create-service-from-command-line","text":"kubectl expose pod redis --name redis-service --port=3839","title":"create service from command line"},{"location":"kubernetes/sheduler/","text":"sheduler manual schedulin every yml file pod definiton has nodename sheduler looks who doesnot have it and runs scheduling algorithm and binds pod to node if there is not scheduler pods will be in a pending state 1 2 spec: nodeName: node02 we cann not update it after pod creation but we can update it with post request Labels and selectors Labels are properties attached to items selectors help to filter labels 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : Pod metadata : name : simple-webapp labels : app : App1 function : Front-end spec : containers : - name : simple-webapp image : simple-webapp ports : - containerPort : 8080 kubectl get pods --selector app=App1 to creaate replicaset with connected to pods 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : apps/v1 kind : ReplicaSet metadata : name : simple-webapp labels : app : App1 function : Front-end spec : replicas : 3 selector : matchLabels : app : App1 template : metadata : labels : app : App1 function : Front-end spec : containers : - name : simple-webapp image : simple-webapp annotations annotations: buildversion: 1.2 record other details for info taint and tolerations allow certain nodes to accept only specific pods kubectl taint nodes node1 app=blue:Noschedule node affinity certain pods to exact node in master taint automaticly is added kubectl describe node kubemaster | grep Taint Node selectors add nodeSelector in pods spec size: Large label nodes kubectl label nodes nodename size=Large limitation u can not small or medium node affinity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : v1 kind : Pod metadata : name : myapp-pod spec : containers : - name : data-processor image : data-processor affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : size operator : In values : - Large - Medium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : v1 kind : Pod metadata : name : myapp-pod spec : containers : - name : data-processor image : data-processor affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : size operator : NotIn values : - Small ``` ## requiredDuringSchedulingIgnoredDuringExecution only this type of node ## prefferedDuringSchedulingIgnoredDuringExecution if not found sheduler ignore affinity rules ## requiredDuringSchedulingRequirdDuringExecution bad pods automaticly will be deleted ## taint and toleration together to place exacly where we want ## Daemon Sets for example we want logging agent in all cluster and node or kube-proxy can be deployed as daemon set ``` yml apiVersion : apps/v1 kind : DaemonSet metadata : name : monitoring-daemon labels : app : nginx spec : selector : matchLabels : app : monitoring-agent template : metadata : labels : app : monitoring-agent spec : containers : - name : monitoring-agent image : monitoring-agent kubectl get daemonset kubectl describe daemonset deemon sets uses node affinity and taint after 1.x verion before was nodeName: what we want custom sheduling program. kubernetes cluster can have multiple shedulers service options: --scheduler-name:custom-scheduler or name int /etc/kubernetes/manifest/kube-sheduler/yaml and add in command: --scheduler-name=custom-scheduler if there is multiple replicas sheduler only one is active there is election process who will be leader where is parameter to aviod newly created schedulers to get leaders --lock-object-name=custom-scheduler get logs kubectl logs custom-scheduler --name-space=kube-system add taint node kubectl taint nodes nodes01 spray=mortein:NoSchedule remove taint node kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule- add label to node kubectl label node node01 color=blue get all daemonsets kubectl get daemonsets --all-namespaces describe daemon shedulet pods kubectl describe daemonset kube-proxy --namespace=kube-system create daemonset yaml kubectl create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml > fluentd.yaml create addidional scheduler from file kubectl create -n kube-system configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml","title":"Scheduler"},{"location":"kubernetes/sheduler/#sheduler","text":"","title":"sheduler"},{"location":"kubernetes/sheduler/#manual-schedulin","text":"every yml file pod definiton has nodename sheduler looks who doesnot have it and runs scheduling algorithm and binds pod to node if there is not scheduler pods will be in a pending state 1 2 spec: nodeName: node02 we cann not update it after pod creation but we can update it with post request","title":"manual schedulin"},{"location":"kubernetes/sheduler/#labels-and-selectors","text":"Labels are properties attached to items selectors help to filter labels 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : Pod metadata : name : simple-webapp labels : app : App1 function : Front-end spec : containers : - name : simple-webapp image : simple-webapp ports : - containerPort : 8080 kubectl get pods --selector app=App1","title":"Labels and selectors"},{"location":"kubernetes/sheduler/#to-creaate-replicaset-with-connected-to-pods","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : apps/v1 kind : ReplicaSet metadata : name : simple-webapp labels : app : App1 function : Front-end spec : replicas : 3 selector : matchLabels : app : App1 template : metadata : labels : app : App1 function : Front-end spec : containers : - name : simple-webapp image : simple-webapp","title":"to creaate replicaset with connected to pods"},{"location":"kubernetes/sheduler/#annotations","text":"annotations: buildversion: 1.2 record other details for info","title":"annotations"},{"location":"kubernetes/sheduler/#taint-and-tolerations","text":"allow certain nodes to accept only specific pods kubectl taint nodes node1 app=blue:Noschedule","title":"taint and tolerations"},{"location":"kubernetes/sheduler/#node-affinity","text":"certain pods to exact node in master taint automaticly is added kubectl describe node kubemaster | grep Taint","title":"node affinity"},{"location":"kubernetes/sheduler/#node-selectors","text":"add nodeSelector in pods spec size: Large","title":"Node selectors"},{"location":"kubernetes/sheduler/#label-nodes","text":"kubectl label nodes nodename size=Large limitation u can not small or medium","title":"label nodes"},{"location":"kubernetes/sheduler/#node-affinity_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : v1 kind : Pod metadata : name : myapp-pod spec : containers : - name : data-processor image : data-processor affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : size operator : In values : - Large - Medium 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : v1 kind : Pod metadata : name : myapp-pod spec : containers : - name : data-processor image : data-processor affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : size operator : NotIn values : - Small ``` ## requiredDuringSchedulingIgnoredDuringExecution only this type of node ## prefferedDuringSchedulingIgnoredDuringExecution if not found sheduler ignore affinity rules ## requiredDuringSchedulingRequirdDuringExecution bad pods automaticly will be deleted ## taint and toleration together to place exacly where we want ## Daemon Sets for example we want logging agent in all cluster and node or kube-proxy can be deployed as daemon set ``` yml apiVersion : apps/v1 kind : DaemonSet metadata : name : monitoring-daemon labels : app : nginx spec : selector : matchLabels : app : monitoring-agent template : metadata : labels : app : monitoring-agent spec : containers : - name : monitoring-agent image : monitoring-agent kubectl get daemonset kubectl describe daemonset deemon sets uses node affinity and taint after 1.x verion before was nodeName:","title":"node affinity"},{"location":"kubernetes/sheduler/#what-we-want-custom-sheduling-program","text":"kubernetes cluster can have multiple shedulers service options: --scheduler-name:custom-scheduler or name int /etc/kubernetes/manifest/kube-sheduler/yaml and add in command: --scheduler-name=custom-scheduler","title":"what we want custom sheduling program."},{"location":"kubernetes/sheduler/#if-there-is-multiple-replicas-sheduler-only-one-is-active","text":"there is election process who will be leader where is parameter to aviod newly created schedulers to get leaders --lock-object-name=custom-scheduler","title":"if there is multiple replicas sheduler only one is active"},{"location":"kubernetes/sheduler/#get-logs","text":"kubectl logs custom-scheduler --name-space=kube-system","title":"get logs"},{"location":"kubernetes/sheduler/#add-taint-node","text":"kubectl taint nodes nodes01 spray=mortein:NoSchedule","title":"add taint node"},{"location":"kubernetes/sheduler/#remove-taint-node","text":"kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-","title":"remove taint node"},{"location":"kubernetes/sheduler/#add-label-to-node","text":"kubectl label node node01 color=blue","title":"add label to node"},{"location":"kubernetes/sheduler/#get-all-daemonsets","text":"kubectl get daemonsets --all-namespaces","title":"get all daemonsets"},{"location":"kubernetes/sheduler/#describe-daemon-shedulet-pods","text":"kubectl describe daemonset kube-proxy --namespace=kube-system","title":"describe daemon shedulet pods"},{"location":"kubernetes/sheduler/#create-daemonset-yaml","text":"kubectl create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml > fluentd.yaml","title":"create daemonset yaml"},{"location":"kubernetes/sheduler/#create-addidional-scheduler-from-file","text":"kubectl create -n kube-system configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml","title":"create addidional scheduler from file"},{"location":"programming/code_review/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 Are CI builds passing? If no, why? Is the code easily understood? Does the code work? Does it perform its intended function, the logic is correct, etc? Does the error handling work? Is memory usage acceptable, even with large inputs? Is code covered by functional or unit tests? Are error paths covered by functional or unit tests? All errors which are relatively easy to check must be checked: error conditions like \u201copen() failed after stat() was successfull\u201d or \u201carray size greater then INT_MAX\u201d may be ignored for being just as unlikely as uneasy to test, but otherwise having bugs in code which does error handling is way too common to be ignored. For new code, are unit tests written where needed? Are invalid parameter values handled where needed? Can any global/static variables be replaced? Are variables/functions named intuitively? Can any function attributes be used? Is there any redundant or duplicate code? Is the code modular enough? Can any of the code be replaced with library functions? Do loops have a set length and correct termination conditions? Can any logging or debugging code be removed? Are there any unneeded assert statements? Does the code conform to the style guide? Optimization that makes code harder to read should only be implemented if a profiler or other tool has indicated that the routine stands to gain from optimization. These kinds of optimizations should be well-documented and code that performs the same task simply should be preserved somewhere. Are return values being checked? Are there any use after frees? Are there any resource leaks? Memory leaks, unclosed sockets, etc. Are there any null pointer dereferences? Are any uninitialized variables used? Are there any cases of possible arithmetic overflow? Documentation 1 2 3 4 5 6 7 8 9 Are there any superfluous comments? Where needed, do comments exist and describe the intent of the code? Are any comments made outdated by the new code? Is any unusual behavior or edge-case handling described? Are complex algorithms explained and justified? Is code that depends on non-obvious behavior in external libraries documented with reference to external documentation? Is the use and function of API functions documented? Are data structures/typedefs explained? Is there any incomplete code, e.g., code marked TODO, FIXME, or XXX?","title":"Code Review"},{"location":"python/classes/classes/","text":"Much about Classes and Object Oriented Programming Classes are used to create new kinds of objects. Classes Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class AccountPortfolio : def __init__ ( self ): self . accounts = [] def add_account ( self , account ): self . accounts . append ( account ) def total_funds ( self ): return sum ( account . inquiry () for account in self . accounts ) def __len__ ( self ): return len ( self . accounts ) def __getitem__ ( self , index ): return self . accounts [ index ] def __iter__ ( self ): return iter ( self . accounts ) Usage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Example port = AccountPortfolio () port . add_account ( Account ( 'Guido' , 1000.0 )) port . add_account ( Account ( 'Eva' , 50.0 )) print ( port . total_funds ()) # -> 1050.0 len ( port ) # -> 2 # Print the accounts for account in port : print ( account ) # Access an individual account by index port [ 1 ] . inquiry () # -> 50.0 Avoiding Inheritance via Composition Inheritance 1 2 3 4 5 6 7 8 9 10 11 class Stack ( list ): def push ( self , item ): self . append ( item ) # Example s = Stack () s . push ( 1 ) s . push ( 2 ) s . push ( 3 ) s . pop () # -> 3 s . pop () # -> 2 Composition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Stack : def __init__ ( self ): self . _items = list () def push ( self , item ): self . _items . append ( item ) def pop ( self ): return self . _items . pop () def __len__ ( self ): return len ( self . _items ) # Example use s = Stack () s . push ( 1 ) s . push ( 2 ) s . push ( 3 ) s . pop () # -> 3 s . pop () # -> 2 Passing Container as Argument 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Stack : def __init__ ( self , * , container = None ): if container is None : container = list () self . _items = container def push ( self , item ): self . _items . append ( item ) def pop ( self ): return self . _items . pop () def __len__ ( self ): return len ( self . _items ) s = Stack ( container = array . array ( 'i' )) s . push ( 42 ) s . push ( 23 ) s . push ( 'a lot' ) # TypeError This is also an example of what\u2019s known as dependency injection . Instead of hardwiring Stack to depend on list, you can make it depend on any container a user decides to pass in, provided it implements the required interface. Avoid Inheritance via Functions If there is too much plumbing going on here. If you\u2019re writing a lot of single-method classes, consider using functions instead. Class Based Parsing 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class DataParser : def parse ( self , lines ): records = [] for line in lines : row = line . split ( ',' ) record = self . make_record ( row ) records . append ( row ) return records def make_record ( self , row ): raise NotImplementedError () class PortfolioDataParser ( DataParser ): def make_record ( self , row ): return { 'name' : row [ 0 ], 'shares' : int ( row [ 1 ]), 'price' : float ( row [ 2 ]) } parser = PortfolioDataParser () data = parser . parse ( open ( 'portfolio.csv' )) Function Based Parsing 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def parse_data ( lines , make_record ): records = [] for line in lines : row = line . split ( ',' ) record = make_record ( row ) records . append ( row ) return records def make_dict ( row ): return { 'name' : row [ 0 ], 'shares' : int ( row [ 1 ]), 'price' : float ( row [ 2 ]) } data = parse_data ( open ( 'portfolio.csv' ), make_dict ) Dynamic Binding and Duck Typing Dynamic binding is the runtime mechanism that Python uses to find the attributes of objects. This is what allows Python to work with instances without regard for their type. In Python, variable names do not have an associated type. Thus, the attribute binding process is independent of what kind of object obj is. If you make a lookup, such as obj.name , it will work on any obj whatsoever that happens to have a name attribute. This behavior is sometimes referred to as duck typing \u2014in reference to the adage \u201cif it looks like a duck, quacks like a duck, and walks like a duck, then it\u2019s a duck.\u201d Python programmers often write programs that rely on this behavior. For example, if you want to make a customized version of an existing object, you can either inherit from it, or you can create a completely new object that looks and acts like it but is otherwise unrelated. This latter approach is often used to maintain loose coupling of program components. For example, code may be written to work with any kind of object whatsoever as long as it has a certain set of methods. One of the most common examples is with various iterable objects defined in the standard library. There are all sorts of objects that work with the for loop to produce values\u2014lists, files, generators, strings, and so on. However, none of these inherit from any kind of special Iterable base class. They merely implement the methods required to perform iteration\u2014and it all works. Don't Inherit Builtin Types dict , list they are written in C and bypass __setitem__ and __getitem__ . If you want to use UserDict , import it like this: from collections import UserDict . Class Variables vs Methods In a class definition, all functions are assumed to operate on an instance, which is always passed as the first parameter self . However, the class itself is also an object that can carry state and be manipulated as well. As an example, you could track how many instances have been created using a class variable num_accounts : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Account : num_accounts = 0 def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance Account . num_accounts += 1 def __repr__ ( self ): return f ' { type ( self ) . __name__ } ( { self . owner !r} , { self . balance !r} )' def deposit ( self , amount ): self . balance += amount def withdraw ( self , amount ): self . deposit ( - amount ) # Must use self.deposit() def inquiry ( self ): return self . balance Class variables are defined outside the normal __init__() method. To modify them, use the class, not self . For example: 1 2 3 4 >>> a = Account ( 'Guido' , 1000.0 ) >>> b = Account ( 'Eva' , 10.0 ) >>> Account . num_accounts 2 classmethod Usage: Alternative Way of Creating a Class 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Account : def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance @classmethod def from_xml ( cls , data ): from xml.etree.ElementTree import XML doc = XML ( data ) return cls ( doc . findtext ( 'owner' ), float ( doc . findtext ( 'amount' ))) # Example use data = ''' <account> <owner>Guido</owner> <amount>1000.0</amount> </account> ''' a = Account . from_xml ( data ) Configuration of Classes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import time class Date : datefmt = ' {year} - {month:02d} - {day:02d} ' def __init__ ( self , year , month , day ): self . year = year self . month = month self . day = day def __str__ ( self ): return self . datefmt . format ( year = self . year , month = self . month , day = self . day ) @classmethod def from_timestamp ( cls , ts ): tm = time . localtime ( ts ) return cls ( tm . tm_year , tm . tm_mon , tm . tm_mday ) @classmethod def today ( cls ): return cls . from_timestamp ( time . time ()) This class features a class variable datefmt that adjusts output from the __str__() method. This is something that can be customized via inheritance: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class MDYDate ( Date ): datefmt = ' {month} / {day} / {year} ' class DMYDate ( Date ): datefmt = ' {day} / {month} / {year} ' # Example a = Date ( 1967 , 4 , 9 ) print ( a ) # 1967-04-09 b = MDYDate ( 1967 , 4 , 9 ) print ( b ) # 4/9/1967 c = DMYDate ( 1967 , 4 , 9 ) print ( c ) # 9/4/1967 dict.from_keys() Example 1 2 dict . from_keys ([ 'a' , 'b' , 'c' ], 0 ) # Output: {'a': 0, 'b': 0, 'c': 0} Static Methods 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class StandardPolicy : @staticmethod def deposit ( account , amount ): account . balance += amount @staticmethod def withdraw ( account , amount ): account . balance -= amount @staticmethod def inquiry ( account ): return account . balance class EvilPolicy ( StandardPolicy ): @staticmethod def deposit ( account , amount ): account . balance += 0.95 * amount @staticmethod def inquiry ( account ): if random . randint ( 0 , 4 ) == 1 : return 1.10 * account . balance else : return account . balance class Account : def __init__ ( self , owner , balance , * , policy = StandardPolicy ): self . owner = owner self . balance = balance self . policy = policy def __repr__ ( self ): return f 'Account( { self . policy } , { self . owner !r} , { self . balance !r} )' def deposit ( self , amount ): self . policy . deposit ( self , amount ) def withdraw ( self , amount ): self . policy . withdraw ( self , amount ) def inquiry ( self ): return self . policy . inquiry ( self ) About Design Patterns In writing object-oriented programs, programmers sometimes get fixated on implementing named design patterns\u2014such as the strategy pattern , the flyweight pattern , the singleton pattern , and so forth. Many of these originate from the famous Design Patterns book by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. If you are familiar with such patterns, the general design principles used in other languages can certainly be applied to Python. However, many of these documented patterns are aimed at working around specific issues that arise from the strict static type system of C++ or Java. The dynamic nature of Python renders a lot of these patterns obsolete, an overkill, or simply unnecessary. That said, there are a few overarching principles of writing good software\u2014such as striving to write code that is debuggable, testable, and extensible. Basic strategies such as writing classes with useful __repr__() methods, preferring composition over inheritance, and allowing dependency injection can go a long way towards these goals. Python programmers also like to work with code that can be said to be Pythonic . Usually, that means that objects obey various built-in protocols, such as iteration, containers, or context management. For example, instead of trying to implement some exotic data traversal pattern from a Java programming book, a Python programmer would probably implement it with a generator function feeding a for loop, or just replace the entire pattern with a few dictionary lookups. Properties As noted in the previous section, Python places no runtime restrictions on attribute values or types. However, such enforcement is possible if you put an attribute under the management of a so-called property . A property is a special kind of attribute that intercepts attribute access and handles it via user-defined methods. These methods have complete freedom to manage the attribute as they see fit. Here is an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import string class Account : def __init__ ( self , owner , balance ): self . owner = owner self . _balance = balance @property def owner ( self ): return self . _owner @owner . setter def owner ( self , value ): if not isinstance ( value , str ): raise TypeError ( 'Expected str' ) if not all ( c in string . ascii_uppercase for c in value ): raise ValueError ( 'Must be uppercase ASCII' ) if len ( value ) > 10 : raise ValueError ( 'Must be 10 characters or less' ) self . _owner = value class SomeClass : @property def attr ( self ): print ( 'Getting' ) @attr . setter def attr ( self , value ): print ( 'Setting' , value ) @attr . deleter def attr ( self ): print ( 'Deleting' ) # Example s = SomeClass () s . attr # Getting s . attr = 13 # Setting del s . attr # Deleting Types, Interfaces, Abstract Classes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class A : pass class B ( A ): pass class C : pass a = A () # Instance of 'A' b = B () # Instance of 'B' c = C () # Instance of 'C' type ( a ) # Returns the class object A isinstance ( a , A ) # Returns True isinstance ( b , A ) # Returns True, B derives from A isinstance ( b , C ) # Returns False, B not derived from C Note : ABC must be implemented. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from abc import ABC , abstractmethod class Stream ( ABC ): @abstractmethod def receive ( self ): pass @abstractmethod def send ( self , msg ): pass @abstractmethod def close ( self ): pass Multiple Inheritance and Mixins Interfaces using ABC Classes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from abc import ABC , abstractmethod class Stream ( ABC ): @abstractmethod def receive ( self ): pass @abstractmethod def send ( self , msg ): pass @abstractmethod def close ( self ): pass class Iterable ( ABC ): @abstractmethod def __iter__ ( self ): pass class MessageStream ( Stream , Iterable ): def receive ( self ): ... def send ( self ): ... def close ( self ): ... def __iter__ ( self ): ... m = MessageStream() isinstance(m, Stream) # -> True isinstance(m, Iterable) # -> True Mixin Classes The other use of multiple inheritance is to define mixin classes. A mixin class is a class that modifies or extends the functionality of other classes. Consider the following class definitions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Duck : def noise ( self ): return 'Quack' def waddle ( self ): return 'Waddle' class Trombonist : def noise ( self ): return 'Blat!' def march ( self ): return 'Clomp' class Cyclist : def noise ( self ): return 'On your left!' def pedal ( self ): return 'Pedaling' These classes are completely unrelated to each other. There is no inheritance relationship and they implement different methods. However, there is a shared commonality in that they each define a noise() method. Using that as a guide, you could define the following classes: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class LoudMixin : def noise ( self ): return super () . noise () . upper () class AnnoyingMixin : def noise ( self ): return 3 * super () . noise () class LoudDuck ( LoudMixin , Duck ): pass class AnnoyingTrombonist ( AnnoyingMixin , Trombonist ): pass class AnnoyingLoudCyclist ( AnnoyingMixin , LoudMixin , Cyclist ): pass d = LoudDuck() d.noise() # -> 'QUACK' t = AnnoyingTrombonist() t.noise() # -> 'Blat!Blat!Blat!' c = AnnoyingLoudCyclist() c.noise() # -> 'ON YOUR LEFT!ON YOUR LEFT!ON YOUR LEFT!' Since mixin classes are defined in the same way as normal classes, it is best to include the word \"Mixin\" as part of the class name. This naming convention provides a greater clarity of purpose. To fully understand mixins, you need to know a bit more about how inheritance and the super() function work. First, whenever you use inheritance, Python builds a linear chain of classes known as the Method Resolution Order, or MRO for short. This is available as the mro attribute on a class. Here are some examples for single inheritance: 1 2 3 4 5 6 7 8 9 10 11 12 class Base : pass class A ( Base ): pass class B ( A ): pass Base . __mro__ # -> (<class 'Base'>, <class 'object'>) A . __mro__ # -> (<class 'A'>, <class 'Base'>, <class 'object'>) B . __mro__ # -> (<class 'B'>, <class 'A'>, <class 'Base'>, <class 'object'>) Class Decorators and Class Methods Factory function that uses the registry 1 2 def create_decoder ( mimetype ): return _registry [ mimetype ]() 1 2 3 4 5 @register_decoder class TextDecoder : mimetypes = [ 'text/plain' ] def decode ( self , data ): ... 1 2 3 4 5 @register_decoder class HTMLDecoder : mimetypes = [ 'text/html' ] def decode ( self , data ): ... 1 2 3 4 5 @register_decoder class ImageDecoder : mimetypes = [ 'image/png' , 'image/jpg' , 'image/gif' ] def decode ( self , data ): ... Example usage 1 decoder = create_decoder ( 'image/jpg' ) A class decorator is free to modify the contents of the class it\u2019s given. For example, it might even rewrite existing methods. This is a common alternative to mixin classes or multiple inheritance. For example, consider these decorators: decorator override function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def loud ( cls ): orig_noise = cls . noise def noise ( self ): return orig_noise ( self ) . upper () cls . noise = noise return cls def annoying ( cls ): orig_noise = cls . noise def noise ( self ): return 3 * orig_noise ( self ) cls . noise = noise return cls @annoying @loud class Cyclist ( object ): def noise ( self ): return 'On your left!' def pedal ( self ): return 'Pedaling' Add code to class at runtime 1 2 3 4 5 6 7 class Point : def __init__ ( self , x , y ): self . x = x self . y = y def __repr__ ( self ): return f ' { type ( self ) . __name__ } ( { self . x !r} , { self . y !r} )' Writing such methods is often annoying. Perhaps a class decorator could create the method for you? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import inspect def with_repr ( cls ): args = list ( inspect . signature ( cls ) . parameters ) argvals = ', ' . join ( '{self. %s !r}' % arg for arg in args ) code = 'def __repr__(self): \\n ' code += f ' return f' { cls . __name__ }({ argvals }) ' \\n ' locs = { } exec ( code , locs ) cls . __repr__ = locs [ '__repr__' ] return cls # Example @with_repr class Point : def __init__ ( self , x , y ): self . x = x self . y = y In this example, a repr() method is generated from the calling signature of the init() method. The method is created as a text string and passed to exec() to create a function. That function is attached to the class. Similar code generation techniques are used in parts of the standard library. For example, a convenient way to define data structures is to use a dataclass: 1 2 3 4 5 6 from dataclasses import dataclass @dataclass class Point : x : int y : int A dataclass automatically creates methods such as init () and repr () from class type hints. The methods are created using exec(), similarly to the prior example. Here\u2019s how the resulting Point class works: 1 2 p = Point ( 2 , 3 ) p Output: 1 Point(x=2, y=3) One downside of such an approach is poor startup performance. Dynamically creating code with exec() bypasses the compilation optimizations that Python normally applies to modules. Defining a large number of classes in this way may therefore significantly slow down the importing of your code. Supervised Inheritance - __init_subclass__ As you saw in the previous section, sometimes you want to define a class and perform additional actions. A class decorator is one mechanism for doing this. However, a parent class can also perform extra actions on behalf of its subclasses. This is accomplished by implementing an __init_subclass__(cls) class method. For example: 1 2 3 4 5 6 7 8 9 10 11 class Base : @classmethod def __init_subclass__ ( cls ): print ( 'Initializing' , cls ) # Example (should see 'Initializing' message for each class) class A ( Base ): pass class B ( A ): pass If an __init_subclass__() method is present, it is triggered automatically upon the definition of any child class. This happens even if the child is buried deeply in an inheritance hierarchy. Many of the tasks commonly performed with class decorators can be performed with __init_subclass__() instead. For example, class registration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class DecoderBase : _registry = { } @classmethod def __init_subclass__ ( cls ): for mt in cls . mimetypes : DecoderBase . _registry [ mt . mimetype ] = cls # Factory function that uses the registry def create_decoder ( mimetype ): return DecoderBase . _registry [ mimetype ]() class TextDecoder ( DecoderBase ): mimetypes = [ 'text/plain' ] def decode ( self , data ): ... class HTMLDecoder ( DecoderBase ): mimetypes = [ 'text/html' ] def decode ( self , data ): ... class ImageDecoder ( DecoderBase ): mimetypes = [ 'image/png' , 'image/jpg' , 'image/gif' ] def decode ( self , data ): ... Class Initialization with __repr__ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import inspect class Base : @classmethod def __init_subclass__ ( cls ): # Create a __repr__ method args = list ( inspect . signature ( cls ) . parameters ) argvals = ', ' . join ( '{self. %s !r}' % arg for arg in args ) code = 'def __repr__(self): \\n ' code += f ' return f' { cls . __name__ }({ argvals }) ' \\n ' locs = { } exec ( code , locs ) cls . __repr__ = locs [ '__repr__' ] class Point ( Base ): def __init__ ( self , x , y ): self . x = x self . y = y If multiple inheritance is being used, you should use super() to make sure all classes that implement __init_subclass__() get called. For example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class A : @classmethod def __init_subclass__ ( cls ): print ( 'A.init_subclass' ) super () . __init_subclass__ () class B : @classmethod def __init_subclass__ ( cls ): print ( 'B.init_subclass' ) super () . __init_subclass__ () # Should see output from both classes here class C ( A , B ): pass Object Life Cycle and Memory Management When a class is defined, the resulting class is a factory for creating new instances. For example: 1 2 3 4 class Account : def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance Create some Account instances: 1 2 a = Account ( 'Guido' , 1000.0 ) b = Account ( 'Eva' , 25.0 ) The creation of an instance is carried out in two steps using the special method __new__() that creates a new instance and __init__() that initializes it. For example, the operation a = Account('Guido', 1000.0) performs these steps: 1 2 3 a = Account . __new__ ( Account , 'Guido' , 1000.0 ) if isinstance ( a , Account ): Account . __init__ ( a , 'Guido' , 1000.0 ) Except for the first argument which is the class instead of an instance, __new__() normally receives the same arguments as __init__() . However, the default implementation of __new__() just ignores them. You\u2019ll sometimes see __new__() invoked with just a single argument. For example, this code also works: 1 2 3 a = Account . __new__ ( Account ) if isinstance ( a , Account ): Account . __init__ ( a , 'Guido' , 1000.0 ) Direct use of the __new__() method is uncommon, but sometimes it\u2019s used to create instances while bypassing the invocation of the __init__() method. One such use is in class methods. 1 2 3 4 class Spam : @classmethod def bar ( cls , * args , ** kwargs ): return cls . __new__ ( cls , * args , ** kwargs ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import time class Date : def __init__ ( self , year , month , day ): self . year = year self . month = month self . day = day @classmethod def today ( cls ): t = time . localtime () self = cls . __new__ ( cls ) # Make instance self . year = t . tm_year self . month = t . tm_month self . day = t . tm_day return self Modules that perform object serialization such as pickle also utilize new() to recreate instances when objects are deserialized. This is done without ever invoking init() . Sometimes a class will define new() if it wants to alter some aspect of instance creation. Typical applications include instance caching, singletons, and immutability. As an example, you might want Date class to perform date interning\u2014that is, caching and reusing Date instances that have an identical year, month, and day. Here is one way that might be implemented: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Date : _cache = { } @staticmethod def __new__ ( cls , year , month , day ): self = Date . _cache . get (( year , month , day )) if not self : self = super () . __new__ ( cls ) self . year = year self . month = month self . day = day Date . _cache [ year , month , day ] = self return self def __init__ ( self , year , month , day ): pass In this example, the class keeps an internal dictionary of previously created Date instances. When creating a new Date , the cache is consulted first. If a match is found, that instance is returned. Otherwise, a new instance is created and initialized. A subtle detail of this solution is the empty init() method. Even though instances are cached, every call to Date() still invokes init() . To avoid duplicated effort, the method simply does nothing\u2014instance creation actually takes place in new() when an instance is created the first time. There are ways to avoid the extra call to init() but it requires sneaky tricks. One way to avoid it is to have new() return an entirely different type instance\u2014for example, one belonging to a different class. Another solution, described later, is to use a metaclass. Once created, instances are managed by reference counting. If the reference count reaches zero, the instance is immediately destroyed. When the instance is about to be destroyed, the interpreter first looks for a del() method associated with the object and calls it. For example: 1 2 3 4 5 6 7 class Account ( object ): def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance def __del__ ( self ): print ( 'Deleting Account' ) Occasionally, a program will use the del statement to delete a reference to an object as shown. If this causes the reference count of the object to reach zero, the del() method is called. However, in general, the del statement doesn\u2019t directly call del() because there may be other object references living elsewhere. There are many other ways that an object might be deleted\u2014for example, reassignment of a variable name or a variable going out of scope in a function: 1 2 3 4 5 6 7 8 >>> a = Account ( 'Guido' , 1000.0 ) >>> a = 42 Deleting Account >>> def func (): ... a = Account ( 'Guido' , 1000.0 ) ... >>> func () Deleting Account In practice, it\u2019s rarely necessary for a class to define a del() method. The only exception is when the destruction of an object requires an extra cleanup action\u2014such as closing a file, shutting down a network connection, or releasing other system resources. Even in these cases, it\u2019s dangerous to rely on del() for a proper shutdown because there\u2019s no guarantee that this method will be called when you think it would. For clean shutdown of resources, you should give the object an explicit close() method. You should also make your class support the context manager protocol so it can be used with the with statement. Here is an example that covers all of the cases: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class SomeClass : def __init__ ( self ): self . resource = open_resource () def __del__ ( self ): self . close () def close ( self ): self . resource . close () def __enter__ ( self ): return self def __exit__ ( self , ty , val , tb ): self . close () # Closed via __del__() s = SomeClass () del s # Explicit close s = SomeClass () s . close () # Closed at the end of a context block with SomeClass () as s : ... Again, it should be emphasized that writing a del() in a class is almost never necessary. Python already has garbage collection and there is simply no need to do it unless there is some extra action that needs to take place upon object destruction. Even then, you still might not need del() as it\u2019s possible that the object is already programmed to clean itself up properly even if you do nothing. As if there weren\u2019t enough dangers with reference counting and object destruction, there are certain kinds of programming patterns\u2014especially those involving parent-child relationships, graphs, or caching\u2014where objects can create a so-called reference cycle. 1 2 3 4 5 6 class SomeClass : def __del__ ( self ): print ( 'Deleting' ) parent = SomeClass () child = SomeClass () Create a child-parent reference cycle parent.child = child child.parent = parent Try deletion (no output from del appears) del parent del child In this example, the variable names are destroyed but you never see execution of the del() method. The two objects each hold internal references to each other, so there\u2019s no way for the reference count to ever drop to 0. To handle this, a special cycle-detecting garbage collector runs every so often. Eventually the objects will be reclaimed, but it\u2019s hard to predict when this might happen. If you want to force garbage collection, you can call gc.collect() . The gc module has a variety of other functions related to the cyclic garbage collector and monitoring memory. Because of the unpredictable timing of garbage collection, the del() method has a few restrictions placed on it. First, any exception that propagates out of del() is printed to sys.stderr , but otherwise ignored. Second, the del() method should avoid operations such as acquiring locks or other resources. Doing so could result in a deadlock when del() is unexpectedly fired in the middle of executing an unrelated function within the seventh inner callback circle of signal handling and threads. If you must define del() , keep it simple. weak references Sometimes objects are kept alive when you\u2019d much rather see them die. In an earlier example, a Date class was shown with internal caching of instances. One problem with this implementation is that there is no way for an instance to ever be removed from the cache. As such, the cache will grow larger and larger over time. One way to fix this problem is to create a weak reference using the weakref module. A weak reference is a way of creating a reference to an object without increasing its reference count. To work with a weak reference, you have to add an extra bit of code to check if the object being referred to still exists. Here\u2019s an example of how you create a weakref: 1 2 import weakref a_ref = weakref . ref ( a ) In this example, a_ref is a weak reference to the object a . You can use the weak reference to access the object, but it doesn't prevent the object from being garbage collected. 1 2 3 4 a = Account ( 'Guido' , 1000.0 ) import weakref a_ref = weakref . ref ( a ) a_ref () Support for weak references requires instances to have a mutable weakref attribute. Instances of user-defined classes normally have such an attribute by default. However, built-in types and certain kinds of special data structures\u2014named tuples, classes with slots\u2014do not. If you want to construct weak references to these types, you can do it by defining variants with a weakref attribute added: 1 2 3 4 5 class wdict ( dict ): __slots__ = ( '__weakref__' ,) w = wdict () w_ref = weakref . ref ( w ) # Now works attribute binding The state associated with an instance is stored in a dictionary that\u2019s accessible as the instance\u2019s __dict__ attribute. This dictionary contains the data that\u2019s unique to each instance. Classes are linked to their base classes by a special attribute __bases__ , which is a tuple of the base classes. The __bases__ attribute is only informational. The actual runtime implementation of inheritance uses the __mro__ attribute which is a tuple of all parent classes listed in search order. This underlying structure is the basis for all operations that get, set, or delete the attributes of instances. Whenever an attribute is set using obj.name = value , the special method obj.__setattr__('name', value) is invoked. If an attribute is deleted using del obj.name , the special method obj.__delattr__('name') is invoked. The default behavior of these methods is to modify or remove values from the local __dict__ of obj unless the requested attribute happens to correspond to a property or descriptor. In that case, the set and delete operations will be carried out by the __set__ and __delete__ functions associated with the property. For attribute lookup such as obj.name , the special method obj.__getattribute__('name') is invoked. This method carries out the search for the attribute, which normally includes checking the properties, looking in the local __dict__ , checking the class dictionary, and searching the MRO. If this search fails, a final attempt to find the attribute is made by invoking the obj.__getattr__('name') method of the class (if defined). If this fails, an AttributeError exception is raised. User-defined classes can implement their own versions of the attribute access functions, if desired. For example, here\u2019s a class that restricts the attribute names that can be set: 1 2 3 4 5 6 7 8 9 class Account : def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance def __setattr__ ( self , name , value ): if name not in { 'owner' , 'balance' }: raise AttributeError ( f 'No attribute { name } ' ) super () . __setattr__ ( name , value ) Example a = Account('Guido', 1000.0) a.balance = 940.25 # Ok a.amount = 540.2 # AttributeError. No attribute amount proxies, wrappers, delegations A common implementation technique for proxies involves the getattr() method. Here is a simple example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class A : def spam ( self ): print ( 'A.spam' ) def grok ( self ): print ( 'A.grok' ) def yow ( self ): print ( 'A.yow' ) class LoggedA : def __init__ ( self ): self . _a = A () def __getattr__ ( self , name ): print ( 'Accessing' , name ) # Delegate to internal A instance return getattr ( self . _a , name ) In this example, the LoggedA class acts as a proxy for class A . When an attribute is accessed on an instance of LoggedA , the __getattr__() method is invoked. It prints the accessed attribute name and then delegates the attribute access to the internal instance of A . Example use: 1 2 3 a = LoggedA () a . spam () # prints 'Accessing spam' and 'A.spam' a . yow () # prints 'Accessing yow' and 'A.yow' Delegation is sometimes used as an alternative to inheritance. Here is an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class A : def spam ( self ): print ( 'A.spam' ) def grok ( self ): print ( 'A.grok' ) def yow ( self ): print ( 'A.yow' ) class B : def __init__ ( self ): self . _a = A () def grok ( self ): print ( 'B.grok' ) def __getattr__ ( self , name ): return getattr ( self . _a , name ) In this example, class B holds an internal reference to an instance of A and delegates attribute access to it. Methods defined in class B override the corresponding methods in class A , while all other methods are delegated to the internal instance of A . Example use: 1 2 3 4 b = B () b . spam () # -> A.spam b . grok () # -> B.grok (redefined method) b . yow () # -> A.yow The technique of forwarding attribute lookup via __getattr__() is a common technique. However, be aware that it does not apply to operations mapped to special methods. For example, consider this class: 1 2 3 4 5 6 class ListLike : def __init__ ( self ): self . _items = list () def __getattr__ ( self , name ): return getattr ( self . _items , name ) In this example, the ListLike class forwards all of the standard list methods to an inner list using __getattr__() . However, operations such as len(a) or a[0] fail because they are not mapped to special methods ( __len__() and __getitem__() ). To make those work, you would have to explicitly implement the required special methods. To illustrate, here's an updated ListLike class that implements the necessary special methods: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class ListLike : def __init__ ( self ): self . _items = list () def __getattr__ ( self , name ): return getattr ( self . _items , name ) def __len__ ( self ): return len ( self . _items ) def __getitem__ ( self , index ): return self . _items [ index ] def __setitem__ ( self , index , value ): self . _items [ index ] = value slots The slots attribute is a definition hint that allows Python to make performance optimizations for both memory use and execution speed. It eliminates the need for a dictionary to store instance data and uses a more compact array-based data structure instead. Using slots can result in a substantial reduction in memory use and a modest improvement in execution time, especially in programs that create a large number of objects. Here are some key points about slots : The slots attribute lists only the instance attributes and does not include methods, properties, class variables, or any other class-level attributes. If a class uses slots , any derived class must also define slots (even if empty) to take advantage of the benefits. Failure to do so will result in slower performance and increased memory usage. slots is not compatible with multiple inheritance. If multiple base classes with non-empty slots are specified, a TypeError will be raised. Code that relies on the underlying __dict__ attribute of instances may break when slots is used. slots has no effect on the invocation of methods such as __getattribute__() , __getattr__() , and __setattr__() if they are redefined in a class. However, the absence of the instance __dict__ attribute should be considered when implementing these methods. Descriptors Descriptors provide a way to customize attribute access in Python by implementing the special methods __get__() , __set__() , and __delete__() . They are class-level objects that manage access to attributes. Properties are implemented using descriptors. Here's an example of a descriptor class called Typed : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Typed : expected_type = object def __set_name__ ( self , cls , name ): self . key = name def __get__ ( self , instance , cls ): if instance : return instance . __dict__ [ self . key ] else : return self def __set__ ( self , instance , value ): if not isinstance ( value , self . expected_type ): raise TypeError ( f 'Expected { self . expected_type } ' ) instance . __dict__ [ self . key ] = value def __delete__ ( self , instance ): raise AttributeError ( \"Can't delete attribute\" ) In this example, the Typed class defines a descriptor that performs type checking when an attribute is assigned and raises an error if an attempt is made to delete the attribute. Subclasses like Integer , Float , and String specialize Typed to match specific types. Descriptors are used by including them as class attributes in another class. For example: 1 2 3 4 5 6 7 class Account : owner = String () balance = Float () def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance In this case, the Account class uses the descriptors String and Float to automatically call the appropriate __get__() , __set__() , or __delete__() methods when accessing the owner and balance attributes. Descriptors take precedence over items in the instance dictionary. Even if an instance dictionary has a matching entry, the descriptor's __set__() method will be invoked. For example: 1 2 a = Account ( 'Guido' , 1000.0 ) a . balance = 'a lot' # Raises TypeError: Expected <class 'float'> The __get__(instance, cls) method of a descriptor takes arguments for both the instance and the class. When invoked at the class level, the instance argument is None . The __get__() method typically returns the descriptor itself if no instance is provided. 1 Account . balance # Returns <__main__.Float object at 0x110606710> Method Descriptor A descriptor that only implements __get__() is known as a method descriptor. It is mainly used to implement Python's various types of methods, such as instance methods, class methods, and static methods. The __get__() method of a method descriptor only gets invoked if there is no matching entry in the instance dictionary. Here's an example of implementing @classmethod and @staticmethod using method descriptors: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import types class classmethod : def __init__ ( self , func ): self . __func__ = func def __get__ ( self , instance , cls ): return types . MethodType ( self . __func__ , cls ) class staticmethod : def __init__ ( self , func ): self . __func__ = func def __get__ ( self , instance , cls ): return self . __func__ Lazy Evaluation Method descriptors can be used to implement lazy evaluation of attributes. By only computing and assigning the attribute value when it is accessed for the first time, we can save computational resources. Here's an example of implementing lazy evaluation using a descriptor called Lazy : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Lazy : def __init__ ( self , func ): self . func = func def __set_name__ ( self , cls , name ): self . key = name def __get__ ( self , instance , cls ): if instance : value = self . func ( instance ) instance . __dict__ [ self . key ] = value return value else : return self In this example, the Lazy descriptor is used in the Rectangle class to lazily compute the area and perimeter attributes: 1 2 3 4 5 6 7 class Rectangle : def __init__ ( self , width , height ): self . width = width self . height = height area = Lazy ( lambda self : self . width * self . height ) perimeter = Lazy ( lambda self : 2 * self . width + 2 * self . height ) When the area or perimeter attributes are accessed for the first time, the corresponding lambda function is executed to compute the value. The computed value is then stored in the instance's __dict__ attribute for future use. Class Definitions The definition of a class is a dynamic process. When you define a class using the class statement, a new dictionary is created that serves as the local class namespace. The body of the class then executes as a script within this namespace. Eventually, the namespace becomes the __dict__ attribute of the resulting class object. Any legal Python statement is allowed in the body of a class. Normally, you just define functions and variables, but control flow, imports, nested classes, and everything else is allowed. For example, here is a class that conditionally defines methods: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Account : def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance if debug : import logging log = logging . getLogger ( f ' { __module__ } . { __qualname__ } ' ) def deposit ( self , amount ): Account . log . debug ( 'Depositing %f ' , amount ) self . balance += amount def withdraw ( self , amount ): Account . log . debug ( 'Withdrawing %f ' , amount ) self . balance -= amount else : def deposit ( self , amount ): self . balance += amount def withdraw ( self , amount ): self . balance -= amount In this example, a global variable debug is being used to conditionally define methods. The __module__ and __qualname__ variables are predefined strings that hold information about the class name and enclosing module. These can be used by statements in the class body. In this example, they're being used to configure the logging system. There are probably cleaner ways of organizing the above code, but the key point is that you can put anything you want in a class. One critical point about class definition is that the namespace used to hold the contents of the class body is not a scope of variables. Any name that gets used within a method (such as Account.log in the above example) needs to be fully qualified. If a function like locals() is used in a class body (but not inside a method), it returns the dictionary being used for the class namespace. Dynamic Class Creation Normally, classes are created using the class statement, but this is not a requirement. As noted in the previous section, classes are defined by executing the body of a class to populate a namespace. If you're able to populate a dictionary with your own definitions, you can make a class without ever using the class statement. To do that, use types.new_class() : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import types # Some methods (not in a class) def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance def deposit ( self , amount ): self . balance -= amount def withdraw ( self , amount ): self . balance += amount methods = { '__init__' : __init__ , 'deposit' : deposit , 'withdraw' : withdraw , } Account = types . new_class ( 'Account' , (), exec_body = lambda ns : ns . update ( methods )) # You now have a class a = Account ( 'Guido' , 1000.0 ) a . deposit ( 50 ) a . withdraw ( 25 ) Dynamic class creation may be useful if you want to create classes from data structures or generate classes programmatically. For example, in the section on descriptors, the following classes were defined: 1 2 3 4 5 6 7 8 class Integer ( Typed ): expected_type = int class Float ( Typed ): expected_type = float class String ( Typed ): expected_type = str This code is highly repetitive. A data-driven approach can be used to generate the classes dynamically: 1 2 3 4 5 6 7 8 9 10 11 12 typed_classes = [ ( 'Integer' , int ), ( 'Float' , float ), ( 'String' , str ), ( 'Bool' , bool ), ( 'Tuple' , tuple ), ] globals () . update ( ( name , types . new_class ( name , ( Typed ,), exec_body = lambda ns : ns . update ( expected_type = ty ))) for name , ty in typed_classes ) In this example, the global module namespace is being updated with dynamically created classes using types.new_class() . The typed_classes list defines the names and expected types for each class. Each class is created by calling types.new_class() with the class name, base classes, and an exec_body function that updates the namespace with the expected type. The resulting classes are then added to the global namespace using globals().update() . Sometimes you will see type() being used to dynamically create a class instead. For example: 1 Account = type ( 'Account' , (), methods ) This works, but it doesn\u2019t take into account some of the more advanced class machinery such as metaclasses. In modern code, try to use types.new_class() instead. Metaclasses When you define a class in Python, the class definition itself becomes an object. Here's an example: 1 2 3 4 5 6 7 8 9 10 class Account : def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance def deposit ( self , amount ): self . balance += amount def withdraw ( self , amount ): self . balance -= amount To check if Account is an object, you can use the isinstance function: 1 isinstance ( Account , object ) If you think about this long enough, you will realize that if Account is an object, then something had to create it. This creation of the class object is controlled by a special kind of class called a metaclass. Simply put, a metaclass is a class that creates instances of classes. In the preceding example, the metaclass that created Account is a built-in class called type . In fact, if you check the type of Account , you will see that it is an instance of type : 1 type ( Account ) It's a bit brain-bending, but it's similar to integers. For example, if you write x = 42 and then look at x.__class__ , you'll get int , which is the class that creates integers. Similarly, type makes instances of types or classes. When a new class is defined with the class statement, a number of things happen. First, a new namespace for the class is created. Next, the body of the class is executed in this namespace. Finally, the class name, base classes, and populated namespace are used to create the class instance. The following code illustrates the low-level steps that take place: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 namespace = type . __prepare__ ( 'Account' , ()) # Step 2: Execute the class body exec ( ''' def __init__(self, owner, balance): self.owner = owner self.balance = balance def deposit(self, amount): self.balance += amount def withdraw(self, amount): self.balance -= amount ''' , globals (), namespace ) # Step 3: Create the final class object Account = type ( 'Account' , (), namespace ) In the definition process, there is interaction with the type class to create the class namespace and to create the final class object. The choice of using type can be customized - a class can choose to be processed by a different metaclass by specifying a different metaclass. This is done by using the metaclass keyword argument in inheritance: 1 class Account ( metaclass = type ): If no metaclass is given, the class statement examines the type of the first entry in the tuple of base classes (if any) and uses that as the metaclass. Therefore, if you write class Account(object) , the resulting Account class will have the same type as object (which is type ). Note that classes that don't specify any parent at all always inherit from object , so this still applies. To create a new metaclass, define a class that inherits from type . Within this class, you can redefine one or more methods that are used during the class creation process. Typically, this includes the __prepare__() method used to create the class namespace, the __new__() method used to create the class instance, the __init__() method called after a class has already been created, and the __call__() method used to create new instances. The following example implements a metaclass that merely prints the input arguments to each method so you can experiment: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class mytype ( type ): # Creates the class namespace @classmethod def __prepare__ ( meta , clsname , bases ): print ( 'Preparing:' , clsname , bases ) return super () . __prepare__ ( clsname , bases ) # Creates the class instance after body has executed @staticmethod def __new__ ( meta , clsname , bases , namespace ): print ( 'Creating:' , clsname , bases , namespace ) return super () . __new__ ( meta , clsname , bases , namespace ) # Initializes the class instance def __init__ ( cls , clsname , bases , namespace ): print ( 'Initializing:' , clsname , bases , namespace ) super () . __init__ ( clsname , bases , namespace ) # Creates new instances of the class def __call__ ( cls , * args , ** kwargs ): print ( 'Creating instance:' , args , kwargs ) return super () . __call__ ( * args , ** kwargs ) Example 1 2 class Base ( metaclass = mytype ): pass The definition of the Base produces the following output: 1 2 3 # Preparing: Base () # Creating: Base () {'__module__': '__main__', '__qualname__': 'Base'} # Initializing: Base () {'__module__': '__main__', '__qualname__': 'Base'} 1 b = Base () Creating instance: () . One tricky facet of working with metaclasses is the naming of variables and keeping track of the various entities involved. In the above code, the meta name refers to the metaclass itself. The cls name refers to a class instance created by the metaclass. Although not used here, the self name refers to a normal instance created by a class. Metaclasses propagate via inheritance. So, if you've defined a base class to use a different metaclass, all child classes will also use that metaclass. Try this example to see your custom metaclass at work: 1 2 3 4 5 6 7 8 9 10 11 12 class Account ( Base ): def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance def deposit ( self , amount ): self . balance += amount def withdraw ( self , amount ): self . balance -= amount print ( type ( Account )) # -> <class 'mytype'> The primary use of metaclasses is in situations where you want to exert extreme low-level control over the class definition environment and creation process. Before proceeding, however, remember that Python already provides a lot of functionality for monitoring and altering class definitions (such as the __init_subclass__() method, class decorators, descriptors, mixins, and so on). Most of the time, you probably don't need a metaclass. That said, the next few examples show situations where a metaclass provides the only sensible solution. One use of a metaclass is in rewriting the contents of the class namespace prior to the creation of the class object. Certain features of classes are established at definition time and can't be modified later. One such feature is __slots__ . As noted earlier, __slots__ is a performance optimization related to the memory layout of instances. Here's a metaclass that automatically sets the __slots__ attribute based on the calling signature of the __init__() method. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import inspect class SlotMeta ( type ): @staticmethod def __new__ ( meta , clsname , bases , methods ): if '__init__' in methods : sig = inspect . signature ( methods [ '__init__' ]) __slots__ = tuple ( sig . parameters )[ 1 :] else : __slots__ = () methods [ '__slots__' ] = __slots__ return super () . __new__ ( meta , clsname , bases , methods ) class Base ( metaclass = SlotMeta ): pass Example 1 2 3 4 class Point ( Base ): def __init__ ( self , x , y ): self . x = x self . y = y In this example, the Point class that's created is automatically created with slots of ('x', 'y') . The resulting instances of Point now get memory savings without knowing that slots are being used. It doesn't have to be specified directly. This kind of trick is not possible with class decorators or with init_subclass() because those features only operate on a class after it's been created. By then, it's too late to apply the slots optimization. Another use of metaclasses is for altering the class definition environment. For example, duplicate definitions of a name during class definition normally result in a silent error - the second definition overwrites the first. Suppose you wanted to catch that. Here's a metaclass that does that by defining a different kind of dictionary for the class namespace: 1 2 3 4 5 6 7 8 9 10 11 12 13 class NoDupeDict ( dict ): def __setitem__ ( self , key , value ): if key in self : raise AttributeError ( f ' { key } already defined' ) super () . __setitem__ ( key , value ) class NoDupeMeta ( type ): @classmethod def __prepare__ ( meta , clsname , bases ): return NoDupeDict () class Base ( metaclass = NoDupeMeta ): pass Example 1 2 3 4 5 6 class SomeClass ( Base ): def yow ( self ): print ( 'Yow!' ) def yow ( self , x ): # Fails. Already defined print ( 'Different Yow!' ) This is only a small sample of what's possible. For framework builders, metaclasses offer an opportunity to tightly control what happens during class definition - allowing classes to serve as a kind of domain-specific language. Historically, metaclasses have been used to accomplish a variety of tasks that are now possible through other means. The init_subclass() method, in particular, can be used to address a wide variety of use cases where metaclasses were once applied. This includes registration of classes with a central registry, automatic decoration of methods, and code generation. Built-in Objects for Instances and Classes Attribute Description cls.__name__ Class name cls.__module__ Module name in which the class is defined cls.__qualname__ Fully qualified class name cls.__bases__ Tuple of base classes cls.__mro__ Method Resolution Order tuple cls.__dict__ Dictionary holding class methods and variables cls.__doc__ Documentation string cls.__annotations__ Dictionary of class type hints cls.__abstractmethods__ Set of abstract method names (may be undefined if there aren't any) Attribute Description i.__class__ Class to which the instance belongs i.__dict__ Dictionary holding instance data (if defined)","title":"Classes"},{"location":"python/classes/classes/#much-about-classes-and-object-oriented-programming","text":"Classes are used to create new kinds of objects.","title":"Much about Classes and Object Oriented Programming"},{"location":"python/classes/classes/#classes-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class AccountPortfolio : def __init__ ( self ): self . accounts = [] def add_account ( self , account ): self . accounts . append ( account ) def total_funds ( self ): return sum ( account . inquiry () for account in self . accounts ) def __len__ ( self ): return len ( self . accounts ) def __getitem__ ( self , index ): return self . accounts [ index ] def __iter__ ( self ): return iter ( self . accounts )","title":"Classes Example"},{"location":"python/classes/classes/#usage","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Example port = AccountPortfolio () port . add_account ( Account ( 'Guido' , 1000.0 )) port . add_account ( Account ( 'Eva' , 50.0 )) print ( port . total_funds ()) # -> 1050.0 len ( port ) # -> 2 # Print the accounts for account in port : print ( account ) # Access an individual account by index port [ 1 ] . inquiry () # -> 50.0","title":"Usage"},{"location":"python/classes/classes/#avoiding-inheritance-via-composition","text":"","title":"Avoiding Inheritance via Composition"},{"location":"python/classes/classes/#inheritance","text":"1 2 3 4 5 6 7 8 9 10 11 class Stack ( list ): def push ( self , item ): self . append ( item ) # Example s = Stack () s . push ( 1 ) s . push ( 2 ) s . push ( 3 ) s . pop () # -> 3 s . pop () # -> 2","title":"Inheritance"},{"location":"python/classes/classes/#composition","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Stack : def __init__ ( self ): self . _items = list () def push ( self , item ): self . _items . append ( item ) def pop ( self ): return self . _items . pop () def __len__ ( self ): return len ( self . _items ) # Example use s = Stack () s . push ( 1 ) s . push ( 2 ) s . push ( 3 ) s . pop () # -> 3 s . pop () # -> 2","title":"Composition"},{"location":"python/classes/classes/#passing-container-as-argument","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Stack : def __init__ ( self , * , container = None ): if container is None : container = list () self . _items = container def push ( self , item ): self . _items . append ( item ) def pop ( self ): return self . _items . pop () def __len__ ( self ): return len ( self . _items ) s = Stack ( container = array . array ( 'i' )) s . push ( 42 ) s . push ( 23 ) s . push ( 'a lot' ) # TypeError This is also an example of what\u2019s known as dependency injection . Instead of hardwiring Stack to depend on list, you can make it depend on any container a user decides to pass in, provided it implements the required interface.","title":"Passing Container as Argument"},{"location":"python/classes/classes/#avoid-inheritance-via-functions","text":"If there is too much plumbing going on here. If you\u2019re writing a lot of single-method classes, consider using functions instead.","title":"Avoid Inheritance via Functions"},{"location":"python/classes/classes/#class-based-parsing","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class DataParser : def parse ( self , lines ): records = [] for line in lines : row = line . split ( ',' ) record = self . make_record ( row ) records . append ( row ) return records def make_record ( self , row ): raise NotImplementedError () class PortfolioDataParser ( DataParser ): def make_record ( self , row ): return { 'name' : row [ 0 ], 'shares' : int ( row [ 1 ]), 'price' : float ( row [ 2 ]) } parser = PortfolioDataParser () data = parser . parse ( open ( 'portfolio.csv' ))","title":"Class Based Parsing"},{"location":"python/classes/classes/#function-based-parsing","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def parse_data ( lines , make_record ): records = [] for line in lines : row = line . split ( ',' ) record = make_record ( row ) records . append ( row ) return records def make_dict ( row ): return { 'name' : row [ 0 ], 'shares' : int ( row [ 1 ]), 'price' : float ( row [ 2 ]) } data = parse_data ( open ( 'portfolio.csv' ), make_dict )","title":"Function Based Parsing"},{"location":"python/classes/classes/#dynamic-binding-and-duck-typing","text":"Dynamic binding is the runtime mechanism that Python uses to find the attributes of objects. This is what allows Python to work with instances without regard for their type. In Python, variable names do not have an associated type. Thus, the attribute binding process is independent of what kind of object obj is. If you make a lookup, such as obj.name , it will work on any obj whatsoever that happens to have a name attribute. This behavior is sometimes referred to as duck typing \u2014in reference to the adage \u201cif it looks like a duck, quacks like a duck, and walks like a duck, then it\u2019s a duck.\u201d Python programmers often write programs that rely on this behavior. For example, if you want to make a customized version of an existing object, you can either inherit from it, or you can create a completely new object that looks and acts like it but is otherwise unrelated. This latter approach is often used to maintain loose coupling of program components. For example, code may be written to work with any kind of object whatsoever as long as it has a certain set of methods. One of the most common examples is with various iterable objects defined in the standard library. There are all sorts of objects that work with the for loop to produce values\u2014lists, files, generators, strings, and so on. However, none of these inherit from any kind of special Iterable base class. They merely implement the methods required to perform iteration\u2014and it all works.","title":"Dynamic Binding and Duck Typing"},{"location":"python/classes/classes/#dont-inherit-builtin-types","text":"dict , list they are written in C and bypass __setitem__ and __getitem__ . If you want to use UserDict , import it like this: from collections import UserDict .","title":"Don't Inherit Builtin Types"},{"location":"python/classes/classes/#class-variables-vs-methods","text":"In a class definition, all functions are assumed to operate on an instance, which is always passed as the first parameter self . However, the class itself is also an object that can carry state and be manipulated as well. As an example, you could track how many instances have been created using a class variable num_accounts : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Account : num_accounts = 0 def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance Account . num_accounts += 1 def __repr__ ( self ): return f ' { type ( self ) . __name__ } ( { self . owner !r} , { self . balance !r} )' def deposit ( self , amount ): self . balance += amount def withdraw ( self , amount ): self . deposit ( - amount ) # Must use self.deposit() def inquiry ( self ): return self . balance Class variables are defined outside the normal __init__() method. To modify them, use the class, not self . For example: 1 2 3 4 >>> a = Account ( 'Guido' , 1000.0 ) >>> b = Account ( 'Eva' , 10.0 ) >>> Account . num_accounts 2","title":"Class Variables vs Methods"},{"location":"python/classes/classes/#classmethod-usage-alternative-way-of-creating-a-class","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Account : def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance @classmethod def from_xml ( cls , data ): from xml.etree.ElementTree import XML doc = XML ( data ) return cls ( doc . findtext ( 'owner' ), float ( doc . findtext ( 'amount' ))) # Example use data = ''' <account> <owner>Guido</owner> <amount>1000.0</amount> </account> ''' a = Account . from_xml ( data )","title":"classmethod Usage: Alternative Way of Creating a Class"},{"location":"python/classes/classes/#configuration-of-classes","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import time class Date : datefmt = ' {year} - {month:02d} - {day:02d} ' def __init__ ( self , year , month , day ): self . year = year self . month = month self . day = day def __str__ ( self ): return self . datefmt . format ( year = self . year , month = self . month , day = self . day ) @classmethod def from_timestamp ( cls , ts ): tm = time . localtime ( ts ) return cls ( tm . tm_year , tm . tm_mon , tm . tm_mday ) @classmethod def today ( cls ): return cls . from_timestamp ( time . time ()) This class features a class variable datefmt that adjusts output from the __str__() method. This is something that can be customized via inheritance: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class MDYDate ( Date ): datefmt = ' {month} / {day} / {year} ' class DMYDate ( Date ): datefmt = ' {day} / {month} / {year} ' # Example a = Date ( 1967 , 4 , 9 ) print ( a ) # 1967-04-09 b = MDYDate ( 1967 , 4 , 9 ) print ( b ) # 4/9/1967 c = DMYDate ( 1967 , 4 , 9 ) print ( c ) # 9/4/1967","title":"Configuration of Classes"},{"location":"python/classes/classes/#dictfrom_keys-example","text":"1 2 dict . from_keys ([ 'a' , 'b' , 'c' ], 0 ) # Output: {'a': 0, 'b': 0, 'c': 0}","title":"dict.from_keys() Example"},{"location":"python/classes/classes/#static-methods","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class StandardPolicy : @staticmethod def deposit ( account , amount ): account . balance += amount @staticmethod def withdraw ( account , amount ): account . balance -= amount @staticmethod def inquiry ( account ): return account . balance class EvilPolicy ( StandardPolicy ): @staticmethod def deposit ( account , amount ): account . balance += 0.95 * amount @staticmethod def inquiry ( account ): if random . randint ( 0 , 4 ) == 1 : return 1.10 * account . balance else : return account . balance class Account : def __init__ ( self , owner , balance , * , policy = StandardPolicy ): self . owner = owner self . balance = balance self . policy = policy def __repr__ ( self ): return f 'Account( { self . policy } , { self . owner !r} , { self . balance !r} )' def deposit ( self , amount ): self . policy . deposit ( self , amount ) def withdraw ( self , amount ): self . policy . withdraw ( self , amount ) def inquiry ( self ): return self . policy . inquiry ( self )","title":"Static Methods"},{"location":"python/classes/classes/#about-design-patterns","text":"In writing object-oriented programs, programmers sometimes get fixated on implementing named design patterns\u2014such as the strategy pattern , the flyweight pattern , the singleton pattern , and so forth. Many of these originate from the famous Design Patterns book by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. If you are familiar with such patterns, the general design principles used in other languages can certainly be applied to Python. However, many of these documented patterns are aimed at working around specific issues that arise from the strict static type system of C++ or Java. The dynamic nature of Python renders a lot of these patterns obsolete, an overkill, or simply unnecessary. That said, there are a few overarching principles of writing good software\u2014such as striving to write code that is debuggable, testable, and extensible. Basic strategies such as writing classes with useful __repr__() methods, preferring composition over inheritance, and allowing dependency injection can go a long way towards these goals. Python programmers also like to work with code that can be said to be Pythonic . Usually, that means that objects obey various built-in protocols, such as iteration, containers, or context management. For example, instead of trying to implement some exotic data traversal pattern from a Java programming book, a Python programmer would probably implement it with a generator function feeding a for loop, or just replace the entire pattern with a few dictionary lookups.","title":"About Design Patterns"},{"location":"python/classes/classes/#properties","text":"As noted in the previous section, Python places no runtime restrictions on attribute values or types. However, such enforcement is possible if you put an attribute under the management of a so-called property . A property is a special kind of attribute that intercepts attribute access and handles it via user-defined methods. These methods have complete freedom to manage the attribute as they see fit. Here is an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import string class Account : def __init__ ( self , owner , balance ): self . owner = owner self . _balance = balance @property def owner ( self ): return self . _owner @owner . setter def owner ( self , value ): if not isinstance ( value , str ): raise TypeError ( 'Expected str' ) if not all ( c in string . ascii_uppercase for c in value ): raise ValueError ( 'Must be uppercase ASCII' ) if len ( value ) > 10 : raise ValueError ( 'Must be 10 characters or less' ) self . _owner = value class SomeClass : @property def attr ( self ): print ( 'Getting' ) @attr . setter def attr ( self , value ): print ( 'Setting' , value ) @attr . deleter def attr ( self ): print ( 'Deleting' ) # Example s = SomeClass () s . attr # Getting s . attr = 13 # Setting del s . attr # Deleting","title":"Properties"},{"location":"python/classes/classes/#types-interfaces-abstract-classes","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class A : pass class B ( A ): pass class C : pass a = A () # Instance of 'A' b = B () # Instance of 'B' c = C () # Instance of 'C' type ( a ) # Returns the class object A isinstance ( a , A ) # Returns True isinstance ( b , A ) # Returns True, B derives from A isinstance ( b , C ) # Returns False, B not derived from C Note : ABC must be implemented. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from abc import ABC , abstractmethod class Stream ( ABC ): @abstractmethod def receive ( self ): pass @abstractmethod def send ( self , msg ): pass @abstractmethod def close ( self ): pass","title":"Types, Interfaces, Abstract Classes"},{"location":"python/classes/classes/#multiple-inheritance-and-mixins","text":"","title":"Multiple Inheritance and Mixins"},{"location":"python/classes/classes/#interfaces-using-abc-classes","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from abc import ABC , abstractmethod class Stream ( ABC ): @abstractmethod def receive ( self ): pass @abstractmethod def send ( self , msg ): pass @abstractmethod def close ( self ): pass class Iterable ( ABC ): @abstractmethod def __iter__ ( self ): pass class MessageStream ( Stream , Iterable ): def receive ( self ): ... def send ( self ): ... def close ( self ): ... def __iter__ ( self ): ... m = MessageStream() isinstance(m, Stream) # -> True isinstance(m, Iterable) # -> True","title":"Interfaces using ABC Classes"},{"location":"python/classes/classes/#mixin-classes","text":"The other use of multiple inheritance is to define mixin classes. A mixin class is a class that modifies or extends the functionality of other classes. Consider the following class definitions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Duck : def noise ( self ): return 'Quack' def waddle ( self ): return 'Waddle' class Trombonist : def noise ( self ): return 'Blat!' def march ( self ): return 'Clomp' class Cyclist : def noise ( self ): return 'On your left!' def pedal ( self ): return 'Pedaling' These classes are completely unrelated to each other. There is no inheritance relationship and they implement different methods. However, there is a shared commonality in that they each define a noise() method. Using that as a guide, you could define the following classes: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class LoudMixin : def noise ( self ): return super () . noise () . upper () class AnnoyingMixin : def noise ( self ): return 3 * super () . noise () class LoudDuck ( LoudMixin , Duck ): pass class AnnoyingTrombonist ( AnnoyingMixin , Trombonist ): pass class AnnoyingLoudCyclist ( AnnoyingMixin , LoudMixin , Cyclist ): pass d = LoudDuck() d.noise() # -> 'QUACK' t = AnnoyingTrombonist() t.noise() # -> 'Blat!Blat!Blat!' c = AnnoyingLoudCyclist() c.noise() # -> 'ON YOUR LEFT!ON YOUR LEFT!ON YOUR LEFT!' Since mixin classes are defined in the same way as normal classes, it is best to include the word \"Mixin\" as part of the class name. This naming convention provides a greater clarity of purpose. To fully understand mixins, you need to know a bit more about how inheritance and the super() function work. First, whenever you use inheritance, Python builds a linear chain of classes known as the Method Resolution Order, or MRO for short. This is available as the mro attribute on a class. Here are some examples for single inheritance: 1 2 3 4 5 6 7 8 9 10 11 12 class Base : pass class A ( Base ): pass class B ( A ): pass Base . __mro__ # -> (<class 'Base'>, <class 'object'>) A . __mro__ # -> (<class 'A'>, <class 'Base'>, <class 'object'>) B . __mro__ # -> (<class 'B'>, <class 'A'>, <class 'Base'>, <class 'object'>)","title":"Mixin Classes"},{"location":"python/classes/classes/#class-decorators-and-class-methods","text":"","title":"Class Decorators and Class Methods"},{"location":"python/classes/classes/#factory-function-that-uses-the-registry","text":"1 2 def create_decoder ( mimetype ): return _registry [ mimetype ]() 1 2 3 4 5 @register_decoder class TextDecoder : mimetypes = [ 'text/plain' ] def decode ( self , data ): ... 1 2 3 4 5 @register_decoder class HTMLDecoder : mimetypes = [ 'text/html' ] def decode ( self , data ): ... 1 2 3 4 5 @register_decoder class ImageDecoder : mimetypes = [ 'image/png' , 'image/jpg' , 'image/gif' ] def decode ( self , data ): ...","title":"Factory function that uses the registry"},{"location":"python/classes/classes/#example-usage","text":"1 decoder = create_decoder ( 'image/jpg' ) A class decorator is free to modify the contents of the class it\u2019s given. For example, it might even rewrite existing methods. This is a common alternative to mixin classes or multiple inheritance. For example, consider these decorators:","title":"Example usage"},{"location":"python/classes/classes/#decorator-override-function","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def loud ( cls ): orig_noise = cls . noise def noise ( self ): return orig_noise ( self ) . upper () cls . noise = noise return cls def annoying ( cls ): orig_noise = cls . noise def noise ( self ): return 3 * orig_noise ( self ) cls . noise = noise return cls @annoying @loud class Cyclist ( object ): def noise ( self ): return 'On your left!' def pedal ( self ): return 'Pedaling'","title":"decorator override function"},{"location":"python/classes/classes/#add-code-to-class-at-runtime","text":"1 2 3 4 5 6 7 class Point : def __init__ ( self , x , y ): self . x = x self . y = y def __repr__ ( self ): return f ' { type ( self ) . __name__ } ( { self . x !r} , { self . y !r} )' Writing such methods is often annoying. Perhaps a class decorator could create the method for you? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import inspect def with_repr ( cls ): args = list ( inspect . signature ( cls ) . parameters ) argvals = ', ' . join ( '{self. %s !r}' % arg for arg in args ) code = 'def __repr__(self): \\n ' code += f ' return f' { cls . __name__ }({ argvals }) ' \\n ' locs = { } exec ( code , locs ) cls . __repr__ = locs [ '__repr__' ] return cls # Example @with_repr class Point : def __init__ ( self , x , y ): self . x = x self . y = y In this example, a repr() method is generated from the calling signature of the init() method. The method is created as a text string and passed to exec() to create a function. That function is attached to the class. Similar code generation techniques are used in parts of the standard library. For example, a convenient way to define data structures is to use a dataclass: 1 2 3 4 5 6 from dataclasses import dataclass @dataclass class Point : x : int y : int A dataclass automatically creates methods such as init () and repr () from class type hints. The methods are created using exec(), similarly to the prior example. Here\u2019s how the resulting Point class works: 1 2 p = Point ( 2 , 3 ) p Output: 1 Point(x=2, y=3) One downside of such an approach is poor startup performance. Dynamically creating code with exec() bypasses the compilation optimizations that Python normally applies to modules. Defining a large number of classes in this way may therefore significantly slow down the importing of your code.","title":"Add code to class at runtime"},{"location":"python/classes/classes/#supervised-inheritance-__init_subclass__","text":"As you saw in the previous section, sometimes you want to define a class and perform additional actions. A class decorator is one mechanism for doing this. However, a parent class can also perform extra actions on behalf of its subclasses. This is accomplished by implementing an __init_subclass__(cls) class method. For example: 1 2 3 4 5 6 7 8 9 10 11 class Base : @classmethod def __init_subclass__ ( cls ): print ( 'Initializing' , cls ) # Example (should see 'Initializing' message for each class) class A ( Base ): pass class B ( A ): pass If an __init_subclass__() method is present, it is triggered automatically upon the definition of any child class. This happens even if the child is buried deeply in an inheritance hierarchy. Many of the tasks commonly performed with class decorators can be performed with __init_subclass__() instead. For example, class registration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class DecoderBase : _registry = { } @classmethod def __init_subclass__ ( cls ): for mt in cls . mimetypes : DecoderBase . _registry [ mt . mimetype ] = cls # Factory function that uses the registry def create_decoder ( mimetype ): return DecoderBase . _registry [ mimetype ]() class TextDecoder ( DecoderBase ): mimetypes = [ 'text/plain' ] def decode ( self , data ): ... class HTMLDecoder ( DecoderBase ): mimetypes = [ 'text/html' ] def decode ( self , data ): ... class ImageDecoder ( DecoderBase ): mimetypes = [ 'image/png' , 'image/jpg' , 'image/gif' ] def decode ( self , data ): ...","title":"Supervised Inheritance - __init_subclass__"},{"location":"python/classes/classes/#class-initialization-with-__repr__","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import inspect class Base : @classmethod def __init_subclass__ ( cls ): # Create a __repr__ method args = list ( inspect . signature ( cls ) . parameters ) argvals = ', ' . join ( '{self. %s !r}' % arg for arg in args ) code = 'def __repr__(self): \\n ' code += f ' return f' { cls . __name__ }({ argvals }) ' \\n ' locs = { } exec ( code , locs ) cls . __repr__ = locs [ '__repr__' ] class Point ( Base ): def __init__ ( self , x , y ): self . x = x self . y = y If multiple inheritance is being used, you should use super() to make sure all classes that implement __init_subclass__() get called. For example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class A : @classmethod def __init_subclass__ ( cls ): print ( 'A.init_subclass' ) super () . __init_subclass__ () class B : @classmethod def __init_subclass__ ( cls ): print ( 'B.init_subclass' ) super () . __init_subclass__ () # Should see output from both classes here class C ( A , B ): pass","title":"Class Initialization with __repr__"},{"location":"python/classes/classes/#object-life-cycle-and-memory-management","text":"When a class is defined, the resulting class is a factory for creating new instances. For example: 1 2 3 4 class Account : def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance Create some Account instances: 1 2 a = Account ( 'Guido' , 1000.0 ) b = Account ( 'Eva' , 25.0 ) The creation of an instance is carried out in two steps using the special method __new__() that creates a new instance and __init__() that initializes it. For example, the operation a = Account('Guido', 1000.0) performs these steps: 1 2 3 a = Account . __new__ ( Account , 'Guido' , 1000.0 ) if isinstance ( a , Account ): Account . __init__ ( a , 'Guido' , 1000.0 ) Except for the first argument which is the class instead of an instance, __new__() normally receives the same arguments as __init__() . However, the default implementation of __new__() just ignores them. You\u2019ll sometimes see __new__() invoked with just a single argument. For example, this code also works: 1 2 3 a = Account . __new__ ( Account ) if isinstance ( a , Account ): Account . __init__ ( a , 'Guido' , 1000.0 ) Direct use of the __new__() method is uncommon, but sometimes it\u2019s used to create instances while bypassing the invocation of the __init__() method. One such use is in class methods. 1 2 3 4 class Spam : @classmethod def bar ( cls , * args , ** kwargs ): return cls . __new__ ( cls , * args , ** kwargs ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import time class Date : def __init__ ( self , year , month , day ): self . year = year self . month = month self . day = day @classmethod def today ( cls ): t = time . localtime () self = cls . __new__ ( cls ) # Make instance self . year = t . tm_year self . month = t . tm_month self . day = t . tm_day return self Modules that perform object serialization such as pickle also utilize new() to recreate instances when objects are deserialized. This is done without ever invoking init() . Sometimes a class will define new() if it wants to alter some aspect of instance creation. Typical applications include instance caching, singletons, and immutability. As an example, you might want Date class to perform date interning\u2014that is, caching and reusing Date instances that have an identical year, month, and day. Here is one way that might be implemented: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Date : _cache = { } @staticmethod def __new__ ( cls , year , month , day ): self = Date . _cache . get (( year , month , day )) if not self : self = super () . __new__ ( cls ) self . year = year self . month = month self . day = day Date . _cache [ year , month , day ] = self return self def __init__ ( self , year , month , day ): pass In this example, the class keeps an internal dictionary of previously created Date instances. When creating a new Date , the cache is consulted first. If a match is found, that instance is returned. Otherwise, a new instance is created and initialized. A subtle detail of this solution is the empty init() method. Even though instances are cached, every call to Date() still invokes init() . To avoid duplicated effort, the method simply does nothing\u2014instance creation actually takes place in new() when an instance is created the first time. There are ways to avoid the extra call to init() but it requires sneaky tricks. One way to avoid it is to have new() return an entirely different type instance\u2014for example, one belonging to a different class. Another solution, described later, is to use a metaclass. Once created, instances are managed by reference counting. If the reference count reaches zero, the instance is immediately destroyed. When the instance is about to be destroyed, the interpreter first looks for a del() method associated with the object and calls it. For example: 1 2 3 4 5 6 7 class Account ( object ): def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance def __del__ ( self ): print ( 'Deleting Account' ) Occasionally, a program will use the del statement to delete a reference to an object as shown. If this causes the reference count of the object to reach zero, the del() method is called. However, in general, the del statement doesn\u2019t directly call del() because there may be other object references living elsewhere. There are many other ways that an object might be deleted\u2014for example, reassignment of a variable name or a variable going out of scope in a function: 1 2 3 4 5 6 7 8 >>> a = Account ( 'Guido' , 1000.0 ) >>> a = 42 Deleting Account >>> def func (): ... a = Account ( 'Guido' , 1000.0 ) ... >>> func () Deleting Account In practice, it\u2019s rarely necessary for a class to define a del() method. The only exception is when the destruction of an object requires an extra cleanup action\u2014such as closing a file, shutting down a network connection, or releasing other system resources. Even in these cases, it\u2019s dangerous to rely on del() for a proper shutdown because there\u2019s no guarantee that this method will be called when you think it would. For clean shutdown of resources, you should give the object an explicit close() method. You should also make your class support the context manager protocol so it can be used with the with statement. Here is an example that covers all of the cases: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class SomeClass : def __init__ ( self ): self . resource = open_resource () def __del__ ( self ): self . close () def close ( self ): self . resource . close () def __enter__ ( self ): return self def __exit__ ( self , ty , val , tb ): self . close () # Closed via __del__() s = SomeClass () del s # Explicit close s = SomeClass () s . close () # Closed at the end of a context block with SomeClass () as s : ... Again, it should be emphasized that writing a del() in a class is almost never necessary. Python already has garbage collection and there is simply no need to do it unless there is some extra action that needs to take place upon object destruction. Even then, you still might not need del() as it\u2019s possible that the object is already programmed to clean itself up properly even if you do nothing. As if there weren\u2019t enough dangers with reference counting and object destruction, there are certain kinds of programming patterns\u2014especially those involving parent-child relationships, graphs, or caching\u2014where objects can create a so-called reference cycle. 1 2 3 4 5 6 class SomeClass : def __del__ ( self ): print ( 'Deleting' ) parent = SomeClass () child = SomeClass ()","title":"Object Life Cycle and Memory Management"},{"location":"python/classes/classes/#create-a-child-parent-reference-cycle","text":"parent.child = child child.parent = parent","title":"Create a child-parent reference cycle"},{"location":"python/classes/classes/#try-deletion-no-output-from-del-appears","text":"del parent del child In this example, the variable names are destroyed but you never see execution of the del() method. The two objects each hold internal references to each other, so there\u2019s no way for the reference count to ever drop to 0. To handle this, a special cycle-detecting garbage collector runs every so often. Eventually the objects will be reclaimed, but it\u2019s hard to predict when this might happen. If you want to force garbage collection, you can call gc.collect() . The gc module has a variety of other functions related to the cyclic garbage collector and monitoring memory. Because of the unpredictable timing of garbage collection, the del() method has a few restrictions placed on it. First, any exception that propagates out of del() is printed to sys.stderr , but otherwise ignored. Second, the del() method should avoid operations such as acquiring locks or other resources. Doing so could result in a deadlock when del() is unexpectedly fired in the middle of executing an unrelated function within the seventh inner callback circle of signal handling and threads. If you must define del() , keep it simple. weak references Sometimes objects are kept alive when you\u2019d much rather see them die. In an earlier example, a Date class was shown with internal caching of instances. One problem with this implementation is that there is no way for an instance to ever be removed from the cache. As such, the cache will grow larger and larger over time. One way to fix this problem is to create a weak reference using the weakref module. A weak reference is a way of creating a reference to an object without increasing its reference count. To work with a weak reference, you have to add an extra bit of code to check if the object being referred to still exists. Here\u2019s an example of how you create a weakref: 1 2 import weakref a_ref = weakref . ref ( a ) In this example, a_ref is a weak reference to the object a . You can use the weak reference to access the object, but it doesn't prevent the object from being garbage collected. 1 2 3 4 a = Account ( 'Guido' , 1000.0 ) import weakref a_ref = weakref . ref ( a ) a_ref () Support for weak references requires instances to have a mutable weakref attribute. Instances of user-defined classes normally have such an attribute by default. However, built-in types and certain kinds of special data structures\u2014named tuples, classes with slots\u2014do not. If you want to construct weak references to these types, you can do it by defining variants with a weakref attribute added: 1 2 3 4 5 class wdict ( dict ): __slots__ = ( '__weakref__' ,) w = wdict () w_ref = weakref . ref ( w ) # Now works attribute binding The state associated with an instance is stored in a dictionary that\u2019s accessible as the instance\u2019s __dict__ attribute. This dictionary contains the data that\u2019s unique to each instance. Classes are linked to their base classes by a special attribute __bases__ , which is a tuple of the base classes. The __bases__ attribute is only informational. The actual runtime implementation of inheritance uses the __mro__ attribute which is a tuple of all parent classes listed in search order. This underlying structure is the basis for all operations that get, set, or delete the attributes of instances. Whenever an attribute is set using obj.name = value , the special method obj.__setattr__('name', value) is invoked. If an attribute is deleted using del obj.name , the special method obj.__delattr__('name') is invoked. The default behavior of these methods is to modify or remove values from the local __dict__ of obj unless the requested attribute happens to correspond to a property or descriptor. In that case, the set and delete operations will be carried out by the __set__ and __delete__ functions associated with the property. For attribute lookup such as obj.name , the special method obj.__getattribute__('name') is invoked. This method carries out the search for the attribute, which normally includes checking the properties, looking in the local __dict__ , checking the class dictionary, and searching the MRO. If this search fails, a final attempt to find the attribute is made by invoking the obj.__getattr__('name') method of the class (if defined). If this fails, an AttributeError exception is raised. User-defined classes can implement their own versions of the attribute access functions, if desired. For example, here\u2019s a class that restricts the attribute names that can be set: 1 2 3 4 5 6 7 8 9 class Account : def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance def __setattr__ ( self , name , value ): if name not in { 'owner' , 'balance' }: raise AttributeError ( f 'No attribute { name } ' ) super () . __setattr__ ( name , value )","title":"Try deletion (no output from del appears)"},{"location":"python/classes/classes/#example","text":"a = Account('Guido', 1000.0) a.balance = 940.25 # Ok a.amount = 540.2 # AttributeError. No attribute amount proxies, wrappers, delegations A common implementation technique for proxies involves the getattr() method. Here is a simple example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class A : def spam ( self ): print ( 'A.spam' ) def grok ( self ): print ( 'A.grok' ) def yow ( self ): print ( 'A.yow' ) class LoggedA : def __init__ ( self ): self . _a = A () def __getattr__ ( self , name ): print ( 'Accessing' , name ) # Delegate to internal A instance return getattr ( self . _a , name ) In this example, the LoggedA class acts as a proxy for class A . When an attribute is accessed on an instance of LoggedA , the __getattr__() method is invoked. It prints the accessed attribute name and then delegates the attribute access to the internal instance of A . Example use: 1 2 3 a = LoggedA () a . spam () # prints 'Accessing spam' and 'A.spam' a . yow () # prints 'Accessing yow' and 'A.yow' Delegation is sometimes used as an alternative to inheritance. Here is an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class A : def spam ( self ): print ( 'A.spam' ) def grok ( self ): print ( 'A.grok' ) def yow ( self ): print ( 'A.yow' ) class B : def __init__ ( self ): self . _a = A () def grok ( self ): print ( 'B.grok' ) def __getattr__ ( self , name ): return getattr ( self . _a , name ) In this example, class B holds an internal reference to an instance of A and delegates attribute access to it. Methods defined in class B override the corresponding methods in class A , while all other methods are delegated to the internal instance of A . Example use: 1 2 3 4 b = B () b . spam () # -> A.spam b . grok () # -> B.grok (redefined method) b . yow () # -> A.yow The technique of forwarding attribute lookup via __getattr__() is a common technique. However, be aware that it does not apply to operations mapped to special methods. For example, consider this class: 1 2 3 4 5 6 class ListLike : def __init__ ( self ): self . _items = list () def __getattr__ ( self , name ): return getattr ( self . _items , name ) In this example, the ListLike class forwards all of the standard list methods to an inner list using __getattr__() . However, operations such as len(a) or a[0] fail because they are not mapped to special methods ( __len__() and __getitem__() ). To make those work, you would have to explicitly implement the required special methods. To illustrate, here's an updated ListLike class that implements the necessary special methods: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class ListLike : def __init__ ( self ): self . _items = list () def __getattr__ ( self , name ): return getattr ( self . _items , name ) def __len__ ( self ): return len ( self . _items ) def __getitem__ ( self , index ): return self . _items [ index ] def __setitem__ ( self , index , value ): self . _items [ index ] = value","title":"Example"},{"location":"python/classes/classes/#slots","text":"The slots attribute is a definition hint that allows Python to make performance optimizations for both memory use and execution speed. It eliminates the need for a dictionary to store instance data and uses a more compact array-based data structure instead. Using slots can result in a substantial reduction in memory use and a modest improvement in execution time, especially in programs that create a large number of objects. Here are some key points about slots : The slots attribute lists only the instance attributes and does not include methods, properties, class variables, or any other class-level attributes. If a class uses slots , any derived class must also define slots (even if empty) to take advantage of the benefits. Failure to do so will result in slower performance and increased memory usage. slots is not compatible with multiple inheritance. If multiple base classes with non-empty slots are specified, a TypeError will be raised. Code that relies on the underlying __dict__ attribute of instances may break when slots is used. slots has no effect on the invocation of methods such as __getattribute__() , __getattr__() , and __setattr__() if they are redefined in a class. However, the absence of the instance __dict__ attribute should be considered when implementing these methods.","title":"slots"},{"location":"python/classes/classes/#descriptors","text":"Descriptors provide a way to customize attribute access in Python by implementing the special methods __get__() , __set__() , and __delete__() . They are class-level objects that manage access to attributes. Properties are implemented using descriptors. Here's an example of a descriptor class called Typed : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Typed : expected_type = object def __set_name__ ( self , cls , name ): self . key = name def __get__ ( self , instance , cls ): if instance : return instance . __dict__ [ self . key ] else : return self def __set__ ( self , instance , value ): if not isinstance ( value , self . expected_type ): raise TypeError ( f 'Expected { self . expected_type } ' ) instance . __dict__ [ self . key ] = value def __delete__ ( self , instance ): raise AttributeError ( \"Can't delete attribute\" ) In this example, the Typed class defines a descriptor that performs type checking when an attribute is assigned and raises an error if an attempt is made to delete the attribute. Subclasses like Integer , Float , and String specialize Typed to match specific types. Descriptors are used by including them as class attributes in another class. For example: 1 2 3 4 5 6 7 class Account : owner = String () balance = Float () def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance In this case, the Account class uses the descriptors String and Float to automatically call the appropriate __get__() , __set__() , or __delete__() methods when accessing the owner and balance attributes. Descriptors take precedence over items in the instance dictionary. Even if an instance dictionary has a matching entry, the descriptor's __set__() method will be invoked. For example: 1 2 a = Account ( 'Guido' , 1000.0 ) a . balance = 'a lot' # Raises TypeError: Expected <class 'float'> The __get__(instance, cls) method of a descriptor takes arguments for both the instance and the class. When invoked at the class level, the instance argument is None . The __get__() method typically returns the descriptor itself if no instance is provided. 1 Account . balance # Returns <__main__.Float object at 0x110606710>","title":"Descriptors"},{"location":"python/classes/classes/#method-descriptor","text":"A descriptor that only implements __get__() is known as a method descriptor. It is mainly used to implement Python's various types of methods, such as instance methods, class methods, and static methods. The __get__() method of a method descriptor only gets invoked if there is no matching entry in the instance dictionary. Here's an example of implementing @classmethod and @staticmethod using method descriptors: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import types class classmethod : def __init__ ( self , func ): self . __func__ = func def __get__ ( self , instance , cls ): return types . MethodType ( self . __func__ , cls ) class staticmethod : def __init__ ( self , func ): self . __func__ = func def __get__ ( self , instance , cls ): return self . __func__ Lazy Evaluation Method descriptors can be used to implement lazy evaluation of attributes. By only computing and assigning the attribute value when it is accessed for the first time, we can save computational resources. Here's an example of implementing lazy evaluation using a descriptor called Lazy : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Lazy : def __init__ ( self , func ): self . func = func def __set_name__ ( self , cls , name ): self . key = name def __get__ ( self , instance , cls ): if instance : value = self . func ( instance ) instance . __dict__ [ self . key ] = value return value else : return self In this example, the Lazy descriptor is used in the Rectangle class to lazily compute the area and perimeter attributes: 1 2 3 4 5 6 7 class Rectangle : def __init__ ( self , width , height ): self . width = width self . height = height area = Lazy ( lambda self : self . width * self . height ) perimeter = Lazy ( lambda self : 2 * self . width + 2 * self . height ) When the area or perimeter attributes are accessed for the first time, the corresponding lambda function is executed to compute the value. The computed value is then stored in the instance's __dict__ attribute for future use.","title":"Method Descriptor"},{"location":"python/classes/classes/#class-definitions","text":"The definition of a class is a dynamic process. When you define a class using the class statement, a new dictionary is created that serves as the local class namespace. The body of the class then executes as a script within this namespace. Eventually, the namespace becomes the __dict__ attribute of the resulting class object. Any legal Python statement is allowed in the body of a class. Normally, you just define functions and variables, but control flow, imports, nested classes, and everything else is allowed. For example, here is a class that conditionally defines methods: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Account : def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance if debug : import logging log = logging . getLogger ( f ' { __module__ } . { __qualname__ } ' ) def deposit ( self , amount ): Account . log . debug ( 'Depositing %f ' , amount ) self . balance += amount def withdraw ( self , amount ): Account . log . debug ( 'Withdrawing %f ' , amount ) self . balance -= amount else : def deposit ( self , amount ): self . balance += amount def withdraw ( self , amount ): self . balance -= amount In this example, a global variable debug is being used to conditionally define methods. The __module__ and __qualname__ variables are predefined strings that hold information about the class name and enclosing module. These can be used by statements in the class body. In this example, they're being used to configure the logging system. There are probably cleaner ways of organizing the above code, but the key point is that you can put anything you want in a class. One critical point about class definition is that the namespace used to hold the contents of the class body is not a scope of variables. Any name that gets used within a method (such as Account.log in the above example) needs to be fully qualified. If a function like locals() is used in a class body (but not inside a method), it returns the dictionary being used for the class namespace.","title":"Class Definitions"},{"location":"python/classes/classes/#dynamic-class-creation","text":"Normally, classes are created using the class statement, but this is not a requirement. As noted in the previous section, classes are defined by executing the body of a class to populate a namespace. If you're able to populate a dictionary with your own definitions, you can make a class without ever using the class statement. To do that, use types.new_class() : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import types # Some methods (not in a class) def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance def deposit ( self , amount ): self . balance -= amount def withdraw ( self , amount ): self . balance += amount methods = { '__init__' : __init__ , 'deposit' : deposit , 'withdraw' : withdraw , } Account = types . new_class ( 'Account' , (), exec_body = lambda ns : ns . update ( methods )) # You now have a class a = Account ( 'Guido' , 1000.0 ) a . deposit ( 50 ) a . withdraw ( 25 ) Dynamic class creation may be useful if you want to create classes from data structures or generate classes programmatically. For example, in the section on descriptors, the following classes were defined: 1 2 3 4 5 6 7 8 class Integer ( Typed ): expected_type = int class Float ( Typed ): expected_type = float class String ( Typed ): expected_type = str This code is highly repetitive. A data-driven approach can be used to generate the classes dynamically: 1 2 3 4 5 6 7 8 9 10 11 12 typed_classes = [ ( 'Integer' , int ), ( 'Float' , float ), ( 'String' , str ), ( 'Bool' , bool ), ( 'Tuple' , tuple ), ] globals () . update ( ( name , types . new_class ( name , ( Typed ,), exec_body = lambda ns : ns . update ( expected_type = ty ))) for name , ty in typed_classes ) In this example, the global module namespace is being updated with dynamically created classes using types.new_class() . The typed_classes list defines the names and expected types for each class. Each class is created by calling types.new_class() with the class name, base classes, and an exec_body function that updates the namespace with the expected type. The resulting classes are then added to the global namespace using globals().update() . Sometimes you will see type() being used to dynamically create a class instead. For example: 1 Account = type ( 'Account' , (), methods ) This works, but it doesn\u2019t take into account some of the more advanced class machinery such as metaclasses. In modern code, try to use types.new_class() instead.","title":"Dynamic Class Creation"},{"location":"python/classes/classes/#metaclasses","text":"When you define a class in Python, the class definition itself becomes an object. Here's an example: 1 2 3 4 5 6 7 8 9 10 class Account : def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance def deposit ( self , amount ): self . balance += amount def withdraw ( self , amount ): self . balance -= amount To check if Account is an object, you can use the isinstance function: 1 isinstance ( Account , object ) If you think about this long enough, you will realize that if Account is an object, then something had to create it. This creation of the class object is controlled by a special kind of class called a metaclass. Simply put, a metaclass is a class that creates instances of classes. In the preceding example, the metaclass that created Account is a built-in class called type . In fact, if you check the type of Account , you will see that it is an instance of type : 1 type ( Account ) It's a bit brain-bending, but it's similar to integers. For example, if you write x = 42 and then look at x.__class__ , you'll get int , which is the class that creates integers. Similarly, type makes instances of types or classes. When a new class is defined with the class statement, a number of things happen. First, a new namespace for the class is created. Next, the body of the class is executed in this namespace. Finally, the class name, base classes, and populated namespace are used to create the class instance. The following code illustrates the low-level steps that take place: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 namespace = type . __prepare__ ( 'Account' , ()) # Step 2: Execute the class body exec ( ''' def __init__(self, owner, balance): self.owner = owner self.balance = balance def deposit(self, amount): self.balance += amount def withdraw(self, amount): self.balance -= amount ''' , globals (), namespace ) # Step 3: Create the final class object Account = type ( 'Account' , (), namespace ) In the definition process, there is interaction with the type class to create the class namespace and to create the final class object. The choice of using type can be customized - a class can choose to be processed by a different metaclass by specifying a different metaclass. This is done by using the metaclass keyword argument in inheritance: 1 class Account ( metaclass = type ): If no metaclass is given, the class statement examines the type of the first entry in the tuple of base classes (if any) and uses that as the metaclass. Therefore, if you write class Account(object) , the resulting Account class will have the same type as object (which is type ). Note that classes that don't specify any parent at all always inherit from object , so this still applies. To create a new metaclass, define a class that inherits from type . Within this class, you can redefine one or more methods that are used during the class creation process. Typically, this includes the __prepare__() method used to create the class namespace, the __new__() method used to create the class instance, the __init__() method called after a class has already been created, and the __call__() method used to create new instances. The following example implements a metaclass that merely prints the input arguments to each method so you can experiment: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class mytype ( type ): # Creates the class namespace @classmethod def __prepare__ ( meta , clsname , bases ): print ( 'Preparing:' , clsname , bases ) return super () . __prepare__ ( clsname , bases ) # Creates the class instance after body has executed @staticmethod def __new__ ( meta , clsname , bases , namespace ): print ( 'Creating:' , clsname , bases , namespace ) return super () . __new__ ( meta , clsname , bases , namespace ) # Initializes the class instance def __init__ ( cls , clsname , bases , namespace ): print ( 'Initializing:' , clsname , bases , namespace ) super () . __init__ ( clsname , bases , namespace ) # Creates new instances of the class def __call__ ( cls , * args , ** kwargs ): print ( 'Creating instance:' , args , kwargs ) return super () . __call__ ( * args , ** kwargs )","title":"Metaclasses"},{"location":"python/classes/classes/#example_1","text":"1 2 class Base ( metaclass = mytype ): pass The definition of the Base produces the following output: 1 2 3 # Preparing: Base () # Creating: Base () {'__module__': '__main__', '__qualname__': 'Base'} # Initializing: Base () {'__module__': '__main__', '__qualname__': 'Base'} 1 b = Base () Creating instance: () . One tricky facet of working with metaclasses is the naming of variables and keeping track of the various entities involved. In the above code, the meta name refers to the metaclass itself. The cls name refers to a class instance created by the metaclass. Although not used here, the self name refers to a normal instance created by a class. Metaclasses propagate via inheritance. So, if you've defined a base class to use a different metaclass, all child classes will also use that metaclass. Try this example to see your custom metaclass at work: 1 2 3 4 5 6 7 8 9 10 11 12 class Account ( Base ): def __init__ ( self , owner , balance ): self . owner = owner self . balance = balance def deposit ( self , amount ): self . balance += amount def withdraw ( self , amount ): self . balance -= amount print ( type ( Account )) # -> <class 'mytype'> The primary use of metaclasses is in situations where you want to exert extreme low-level control over the class definition environment and creation process. Before proceeding, however, remember that Python already provides a lot of functionality for monitoring and altering class definitions (such as the __init_subclass__() method, class decorators, descriptors, mixins, and so on). Most of the time, you probably don't need a metaclass. That said, the next few examples show situations where a metaclass provides the only sensible solution. One use of a metaclass is in rewriting the contents of the class namespace prior to the creation of the class object. Certain features of classes are established at definition time and can't be modified later. One such feature is __slots__ . As noted earlier, __slots__ is a performance optimization related to the memory layout of instances. Here's a metaclass that automatically sets the __slots__ attribute based on the calling signature of the __init__() method. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import inspect class SlotMeta ( type ): @staticmethod def __new__ ( meta , clsname , bases , methods ): if '__init__' in methods : sig = inspect . signature ( methods [ '__init__' ]) __slots__ = tuple ( sig . parameters )[ 1 :] else : __slots__ = () methods [ '__slots__' ] = __slots__ return super () . __new__ ( meta , clsname , bases , methods ) class Base ( metaclass = SlotMeta ): pass","title":"Example"},{"location":"python/classes/classes/#example_2","text":"1 2 3 4 class Point ( Base ): def __init__ ( self , x , y ): self . x = x self . y = y In this example, the Point class that's created is automatically created with slots of ('x', 'y') . The resulting instances of Point now get memory savings without knowing that slots are being used. It doesn't have to be specified directly. This kind of trick is not possible with class decorators or with init_subclass() because those features only operate on a class after it's been created. By then, it's too late to apply the slots optimization. Another use of metaclasses is for altering the class definition environment. For example, duplicate definitions of a name during class definition normally result in a silent error - the second definition overwrites the first. Suppose you wanted to catch that. Here's a metaclass that does that by defining a different kind of dictionary for the class namespace: 1 2 3 4 5 6 7 8 9 10 11 12 13 class NoDupeDict ( dict ): def __setitem__ ( self , key , value ): if key in self : raise AttributeError ( f ' { key } already defined' ) super () . __setitem__ ( key , value ) class NoDupeMeta ( type ): @classmethod def __prepare__ ( meta , clsname , bases ): return NoDupeDict () class Base ( metaclass = NoDupeMeta ): pass","title":"Example"},{"location":"python/classes/classes/#example_3","text":"1 2 3 4 5 6 class SomeClass ( Base ): def yow ( self ): print ( 'Yow!' ) def yow ( self , x ): # Fails. Already defined print ( 'Different Yow!' ) This is only a small sample of what's possible. For framework builders, metaclasses offer an opportunity to tightly control what happens during class definition - allowing classes to serve as a kind of domain-specific language. Historically, metaclasses have been used to accomplish a variety of tasks that are now possible through other means. The init_subclass() method, in particular, can be used to address a wide variety of use cases where metaclasses were once applied. This includes registration of classes with a central registry, automatic decoration of methods, and code generation.","title":"Example"},{"location":"python/classes/classes/#built-in-objects-for-instances-and-classes","text":"Attribute Description cls.__name__ Class name cls.__module__ Module name in which the class is defined cls.__qualname__ Fully qualified class name cls.__bases__ Tuple of base classes cls.__mro__ Method Resolution Order tuple cls.__dict__ Dictionary holding class methods and variables cls.__doc__ Documentation string cls.__annotations__ Dictionary of class type hints cls.__abstractmethods__ Set of abstract method names (may be undefined if there aren't any) Attribute Description i.__class__ Class to which the instance belongs i.__dict__ Dictionary holding instance data (if defined)","title":"Built-in Objects for Instances and Classes"},{"location":"python/classes/data_model/","text":"Python Data Model When using a framework, we spend a lot of time coding methods that are called by the framework. The same happens when we leverage the Python Data Model. The Python interpreter invokes special methods to perform basic object operations, often triggered by special syntax. The special method names are always written with leading and trailing double underscores (i.e., __getitem__ ). For example, the syntax obj[key] is supported by the __getitem__ special method. In order to evaluate my_collection[key] , the interpreter calls my_collection.getitem(key) . The special method names allow your objects to implement, support, and interact with fundamental language constructs such as: Collections Attribute access Iteration (including asynchronous iteration using async for ) Operator overloading Function and method invocation String representation and formatting Asynchronous programming using await Object creation and destruction Managed contexts (including asynchronous context managers using async with ) Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import collections Card = collections . namedtuple ( 'Card' , [ 'rank' , 'suit' ]) class FrenchDeck : ranks = [ str ( n ) for n in range ( 2 , 11 )] + list ( 'JQKA' ) suits = 'spades diamonds clubs hearts' . split () def __init__ ( self ): self . _cards = [ Card ( rank , suit ) for suit in self . suits for rank in self . ranks ] def __len__ ( self ): return len ( self . _cards ) def __getitem__ ( self , position ): return self . _cards [ position ] Special Methods Description __iter__ Iterable __len__ Sized __contains__ Container Strings/bytes __repr__ String representation __str__ String representation __format__ Formatting __bytes__ Bytes representation __fspath__ File path representation Number __abs__ Absolute value __bool__ Boolean value __complex__ Complex number __int__ Integer representation __float__ Float representation __hash__ Hash value __index__ Indexing Collections __len__ Length __getitem__ Item access __setitem__ Item assignment __delitem__ Item deletion __contains__ Membership test Iteration __iter__ Iteration __aiter__ Asynchronous iteration __next__ Next item __anext__ Asynchronous next item __reversed__ Reversed iteration Callable or coroutine __call__ Function or method invocation __await__ Asynchronous await Context managers __enter__ Context manager enter __aenter__ Asynchronous context manager enter __exit__ Context manager exit __aexit__ Asynchronous context manager exit Instance creation and destruction __new__ Object creation __init__ Object initialization __del__ Object destruction Attribute management __getattr__ Attribute retrieval __getattribute__ Attribute access __setattr__ Attribute assignment __delattr__ Attribute deletion __dir__ Directory listing Attribute descriptors __get__ Descriptor get __set__ Descriptor set __delete__ Descriptor deletion __set_name__ Descriptor set name Class services __prepare__ Class creation __init_subclass__ Subclass initialization __instancecheck__ Instance check __subclasscheck__ Subclass check Why len is not a method len runs very fast when x is a built-in type. No method is called for built-in types in CPython; length is simply read from a field from the C struct. len is not called as methods, but in our Python objects, it works as normal. Data Structure Every Python object in a C struct has two fields: ob_refcnt and ob_fval : reference count and pointer value. Mutable Sequences vs Immutable Mutable: list, bytearray, array.array, collections.deque, and memoryview. Immutable: tuple, str, and bytes. TIP In Python code, line breaks are ignored inside pairs of [], {}, or (). So you can build multiline lists, listcomps, genexps, dictionaries, and the like without using the ugly \\ line continuation escape. Also, when those delimiters are used to define a literal with a comma-separated series of items, a trailing comma will be ignored. So, for example, when coding a multi-line list literal, it is thoughtful to put a comma after the last item. List Comps Versus map and filter map and filter were faster, but nowadays they are the same. Tuple is not just immutable lists. It can be used as immutable lists or records with no field names (1,2) lat long. If you write internationalized software, _ is not a good dummy variable because it is traditionally used as an alias to the gettext.gettext function, as recommended in the gettext module documentation. Otherwise, it\u2019s a conventional name for a placeholder variable to be ignored. Tuple as Immutable List 1. Clarity: You know it never changes. 2. Performance: It uses less memory. Are tuples more efficient than lists? Raymond Hettinger answers: - To evaluate a tuple, Python generates bytecode in constant one operation, but for a list, it pushes every element as a separate constant to data stacks and builds the list. - Hashable tuple: tuple(t) returns a reference to the same t . No need to copy; the list makes a copy anyway. - For fixed length, exact memory is allocated. The list has room to spare for the future. - References to items of a tuple are stored in an array with the tuple struct itself. The list holds a pointer to the array of references stored elsewhere and makes the CPU cache less effective. But it is necessary because of the need to make room. Slicing seq[start:stop:step] - Python calls seq.getitem(slice(start, stop, step)) . Building a List of Objects 1 2 3 my_list = [[]] * 3 # same board appended, one changes everyone changes board = [['_'] * 3 for i in range(3)] # no problem When List is Not the Answer If it contains the same type, maybe array.array will be better. You can dump it to a binary file directly, and it's memory-efficient. Queue Why don't use List as a queue? Because every item has to be shifted in memory. Use collections.deque instead; it is thread-safe and has the maxlen attribute. There are more queues: - queue : SimpleQueue , Queue , LifoQueue , and PriorityQueue . - multiprocessing : SimpleQueue and bounded Queue - very similar to those in the queue package but designed for interprocess communication. A specialized multiprocessing.JoinableQueue . - asyncio : Provides Queue , LifoQueue , PriorityQueue , and JoinableQueue . - heap : heappush , heappop . Flat vs Container Sequence Flat is all the same type. hash() Calling hash(t) on a tuple is a quick way to assert that its value is fixed. A TypeError will be raised if t contains mutable items. Decode vs Encode Imagine str is human-readable bytes; don't. Bytes need decoding; string encoding.","title":"Data Model"},{"location":"python/classes/data_model/#python-data-model","text":"When using a framework, we spend a lot of time coding methods that are called by the framework. The same happens when we leverage the Python Data Model. The Python interpreter invokes special methods to perform basic object operations, often triggered by special syntax. The special method names are always written with leading and trailing double underscores (i.e., __getitem__ ). For example, the syntax obj[key] is supported by the __getitem__ special method. In order to evaluate my_collection[key] , the interpreter calls my_collection.getitem(key) . The special method names allow your objects to implement, support, and interact with fundamental language constructs such as: Collections Attribute access Iteration (including asynchronous iteration using async for ) Operator overloading Function and method invocation String representation and formatting Asynchronous programming using await Object creation and destruction Managed contexts (including asynchronous context managers using async with )","title":"Python Data Model"},{"location":"python/classes/data_model/#example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import collections Card = collections . namedtuple ( 'Card' , [ 'rank' , 'suit' ]) class FrenchDeck : ranks = [ str ( n ) for n in range ( 2 , 11 )] + list ( 'JQKA' ) suits = 'spades diamonds clubs hearts' . split () def __init__ ( self ): self . _cards = [ Card ( rank , suit ) for suit in self . suits for rank in self . ranks ] def __len__ ( self ): return len ( self . _cards ) def __getitem__ ( self , position ): return self . _cards [ position ] Special Methods Description __iter__ Iterable __len__ Sized __contains__ Container Strings/bytes __repr__ String representation __str__ String representation __format__ Formatting __bytes__ Bytes representation __fspath__ File path representation Number __abs__ Absolute value __bool__ Boolean value __complex__ Complex number __int__ Integer representation __float__ Float representation __hash__ Hash value __index__ Indexing Collections __len__ Length __getitem__ Item access __setitem__ Item assignment __delitem__ Item deletion __contains__ Membership test Iteration __iter__ Iteration __aiter__ Asynchronous iteration __next__ Next item __anext__ Asynchronous next item __reversed__ Reversed iteration Callable or coroutine __call__ Function or method invocation __await__ Asynchronous await Context managers __enter__ Context manager enter __aenter__ Asynchronous context manager enter __exit__ Context manager exit __aexit__ Asynchronous context manager exit Instance creation and destruction __new__ Object creation __init__ Object initialization __del__ Object destruction Attribute management __getattr__ Attribute retrieval __getattribute__ Attribute access __setattr__ Attribute assignment __delattr__ Attribute deletion __dir__ Directory listing Attribute descriptors __get__ Descriptor get __set__ Descriptor set __delete__ Descriptor deletion __set_name__ Descriptor set name Class services __prepare__ Class creation __init_subclass__ Subclass initialization __instancecheck__ Instance check __subclasscheck__ Subclass check Why len is not a method len runs very fast when x is a built-in type. No method is called for built-in types in CPython; length is simply read from a field from the C struct. len is not called as methods, but in our Python objects, it works as normal.","title":"Example"},{"location":"python/classes/data_model/#data-structure","text":"Every Python object in a C struct has two fields: ob_refcnt and ob_fval : reference count and pointer value.","title":"Data Structure"},{"location":"python/classes/data_model/#mutable-sequences-vs-immutable","text":"Mutable: list, bytearray, array.array, collections.deque, and memoryview. Immutable: tuple, str, and bytes. TIP In Python code, line breaks are ignored inside pairs of [], {}, or (). So you can build multiline lists, listcomps, genexps, dictionaries, and the like without using the ugly \\ line continuation escape. Also, when those delimiters are used to define a literal with a comma-separated series of items, a trailing comma will be ignored. So, for example, when coding a multi-line list literal, it is thoughtful to put a comma after the last item.","title":"Mutable Sequences vs Immutable"},{"location":"python/classes/data_model/#list-comps-versus-map-and-filter","text":"map and filter were faster, but nowadays they are the same. Tuple is not just immutable lists. It can be used as immutable lists or records with no field names (1,2) lat long. If you write internationalized software, _ is not a good dummy variable because it is traditionally used as an alias to the gettext.gettext function, as recommended in the gettext module documentation. Otherwise, it\u2019s a conventional name for a placeholder variable to be ignored. Tuple as Immutable List 1. Clarity: You know it never changes. 2. Performance: It uses less memory. Are tuples more efficient than lists? Raymond Hettinger answers: - To evaluate a tuple, Python generates bytecode in constant one operation, but for a list, it pushes every element as a separate constant to data stacks and builds the list. - Hashable tuple: tuple(t) returns a reference to the same t . No need to copy; the list makes a copy anyway. - For fixed length, exact memory is allocated. The list has room to spare for the future. - References to items of a tuple are stored in an array with the tuple struct itself. The list holds a pointer to the array of references stored elsewhere and makes the CPU cache less effective. But it is necessary because of the need to make room. Slicing seq[start:stop:step] - Python calls seq.getitem(slice(start, stop, step)) . Building a List of Objects 1 2 3 my_list = [[]] * 3 # same board appended, one changes everyone changes board = [['_'] * 3 for i in range(3)] # no problem When List is Not the Answer If it contains the same type, maybe array.array will be better. You can dump it to a binary file directly, and it's memory-efficient. Queue Why don't use List as a queue? Because every item has to be shifted in memory. Use collections.deque instead; it is thread-safe and has the maxlen attribute. There are more queues: - queue : SimpleQueue , Queue , LifoQueue , and PriorityQueue . - multiprocessing : SimpleQueue and bounded Queue - very similar to those in the queue package but designed for interprocess communication. A specialized multiprocessing.JoinableQueue . - asyncio : Provides Queue , LifoQueue , PriorityQueue , and JoinableQueue . - heap : heappush , heappop . Flat vs Container Sequence Flat is all the same type. hash() Calling hash(t) on a tuple is a quick way to assert that its value is fixed. A TypeError will be raised if t contains mutable items. Decode vs Encode Imagine str is human-readable bytes; don't. Bytes need decoding; string encoding.","title":"List Comps Versus map and filter"},{"location":"python/classes/descriptors/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class DescriptorClass : def __get__ ( self , instance , owner ): if instance is None : return self print ( self . __class__ . __name__ , instance , owner ) return instance class ClientClass : descriptor = DescriptorClass () client = ClientClass () client . descriptor Descriptor Methods __get__(self, instance, owner) : The __get__ method of the descriptor class. It takes three arguments: self , instance (where the descriptor is called from), and owner (a reference to the class object). owner is the same as instance.__class__ . __set__(self, instance, value) : The __set__ method of the descriptor class. It is called when assigning a value to the descriptor. Example usage: client.descriptor = 'value' . __delete__(self, instance) : The __delete__ method of the descriptor class. It is called when deleting the descriptor. Example usage: del client.descriptor . __set_name__(self, owner, name) : The __set_name__ method of the descriptor class. It is called during the class creation and provides the field name. 1 2 3 4 5 6 7 8 9 10 11 12 13 class DescriptorWithName : def __init__ ( self , name ): self . name = name def __get__ ( self , instance , value ): if instance is None : return self print ( self . name , instance ) return instance . __dict__ [ self . name ] def __set__ ( self , instance , value ): instance . __dict__ [ self . name ] = value class ClientClass : descriptor = DescriptorWithName ( \"descriptor\" ) Descriptor Types Non-data descriptor: Implements only the __get__ method. Data descriptor: Implements both the __get__ and __set__ methods. Why is it accessing the __dict__ attribute of the instance directly? Another good question, which also has at least two explanations. First, you might be thinking why not just do the following? setattr(instance, \"descriptor\", value) Remember that this method ( __set__ ) is called when we try to assign something to the attribute that is a descriptor. So, using setattr() will call this descriptor again, which, in turn, will call it again, and so on and so forth. This will end up in an infinite recursion. Why, then, is the descriptor not able to book-keep the values of the properties for all of its objects? The client class already has a reference to the descriptor. If we add a reference from the descriptor back to the client object, we are creating circular dependencies, and these objects will never be garbage-collected. Since they are pointing at each other, their reference counts will never drop below the threshold for removal, and that will cause memory leaks in our program. A possible alternative here is to use weak references, with the weakref module, and create a weak reference key dictionary if we want to do that. This implementation is explained later on in this chapter, but for the implementations within this book, we prefer to use this idiom (and not weakref ), since it is fairly common and accepted when writing descriptors. As of now, we have studied the different kinds of descriptors, what they are, and how they work, and we even got a first idea of how we can use them to our advantage. The next section emphasizes precisely that last point: we'll see descriptors in action. From now on, we'll take a more practical approach, and see how we can use descriptors to achieve better code. After that, we'll even explore examples of good descriptors. Functions and Methods The most resonating case of an object that is a descriptor is probably a function. Functions implement the __get__ method, so they can work as methods when defined inside a class. In Python, methods are just regular functions, only they take an extra argument. By convention, the first argument of a method is named self , and it represents an instance of the class that the method is being defined in. Then, whatever the method does with self would be the same as any other function receiving the object and applying modifications to it. In other words, when we define something like this: 1 2 3 class MyClass : def method ( self , ... ): self . x = 1 Since functions implement the descriptor protocol, before calling the method, the __get__ method is invoked first. Then, within this __get__ method, some transformations happen before running the code on the internal callable. Function as Descriptor 1 2 3 4 5 6 7 8 9 10 11 from types import MethodType class Method : def __init__ ( self , name ): self . name = name def __call__ ( self , instance , arg1 , arg2 ): print ( f \" { self . name } : { instance } called with { arg1 } and { arg2 } \" ) def __get__ ( self , instance , owner ): if instance is None : return self return MethodType ( self , instance ) Since this is a very elegant solution, it's worth exploring it to keep it in mind as a Pythonic approach when defining our own objects. For instance, if we were to define our own callable, it would be a good idea to also make it a descriptor so that we can use it in classes as class attributes as well. ```","title":"Descriptors"},{"location":"python/classes/descriptors/#descriptor-methods","text":"__get__(self, instance, owner) : The __get__ method of the descriptor class. It takes three arguments: self , instance (where the descriptor is called from), and owner (a reference to the class object). owner is the same as instance.__class__ . __set__(self, instance, value) : The __set__ method of the descriptor class. It is called when assigning a value to the descriptor. Example usage: client.descriptor = 'value' . __delete__(self, instance) : The __delete__ method of the descriptor class. It is called when deleting the descriptor. Example usage: del client.descriptor . __set_name__(self, owner, name) : The __set_name__ method of the descriptor class. It is called during the class creation and provides the field name. 1 2 3 4 5 6 7 8 9 10 11 12 13 class DescriptorWithName : def __init__ ( self , name ): self . name = name def __get__ ( self , instance , value ): if instance is None : return self print ( self . name , instance ) return instance . __dict__ [ self . name ] def __set__ ( self , instance , value ): instance . __dict__ [ self . name ] = value class ClientClass : descriptor = DescriptorWithName ( \"descriptor\" )","title":"Descriptor Methods"},{"location":"python/classes/descriptors/#descriptor-types","text":"Non-data descriptor: Implements only the __get__ method. Data descriptor: Implements both the __get__ and __set__ methods. Why is it accessing the __dict__ attribute of the instance directly? Another good question, which also has at least two explanations. First, you might be thinking why not just do the following? setattr(instance, \"descriptor\", value) Remember that this method ( __set__ ) is called when we try to assign something to the attribute that is a descriptor. So, using setattr() will call this descriptor again, which, in turn, will call it again, and so on and so forth. This will end up in an infinite recursion. Why, then, is the descriptor not able to book-keep the values of the properties for all of its objects? The client class already has a reference to the descriptor. If we add a reference from the descriptor back to the client object, we are creating circular dependencies, and these objects will never be garbage-collected. Since they are pointing at each other, their reference counts will never drop below the threshold for removal, and that will cause memory leaks in our program. A possible alternative here is to use weak references, with the weakref module, and create a weak reference key dictionary if we want to do that. This implementation is explained later on in this chapter, but for the implementations within this book, we prefer to use this idiom (and not weakref ), since it is fairly common and accepted when writing descriptors. As of now, we have studied the different kinds of descriptors, what they are, and how they work, and we even got a first idea of how we can use them to our advantage. The next section emphasizes precisely that last point: we'll see descriptors in action. From now on, we'll take a more practical approach, and see how we can use descriptors to achieve better code. After that, we'll even explore examples of good descriptors.","title":"Descriptor Types"},{"location":"python/classes/descriptors/#functions-and-methods","text":"The most resonating case of an object that is a descriptor is probably a function. Functions implement the __get__ method, so they can work as methods when defined inside a class. In Python, methods are just regular functions, only they take an extra argument. By convention, the first argument of a method is named self , and it represents an instance of the class that the method is being defined in. Then, whatever the method does with self would be the same as any other function receiving the object and applying modifications to it. In other words, when we define something like this: 1 2 3 class MyClass : def method ( self , ... ): self . x = 1 Since functions implement the descriptor protocol, before calling the method, the __get__ method is invoked first. Then, within this __get__ method, some transformations happen before running the code on the internal callable.","title":"Functions and Methods"},{"location":"python/classes/descriptors/#function-as-descriptor","text":"1 2 3 4 5 6 7 8 9 10 11 from types import MethodType class Method : def __init__ ( self , name ): self . name = name def __call__ ( self , instance , arg1 , arg2 ): print ( f \" { self . name } : { instance } called with { arg1 } and { arg2 } \" ) def __get__ ( self , instance , owner ): if instance is None : return self return MethodType ( self , instance ) Since this is a very elegant solution, it's worth exploring it to keep it in mind as a Pythonic approach when defining our own objects. For instance, if we were to define our own callable, it would be a good idea to also make it a descriptor so that we can use it in classes as class attributes as well. ```","title":"Function as Descriptor"},{"location":"python/classes/iterators/","text":"Iterators Generators Generators were introduced in Python a long time ago (PEP-255), with the idea of introducing iteration in Python while improving the performance of the program (by using less memory) at the same time. The idea of a generator is to create an object that is iterable and, while it's being iterated, will produce the elements it contains, one at a time. The main use of generators is to save memory\u2014instead of having a very large list of elements in memory, holding everything at once, we have an object that knows how to produce each particular element, one at a time, as it is required. This feature enables lazy computations of heavyweight objects in memory, in a similar manner to what other functional programming languages (Haskell, for instance) provide. It would even be possible to work with infinite sequences because the lazy nature of generators enables such an option. next() The next() built-in function will advance the iterable to its next element and return it. itertools 1 2 3 4 def process ( self ): for purchase in self . purchases : if purchase > 1000.0 : ... This is not only non-Pythonic, but it's also rigid (and rigidity is a trait that denotes bad code). It doesn't handle changes very well. What if the number changes now? Do we pass it by parameter? What if we need more than one? What if the condition is different (less than, for instance)? Do we pass a lambda? These questions should not be answered by this object, whose sole responsibility is to compute a set of well-defined metrics over a stream of purchases represented as numbers. And, of course, the answer is no. It would be a huge mistake to make such a change (once again, clean code is flexible, and we don't want to make it rigid by coupling this object to external factors). These requirements will have to be addressed elsewhere. itertools.islice - Takes First Ten 1 2 3 from itertools import islice purchases = islice ( filter ( lambda p : p > 1000.0 , purchases ), 10 ) There is no memory penalization for filtering this way because since they are all generators, the evaluation is always lazy. This gives us the power of thinking as if we had filtered the entire set at once and then passed it to the object, but without actually fitting everything in memory. Keep in mind the trade-off mentioned at the beginning of the chapter, between memory and CPU usage. While the code might use less memory, it could take up more CPU time, but most of the times, this is acceptable when we have to process lots of objects in memory while keeping the code maintainable. Repeated Iterations with itertools.tee 1 2 3 def process_purchases ( purchases ): min_ , max_ , avg = itertools . tee ( purchases , 3 ) return min ( min_ ), max ( max_ ), median ( avg ) In this example, itertools.tee will split the original iterable into three new ones. We will use each of these for the different kinds of iterations that we require, without needing to repeat three different loops over purchases. Yielding 1 2 3 4 def _iterate_array2d ( array2d ): for i , row in enumerate ( array2d ): for j , cell in enumerate ( row ): yield ( i , j ), cell 1 2 3 4 5 6 7 8 9 10 11 def search_nested ( array , desired_value ): try : coord = next ( coord for ( coord , cell ) in _iterate_array2d ( array ) if cell == desired_value ) except StopIteration as e : raise ValueError ( f \" { desired_value } not found\" ) from e logger . info ( \"value %r found at [ %i , %i ]\" , desired_value , * coord ) return coord Iterator but Not Iterable 1 2 3 4 5 6 7 8 class SequenceIterator : def __init__ ( self , start = 0 , step = 1 ): self . current = start self . step = step def __next__ ( self ): value = self . current self . current += self . step return value Sequence are Iterables 1 2 3 4 5 6 7 8 9 10 11 12 class MappedRange : \"\"\"Apply a transformation to a range of numbers.\"\"\" def __init__ ( self , transformation , start , end ): self . _transformation = transformation self . _wrapped = range ( start , end ) def __getitem__ ( self , index ): value = self . _wrapped . __getitem__ ( index ) result = self . _transformation ( value ) logger . info ( \"Index %d : %s \" , index , result ) return result def __len__ ( self ): return len ( self . _wrapped ) Coroutines .close() .throw() .send() Python takes advantage of generators in order to create coroutines. Because generators can naturally suspend, they're a convenient starting point. But generators weren't enough as they were originally thought to be, so these methods were added. This is because typically, it's not enough to just be able to suspend some part of the code; you'd also want to communicate with it (pass data and signal changes in the context). close() When calling this method, the generator will receive the GeneratorExit exception. If it's not handled, then the generator will finish without producing any more values, and its iteration will stop. throw() This method will throw the exception at the line where the generator is currently suspended. If the generator handles the exception that was sent, the code in that particular except clause will be called; otherwise, the exception will propagate to the caller. 1 2 3 4 5 6 7 8 9 10 def stream_data ( db_handler ): while True : try : yield db_handler . read_n_records ( 10 ) except CustomException as e : logger . info ( \"controlled error %r , continuing\" , e ) except Exception as e : logger . info ( \"unhandled error %r , stopping\" , e ) db_handler . close () break send(value) 1 2 3 4 5 6 7 8 9 10 11 12 def stream_db_records ( db_handler ): retrieved_data = None previous_page_size = 10 try : while True : page_size = yield retrieved_data if page_size is None : page_size = previous_page_size previous_page_size = page_size retrieved_data = db_handler . read_n_records ( page_size ) except GeneratorExit : db_handler . close () First None yield from yield from iterable Async Programming Async Context Managers 1 2 3 4 5 6 7 @contextlib . asynccontextmanager async def db_management (): try : await stop_database () yield finally : await start_database () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import asyncio import random async def coroutine (): await asyncio . sleep ( 0.1 ) return random . randint ( 1 , 10000 ) class RecordStreamer : def __init__ ( self , max_rows = 100 ): self . _current_row = 0 self . _max_rows = max_rows def __aiter__ ( self ): return self async def __anext__ ( self ): if self . _current_row < self . _max_rows : row = ( self . _current_row , await coroutine ()) self . _current_row += 1 return row raise StopAsyncIteration await async_iterator.__anext__() Async Generators 1 2 3 4 5 6 async def record_streamer ( max_rows ): current_row = 0 while current_row < max_rows : row = ( current_row , await coroutine ()) current_row += 1 yield row","title":"Iterators"},{"location":"python/classes/iterators/#iterators","text":"","title":"Iterators"},{"location":"python/classes/iterators/#generators","text":"Generators were introduced in Python a long time ago (PEP-255), with the idea of introducing iteration in Python while improving the performance of the program (by using less memory) at the same time. The idea of a generator is to create an object that is iterable and, while it's being iterated, will produce the elements it contains, one at a time. The main use of generators is to save memory\u2014instead of having a very large list of elements in memory, holding everything at once, we have an object that knows how to produce each particular element, one at a time, as it is required. This feature enables lazy computations of heavyweight objects in memory, in a similar manner to what other functional programming languages (Haskell, for instance) provide. It would even be possible to work with infinite sequences because the lazy nature of generators enables such an option.","title":"Generators"},{"location":"python/classes/iterators/#next","text":"The next() built-in function will advance the iterable to its next element and return it.","title":"next()"},{"location":"python/classes/iterators/#itertools","text":"1 2 3 4 def process ( self ): for purchase in self . purchases : if purchase > 1000.0 : ... This is not only non-Pythonic, but it's also rigid (and rigidity is a trait that denotes bad code). It doesn't handle changes very well. What if the number changes now? Do we pass it by parameter? What if we need more than one? What if the condition is different (less than, for instance)? Do we pass a lambda? These questions should not be answered by this object, whose sole responsibility is to compute a set of well-defined metrics over a stream of purchases represented as numbers. And, of course, the answer is no. It would be a huge mistake to make such a change (once again, clean code is flexible, and we don't want to make it rigid by coupling this object to external factors). These requirements will have to be addressed elsewhere.","title":"itertools"},{"location":"python/classes/iterators/#itertoolsislice-takes-first-ten","text":"1 2 3 from itertools import islice purchases = islice ( filter ( lambda p : p > 1000.0 , purchases ), 10 ) There is no memory penalization for filtering this way because since they are all generators, the evaluation is always lazy. This gives us the power of thinking as if we had filtered the entire set at once and then passed it to the object, but without actually fitting everything in memory. Keep in mind the trade-off mentioned at the beginning of the chapter, between memory and CPU usage. While the code might use less memory, it could take up more CPU time, but most of the times, this is acceptable when we have to process lots of objects in memory while keeping the code maintainable.","title":"itertools.islice - Takes First Ten"},{"location":"python/classes/iterators/#repeated-iterations-with-itertoolstee","text":"1 2 3 def process_purchases ( purchases ): min_ , max_ , avg = itertools . tee ( purchases , 3 ) return min ( min_ ), max ( max_ ), median ( avg ) In this example, itertools.tee will split the original iterable into three new ones. We will use each of these for the different kinds of iterations that we require, without needing to repeat three different loops over purchases.","title":"Repeated Iterations with itertools.tee"},{"location":"python/classes/iterators/#yielding","text":"1 2 3 4 def _iterate_array2d ( array2d ): for i , row in enumerate ( array2d ): for j , cell in enumerate ( row ): yield ( i , j ), cell 1 2 3 4 5 6 7 8 9 10 11 def search_nested ( array , desired_value ): try : coord = next ( coord for ( coord , cell ) in _iterate_array2d ( array ) if cell == desired_value ) except StopIteration as e : raise ValueError ( f \" { desired_value } not found\" ) from e logger . info ( \"value %r found at [ %i , %i ]\" , desired_value , * coord ) return coord","title":"Yielding"},{"location":"python/classes/iterators/#iterator-but-not-iterable","text":"1 2 3 4 5 6 7 8 class SequenceIterator : def __init__ ( self , start = 0 , step = 1 ): self . current = start self . step = step def __next__ ( self ): value = self . current self . current += self . step return value","title":"Iterator but Not Iterable"},{"location":"python/classes/iterators/#sequence-are-iterables","text":"1 2 3 4 5 6 7 8 9 10 11 12 class MappedRange : \"\"\"Apply a transformation to a range of numbers.\"\"\" def __init__ ( self , transformation , start , end ): self . _transformation = transformation self . _wrapped = range ( start , end ) def __getitem__ ( self , index ): value = self . _wrapped . __getitem__ ( index ) result = self . _transformation ( value ) logger . info ( \"Index %d : %s \" , index , result ) return result def __len__ ( self ): return len ( self . _wrapped )","title":"Sequence are Iterables"},{"location":"python/classes/iterators/#coroutines","text":".close() .throw() .send() Python takes advantage of generators in order to create coroutines. Because generators can naturally suspend, they're a convenient starting point. But generators weren't enough as they were originally thought to be, so these methods were added. This is because typically, it's not enough to just be able to suspend some part of the code; you'd also want to communicate with it (pass data and signal changes in the context).","title":"Coroutines"},{"location":"python/classes/iterators/#close","text":"When calling this method, the generator will receive the GeneratorExit exception. If it's not handled, then the generator will finish without producing any more values, and its iteration will stop.","title":"close()"},{"location":"python/classes/iterators/#throw","text":"This method will throw the exception at the line where the generator is currently suspended. If the generator handles the exception that was sent, the code in that particular except clause will be called; otherwise, the exception will propagate to the caller. 1 2 3 4 5 6 7 8 9 10 def stream_data ( db_handler ): while True : try : yield db_handler . read_n_records ( 10 ) except CustomException as e : logger . info ( \"controlled error %r , continuing\" , e ) except Exception as e : logger . info ( \"unhandled error %r , stopping\" , e ) db_handler . close () break","title":"throw()"},{"location":"python/classes/iterators/#sendvalue","text":"1 2 3 4 5 6 7 8 9 10 11 12 def stream_db_records ( db_handler ): retrieved_data = None previous_page_size = 10 try : while True : page_size = yield retrieved_data if page_size is None : page_size = previous_page_size previous_page_size = page_size retrieved_data = db_handler . read_n_records ( page_size ) except GeneratorExit : db_handler . close () First None","title":"send(value)"},{"location":"python/classes/iterators/#yield-from","text":"yield from iterable","title":"yield from"},{"location":"python/classes/iterators/#async-programming","text":"","title":"Async Programming"},{"location":"python/classes/iterators/#async-context-managers","text":"1 2 3 4 5 6 7 @contextlib . asynccontextmanager async def db_management (): try : await stop_database () yield finally : await start_database () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import asyncio import random async def coroutine (): await asyncio . sleep ( 0.1 ) return random . randint ( 1 , 10000 ) class RecordStreamer : def __init__ ( self , max_rows = 100 ): self . _current_row = 0 self . _max_rows = max_rows def __aiter__ ( self ): return self async def __anext__ ( self ): if self . _current_row < self . _max_rows : row = ( self . _current_row , await coroutine ()) self . _current_row += 1 return row raise StopAsyncIteration await async_iterator.__anext__()","title":"Async Context Managers"},{"location":"python/classes/iterators/#async-generators","text":"1 2 3 4 5 6 async def record_streamer ( max_rows ): current_row = 0 while current_row < max_rows : row = ( current_row , await coroutine ()) current_row += 1 yield row","title":"Async Generators"},{"location":"python/context_managers/context_managers/","text":"EXAMPLE 1 2 3 4 5 6 7 8 9 10 11 12 class ListTransaction : def __init__ ( self , thelist ): self . thelist = thelist def __enter__ ( self ): self . workingcopy = list ( self . thelist ) return self . workingcopy def __exit__ ( self , type , value , tb ): if type is None : self . thelist [:] = self . workingcopy return False READING FILES USING CONTEXT MANAGER WITH CHUNKS 1 2 3 with open ( 'data.txt' ) as file : while ( chunk := file . read ( 10000 )): print ( chunk , end = '' )","title":"Context Managers"},{"location":"python/context_managers/context_managers/#example","text":"1 2 3 4 5 6 7 8 9 10 11 12 class ListTransaction : def __init__ ( self , thelist ): self . thelist = thelist def __enter__ ( self ): self . workingcopy = list ( self . thelist ) return self . workingcopy def __exit__ ( self , type , value , tb ): if type is None : self . thelist [:] = self . workingcopy return False","title":"EXAMPLE"},{"location":"python/context_managers/context_managers/#reading-files-using-context-manager-with-chunks","text":"1 2 3 with open ( 'data.txt' ) as file : while ( chunk := file . read ( 10000 )): print ( chunk , end = '' )","title":"READING FILES USING CONTEXT MANAGER WITH CHUNKS"},{"location":"python/data_types/data_types/","text":"LITERALS 0b101010 # Binary integer 0o52 # Octal integer 0x2a # Hexadecimal integer","title":"Data Types"},{"location":"python/data_types/data_types/#literals","text":"0b101010 # Binary integer 0o52 # Octal integer 0x2a # Hexadecimal integer","title":"LITERALS"},{"location":"python/data_types/dic_set/","text":"dict vs set what is hashable An object is hashable if it has a hash value which never changes during its lifetime (it needs a hash() method), and can be compared to other objects (it needs an eq() method). Hashable objects which compare equal must have the same hash value. [...] User-defined types are hashable by default because their hash code is their id() and the eq() method inherited from the object class simply compares the object ids. If an object implements a custom eq() which takes into account its internal state, it will be hashable only if its hash() always returns the same hash code. In practice, this requires that eq() and hash() only take into account instance attributes that never change during the life of the object. missing keys with setdefault d.get(k, default) my_dict.setdefault(key, []).append(new_value) missing with missinng o subclass dict or any other mapping type and add a missing method. Both solutions are covered next. The missing method is only called by getitem (i.e., for the d[k] operator). The presence of a missing method has no effect on the behavior of other methods that look up keys, such as get or contains (which implements the in operator). This is why the default_factory of defaultdict works only with getitem, as noted in the warning at the end of the previous section. subclass builtin A better way to create a user-defined mapping type is to subclass collections.UserDict instead of dict (as we\u2019ll do in Example 3-8). Here we subclass dict just to show that missing is supported by the built-in dict.getitem method dict variations 1 2 3 4 collections.OrderedDict collections.ChainMap ChainMap(locals(), globals(), vars(builtins)) collections.Counter custom mapping 1 2 collections.UserDict typing.TypedDict The collections.UserDict class behaves like a dict, but it is slower because it is implemented in Python, not in C. We\u2019ll cover it in more detail next Set Theory Set elements must be hashable. The set type is not hashable, so you can\u2019t build a set with nested set instances. But frozenset is hashable, so you can have frozenset elements inside a set. n CPython built for a 64-bit CPU, each bucket in a set has two fields: a 64-bit hash code, and a 64-bit pointer to the element value\u2014which is a Python object stored elsewhere in memory. Because buckets have a fixed size, access to an individual bucket is done by offset. There is no field for the indexes from 0 to 7 The hash() built-in function works directly with built-in types and falls back to calling hash for user-defined types. If two objects compare equal, their hash codes must also be equal, otherwise the hash table algorithm does not work. For example, because 1 == 1.0 is True, hash(1) == hash(1.0) must also be True, even though the internal representation of an int and a float are very different. Also, to be effective as hash table indexes, hash codes should scatter around the index space as much as possible. This means that, ideally, objects that are similar but not equal should have hash codes that differ widely. Example 3-17 is the output of a script to compare the bit patterns of hash codes. Note how the hashes of 1 and 1.0 are the same, but those of 1.0001, 1.0002, and 1.0003 are very different. salt value of hash Starting with Python 3.3, a random salt value is included when computing hash codes for str, bytes, and datetime objects, as documented in Issue 13703\u2014Hash collision security issue. The salt value is constant within a Python process but varies between interpreter runs. With PEP-456, Python 3.4 adopted the SipHash cryptographic function to compute hash codes for str and bytes objects. The random salt and SipHash are security measures to prevent DoS attacks. Details are in a note in the documentation for the hash special method. hasing in python As mentioned earlier, the hash table for a set starts with 8 empty buckets. As elements are added, Python makes sure at least \u2153 of the buckets are empty\u2014doubling the size of the hash table when more space is needed. The hash code field of each bucket is initialized with -1, which means \u201cno hash code\u201d iven the literal {'Mon', 'Tue', 'Wed', 'Thu', 'Fri'}, Python gets the hash code for the first element, 'Mon'. For example, here is a realistic hash code for 'Mon'\u2014you\u2019ll probably get a different result because of the random salt Python uses to compute the hash code of string Python takes the modulus of the hash code with the table size to find a hash table index. Here the table size is 8, and the modulus is 3: Probing consists of computing the index from the hash, then looking at the corresponding bucket in the hash table. In this case, Python looks at the bucket at offset 3 and finds -1 in the hash code field, marking an empty bucke Python stores the hash code of the new element, 4199492796428269555, in the hash code field at offset 3, and a pointer to the string object 'Mon' in the element field. Figure 3-5 shows the current state of the hash table For the second element, 'Tue', steps 1, 2, 3 above are repeated. The hash code for 'Tue' is 2414279730484651250, and the resulting index is 2. When adding 'Wed' to the set, Python computes the hash -5145319347887138165 and index 3. Python probes bucket 3 and sees that it is already taken. But the hash code stored there, 4199492796428269555 is different. As discussed in \u201cHashes and equality\u201d, if two objects have different hashes, then their value is also different. This is an index collision. Python then probes the next bucket and finds it empty. So 'Wed' ends up at index 4, as shown in Figure 3-7. Adding the next element, 'Thu', is boring: there\u2019s no collision, and it lands in its natural bucket, at index 7. Placing 'Fri' is more interesting. Its hash, 7021641685991143771 implies index 3, which is taken by 'Mon'. Probing the next bucket\u20144\u2014 Python finds the hash for 'Wed' stored there. The hash codes don\u2019t match, so this is another index collision. Python probes the next bucket. It\u2019s empty, so 'Fri' ends up at index 5. The end state of the hash table is shown in Figure 3-8. ahsh table for the set {'Mon', 'Tue', 'Wed', 'Thu', 'Fri'}. It is now 62.5% full\u2014close to the \u2154 threshold.","title":"Dic Set"},{"location":"python/data_types/list_tuple/","text":"LIST COMPREHENSION 1 2 3 [ expression for item1 in iterable1 if condition1 for item2 in iterable2 if condition2 ]","title":"List Tuple"},{"location":"python/data_types/list_tuple/#list-comprehension","text":"1 2 3 [ expression for item1 in iterable1 if condition1 for item2 in iterable2 if condition2 ]","title":"LIST COMPREHENSION"},{"location":"python/data_types/object/","text":"Variables Are Not Boxes In 1997, I took a summer course on Java at MIT. The professor, Lynn Andrea Stein\u2014an award-winning computer science educator who currently teaches at Olin College of Engineering\u2014made the point that the usual \u201cvariables as boxes\u201d metaphor actually hinders the understanding of reference variables in OO languages. Python variables are like reference variables in Java, so it\u2019s better to think of them as labels attached to objects sentiel obj END_OF_DATA = object() ... many lines def traverse(...): ... more lines if node is END_OF_DATA: raise StopIteration etc. Copies Are Shallow by Default Function Parameters as References In CPython, the primary algorithm for garbage collection is reference counting. Essentially, each object keeps count of how many references point to it. As soon as that refcount reaches zero, the object is immediately destroyed: CPython calls the del method on the object (if defined) and then frees the memory allocated to the object. In CPython 2.0, a generational garbage collection algorithm was added to detect groups of objects involved in reference cycles\u2014which may be unreachable even with outstanding references to them, when all the mutual references are contained within the group. Other implementations of Python have more sophisticated garbage collectors that do not rely on reference counting, which means the del method may not be called immediately when there are no more references to the object. See \u201cPyPy, Garbage Collection, wref = weakref.ref(a_set) weakref.finalize to register a callback function to be called when an object is destroyed The WeakValueDictionary Skit The class WeakValueDictionary implements a mutable mapping where the values are weak references to objects. When a referred object is garbage collected elsewhere in the program, the corresponding key is automatically removed from WeakValueDictionary. This is commonly used for caching. Our demonstration of a WeakValueDictionary is inspired by the classic Cheese Shop skit by Monty Python, in which a customer asks for more than 40 kinds of cheese, including cheddar and mozzarella, but none are in stock. import weakref 1 stock = weakref.WeakValueDictionary() catalog = [Cheese('Red Leicester'), Cheese('Tilsit'), ... Cheese('Brie'), Cheese('Parmesan')] ... for cheese in catalog: ... stock[cheese.kind] = cheese ... sorted(stock.keys()) ['Brie', 'Parmesan', 'Red Leicester', 'Tilsit'] del catalog sorted(stock.keys()) ['Parmesan'] del cheese sorted(stock.keys()) A temporary variable may cause an object to last longer than expected by holding a reference to it. This is usually not a problem with local variables: they are destroyed when the function returns. But in Example 6-19, the for loop variable cheese is a global variable and will never go away unless explicitly deleted Not every Python object may be the target, or referent, of a weak reference. Basic list and dict instances may not be referents, but a plain subclass of either can solve this problem easily class MyList(list): \"\"\"list subclass whose instances may be weakly referenced\"\"\" a_list = MyList(range(10)) a_list can be the target of a weak reference wref_to_a_list = weakref.ref(a_list) I was surprised to learn that, for a tuple t, t[:] does not make a copy, but returns a reference to the same object","title":"Object"},{"location":"python/data_types/object/#many-lines","text":"def traverse(...):","title":"... many lines"},{"location":"python/data_types/object/#more-lines","text":"if node is END_OF_DATA: raise StopIteration","title":"... more lines"},{"location":"python/data_types/object/#etc","text":"Copies Are Shallow by Default Function Parameters as References In CPython, the primary algorithm for garbage collection is reference counting. Essentially, each object keeps count of how many references point to it. As soon as that refcount reaches zero, the object is immediately destroyed: CPython calls the del method on the object (if defined) and then frees the memory allocated to the object. In CPython 2.0, a generational garbage collection algorithm was added to detect groups of objects involved in reference cycles\u2014which may be unreachable even with outstanding references to them, when all the mutual references are contained within the group. Other implementations of Python have more sophisticated garbage collectors that do not rely on reference counting, which means the del method may not be called immediately when there are no more references to the object. See \u201cPyPy, Garbage Collection, wref = weakref.ref(a_set) weakref.finalize to register a callback function to be called when an object is destroyed The WeakValueDictionary Skit The class WeakValueDictionary implements a mutable mapping where the values are weak references to objects. When a referred object is garbage collected elsewhere in the program, the corresponding key is automatically removed from WeakValueDictionary. This is commonly used for caching. Our demonstration of a WeakValueDictionary is inspired by the classic Cheese Shop skit by Monty Python, in which a customer asks for more than 40 kinds of cheese, including cheddar and mozzarella, but none are in stock. import weakref 1 stock = weakref.WeakValueDictionary() catalog = [Cheese('Red Leicester'), Cheese('Tilsit'), ... Cheese('Brie'), Cheese('Parmesan')] ... for cheese in catalog: ... stock[cheese.kind] = cheese ... sorted(stock.keys()) ['Brie', 'Parmesan', 'Red Leicester', 'Tilsit'] del catalog sorted(stock.keys()) ['Parmesan'] del cheese sorted(stock.keys()) A temporary variable may cause an object to last longer than expected by holding a reference to it. This is usually not a problem with local variables: they are destroyed when the function returns. But in Example 6-19, the for loop variable cheese is a global variable and will never go away unless explicitly deleted Not every Python object may be the target, or referent, of a weak reference. Basic list and dict instances may not be referents, but a plain subclass of either can solve this problem easily class MyList(list): \"\"\"list subclass whose instances may be weakly referenced\"\"\" a_list = MyList(range(10)) a_list can be the target of a weak reference wref_to_a_list = weakref.ref(a_list) I was surprised to learn that, for a tuple t, t[:] does not make a copy, but returns a reference to the same object","title":"etc."},{"location":"python/decorators/decorators/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @dataclass class Serializer : def __init__ ( self , dict_values ): self . values = dict_values def serialize ( self , object ): return [ trans ( getattr ( object , field )) for field , trans in self . values . items ()] class Serialize : def __init__ ( self , ** trans ) -> None : self . serializer = Serializer ( trans ) def __call__ ( self , object ): print ( object ) ## Event def wrapper ( instance ): # Intance return self . serializer . serialize ( instance ) object . serialize = wrapper return object def serialize ( ** trans ): serializer = Serializer ( trans ) def wrapper ( class_obj ): def inner ( instance ): return serializer . serialize ( instance ) class_obj . serialize = inner return class_obj return wrapper @serialize ( username = str , password = str , ip = str ) @dataclass class Event : username : int password : int ip : int x = Event ( 11 , 33 , 444 ) print ( x . serialize ()) wrapper coroutines 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import inspect def timing ( callable ): @wraps ( callable ) def wrapped ( * args , ** kwargs ): start = time . time () result = callable ( * args , ** kwargs ) latency = time . time () - start return { \"latency\" : latency , \"result\" : result } @wraps ( callable ) async def wrapped_coro ( * args , ** kwargs ): start = time . time () result = await callable ( * args , ** kwargs ) latency = time . time () - start return { \"latency\" : latency , \"result\" : result } if inspect . iscoroutinefunction ( callable ): return wrapped_coro return wrapped extended syntax for decorators 1 2 3 4 5 6 7 def _log ( f , * args , ** kwargs ): print ( f \"calling { f . __qualname__ !r} with { args =} and { kwargs =} \" ) return f ( * args , ** kwargs ) @ ( lambda f : lambda * args , ** kwargs : _log ( f , * args , ** kwargs )) def func ( x ): return x + 1 same decorator for function and class 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from functools import wraps from types import MethodType class inject_db_driver : def __init__ ( self , function ): self . function = function wraps ( self . function )( self ) def __call__ ( self , dbstring ): print ( dbstring ) return self . function ( lambda dbstring : dbstring ) def __get__ ( self , instance , owner ): print ( \"dd\" ) if instance is None : return self print ( MethodType ( self . function , instance )) return self . __class__ ( MethodType ( self . function , instance )) @inject_db_driver def run_query ( driver ): return \"test\" class DataHandler : @inject_db_driver def run_query ( self , driver ): return \"test\" # run_query(\"dato\") x = DataHandler () x . run_query ( \"dato\" ) composition over inheritance 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from dataclasses import dataclass class BaseResolverMixin : def __getattr__ ( self , attr : str ): if attr . startswith ( \"resolve_\" ): * _ , actual_attr = attr . partition ( \"resolve_\" ) else : actual_attr = attr try : return self . __dict__ [ actual_attr ] except KeyError as e : raise AttributeError from e @dataclass class Customer ( BaseResolverMixin ): customer_id : str name : str address : str ####### def _resolver_method ( self , attr ): if attr . startswith ( \"resolve_\" ): * _ , actual_attr = attr . partition ( \"resolve_\" ) else : actual_attr = attr try : return self . __dict__ [ actual_attr ] except KeyError as e : raise AttributeError from e def with_resolver ( cls ): cls . __getattr__ = _resolver_method return cls @dataclass @with_resolver class Customer ( BaseResolverMixin ): customer_id : str name : str address : str","title":"Decorators"},{"location":"python/decorators/decorators/#wrapper-coroutines","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import inspect def timing ( callable ): @wraps ( callable ) def wrapped ( * args , ** kwargs ): start = time . time () result = callable ( * args , ** kwargs ) latency = time . time () - start return { \"latency\" : latency , \"result\" : result } @wraps ( callable ) async def wrapped_coro ( * args , ** kwargs ): start = time . time () result = await callable ( * args , ** kwargs ) latency = time . time () - start return { \"latency\" : latency , \"result\" : result } if inspect . iscoroutinefunction ( callable ): return wrapped_coro return wrapped","title":"wrapper coroutines"},{"location":"python/decorators/decorators/#extended-syntax-for-decorators","text":"1 2 3 4 5 6 7 def _log ( f , * args , ** kwargs ): print ( f \"calling { f . __qualname__ !r} with { args =} and { kwargs =} \" ) return f ( * args , ** kwargs ) @ ( lambda f : lambda * args , ** kwargs : _log ( f , * args , ** kwargs )) def func ( x ): return x + 1","title":"extended syntax for decorators"},{"location":"python/decorators/decorators/#same-decorator-for-function-and-class","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from functools import wraps from types import MethodType class inject_db_driver : def __init__ ( self , function ): self . function = function wraps ( self . function )( self ) def __call__ ( self , dbstring ): print ( dbstring ) return self . function ( lambda dbstring : dbstring ) def __get__ ( self , instance , owner ): print ( \"dd\" ) if instance is None : return self print ( MethodType ( self . function , instance )) return self . __class__ ( MethodType ( self . function , instance )) @inject_db_driver def run_query ( driver ): return \"test\" class DataHandler : @inject_db_driver def run_query ( self , driver ): return \"test\" # run_query(\"dato\") x = DataHandler () x . run_query ( \"dato\" )","title":"same decorator for function and class"},{"location":"python/decorators/decorators/#composition-over-inheritance","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from dataclasses import dataclass class BaseResolverMixin : def __getattr__ ( self , attr : str ): if attr . startswith ( \"resolve_\" ): * _ , actual_attr = attr . partition ( \"resolve_\" ) else : actual_attr = attr try : return self . __dict__ [ actual_attr ] except KeyError as e : raise AttributeError from e @dataclass class Customer ( BaseResolverMixin ): customer_id : str name : str address : str ####### def _resolver_method ( self , attr ): if attr . startswith ( \"resolve_\" ): * _ , actual_attr = attr . partition ( \"resolve_\" ) else : actual_attr = attr try : return self . __dict__ [ actual_attr ] except KeyError as e : raise AttributeError from e def with_resolver ( cls ): cls . __getattr__ = _resolver_method return cls @dataclass @with_resolver class Customer ( BaseResolverMixin ): customer_id : str name : str address : str","title":"composition over inheritance"},{"location":"python/functions/","text":"RECURSION current limit sys.getrecursionlimit() default is 1000 set limit sys.setrecursionlimit() LAMBDA FUNCTIONS 1 2 3 4 x = 2 f = lambda y : x * y x = 3 g = lambda y : x * y print(f(10)) # --> prints 30 print(g(10)) # --> prints 30 TODO late binding INNER FUNCTIONS nonlocal cannot be used to refer to a global variable\u2014it must reference a local variable in an outer scope. Thus, if a function is assigning to a global, you should still use the global declaration Use of nested functions and nonlocal declarations is not a common programming style. For example, inner functions have no outside visibility, which can complicate testing and debugging. Nevertheless, nested functions are sometimes useful for breaking INSPECTION 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 f.__name__ Function name f.__qualname__ Fully qualified name (if nested) f.__module__ Name of module in which defined f.__doc__ Documentation string f.__annotations__ Type hints f.__globals__ Dictionary that is the global namespace f.__closure__ Closure variables (if any) f.__code__ CHECK FUNCTION PARAMETERS 1 2 3 4 5 import inspect def func ( x : int , y : float , debug = False ) -> float : pass sig = inspect . signature ( func ) assert inspect . signature ( func1 ) == inspect . signature ( func2 ) GET CURRENT FRAME LOCALS 1 2 3 4 5 6 7 def spam ( x , y ): z = x + y grok ( z ) def grok ( a ): b = a * 10 # outputs: {'a':5, 'b':50 } print ( inspect . currentframe () . f_locals ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 f.f_back Previous stack frame (toward the caller) f.f_code Code object being executed f.f_locals Dictionary of local variables (locals()) f.f_globals Dictionary used for global variables (globals()) f.f_builtins Dictionary used for built-in names f.f_lineno Line number f.f_lasti Current instruction. This is an index into the bytecode string of f_code. f.f_trace Function called at start of each source code line","title":"Index"},{"location":"python/functions/#recursion","text":"current limit sys.getrecursionlimit() default is 1000 set limit sys.setrecursionlimit()","title":"RECURSION"},{"location":"python/functions/#lambda-functions","text":"1 2 3 4 x = 2 f = lambda y : x * y x = 3 g = lambda y : x * y print(f(10)) # --> prints 30 print(g(10)) # --> prints 30","title":"LAMBDA FUNCTIONS"},{"location":"python/functions/#todo-late-binding","text":"","title":"TODO late binding"},{"location":"python/functions/#inner-functions","text":"nonlocal cannot be used to refer to a global variable\u2014it must reference a local variable in an outer scope. Thus, if a function is assigning to a global, you should still use the global declaration Use of nested functions and nonlocal declarations is not a common programming style. For example, inner functions have no outside visibility, which can complicate testing and debugging. Nevertheless, nested functions are sometimes useful for breaking","title":"INNER FUNCTIONS"},{"location":"python/functions/#inspection","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 f.__name__ Function name f.__qualname__ Fully qualified name (if nested) f.__module__ Name of module in which defined f.__doc__ Documentation string f.__annotations__ Type hints f.__globals__ Dictionary that is the global namespace f.__closure__ Closure variables (if any) f.__code__","title":"INSPECTION"},{"location":"python/functions/#check-function-parameters","text":"1 2 3 4 5 import inspect def func ( x : int , y : float , debug = False ) -> float : pass sig = inspect . signature ( func ) assert inspect . signature ( func1 ) == inspect . signature ( func2 )","title":"CHECK  FUNCTION PARAMETERS"},{"location":"python/functions/#get-current-frame-locals","text":"1 2 3 4 5 6 7 def spam ( x , y ): z = x + y grok ( z ) def grok ( a ): b = a * 10 # outputs: {'a':5, 'b':50 } print ( inspect . currentframe () . f_locals ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 f.f_back Previous stack frame (toward the caller) f.f_code Code object being executed f.f_locals Dictionary of local variables (locals()) f.f_globals Dictionary used for global variables (globals()) f.f_builtins Dictionary used for built-in names f.f_lineno Line number f.f_lasti Current instruction. This is an index into the bytecode string of f_code. f.f_trace Function called at start of each source code line","title":"GET CURRENT FRAME LOCALS"},{"location":"python/modules/builtins/","text":"Builtins abs(x) Returns the absolute value of x . all(s) Returns True if all of the values in the iterable s evaluate as True . Returns True if s is empty. any(s) Returns True if any of the values in the iterable s evaluate as True . Returns False if s is empty. ascii(x) Creates a printable representation of the object x just like the repr() , but only uses ASCII characters in the result. Non-ASCII characters are turned into appropriate escape sequences. This can be used to view Unicode strings in a terminal or shell that doesn\u2019t support Unicode. bin(x) Returns a string with the binary representation of the integer x . bool([x]) Type representing Boolean values True and False . If used to convert x , it returns True if x evaluates to true using the usual truth-testing semantics\u2014that is, nonzero number, nonempty list, and so on. Otherwise, False is returned. False is also the default value returned if bool() is called without any arguments. The bool class inherits from int , so the Boolean values True and False can be used as integers with values 1 and 0 in mathematical calculations. breakpoint() Sets a manual debugger breakpoint. When encountered, control will transfer to pdb , the Python debugger. bytearray([x]) A type representing a mutable array of bytes. When creating an instance, x may either be an iterable sequence of integers in the range 0 to 255, an 8-bit string or bytes literal, or an integer that specifies the size of the byte array (in which case every entry will be initialized to 0). bytearray(s, encoding) An alternative calling convention for creating a bytearray instance from characters in a string s where encoding specifies the character encoding to use in the conversion. bytes([x]) A type representing an immutable array of bytes. bytes(s, encoding) An alternate calling convention for creating bytes from a string s where encoding specifies the encoding to use in conversion. Table 10.1 shows operations supported by both bytes and byte arrays. Table 10.1: Operations on Bytes and Bytearrays Operation Description s + t Concatenates if t is bytes. s * n Replicates if n is an integer. s % x Formats bytes. x is tuple. s[i] Returns element i as an integer. s[i:j] Returns a slice. s[i:j:stride] Returns an extended slice. len(s) Number of bytes in s . s.capitalize() Capitalizes the first character. s.center(width [, pad]) Centers the string in a field of length width . pad is a padding character. s.count(sub [, start [, end]]) Counts occurrences of the specified substring sub . s.decode([encoding [, errors]]) Decodes a byte string into text (bytes type only). s.endswith(suffix [, start [, end]]) Checks the end of the string for a suffix. s.expandtabs([tabsize]) Replaces tabs with spaces. s.find(sub [, start [, end]]) Finds the first occurrence of the specified substring sub . s.hex() Converts to a hexadecimal string. s.index(sub [, start [, end]]) Finds the first occurrence or error in the specified substring sub . s.isalnum() Checks whether all characters are alphanumeric. s.isalpha() Checks whether all characters are alphabetic. s.isascii() Checks whether all characters are ASCII. s.isdigit() Checks whether all characters are digits. s.islower() Checks whether all characters are lowercase. s.isspace() Checks whether all characters are whitespace. s.istitle() Checks whether the string is a title-cased string (first letter of each word capitalized). s.isupper() Checks whether all characters are uppercase. s.join(t) Joins a sequence of strings t using a delimiter s . s.ljust(width [, fill]) Left-aligns s in a string of size width . fill is a padding character. s.lower() Converts to lowercase. s.lstrip([chrs]) Removes leading whitespace or characters supplied in chrs . s.maketrans(x [, y [, z]]) Makes a translation table for s.translate() . s.partition(sep) Partitions a string based on a separator string sep . Returns a tuple (head, sep, tail) or (s, '', '') if sep isn\u2019t found. s.removeprefix(prefix) Returns s with a given prefix removed if present. s.removesuffix(suffix) Returns s with a given suffix removed if present. s.replace(old, new [, maxreplace]) Replaces a substring. s.rfind(sub [, start [, end]]) Finds the last occurrence of a substring. s.rindex(sub [, start [, end]]) Finds the last occurrence or raises an error. s.rjust(width [, fill]) Right-aligns s in a string of length width . fill is a padding character. s.rpartition(sep) Partitions s based on a separator sep , but searches from the end of the string. s.rsplit([sep [, maxsplit]]) Splits a string from the end of the string using sep as a delimiter. maxsplit is the maximum number of splits to perform. If maxsplit is omitted, the result is identical to the split() method. s.rstrip([chrs]) Removes trailing whitespace or characters supplied in chrs . s.split([sep [, maxsplit]]) Splits a string using sep as a delimiter. maxsplit is the maximum number of splits to perform. s.splitlines([keepends]) Splits a string into a list of lines. If keepends is 1 , trailing newlines are preserved. s.startswith(prefix [, start [, end]]) Checks whether a string starts with prefix . s.strip([chrs]) Removes leading and trailing whitespace or characters supplied in chrs . s.swapcase() Converts uppercase to lowercase, and vice versa. s.title() Returns a title-cased version of the string. s.translate(table [, deletechars]) Translates a string using a character translation table table , removing characters in deletechars . s.upper() Converts a string to uppercase. s.zfill(width) Pads a string with zeros on the left up to the specified width . Byte arrays additionally support the methods in Table 10.2. Table 10.2: Additional Operations on Byte Arrays Operation Description s[i] = v Item assignment. s[i:j] = t Slice assignment. s[i:j:stride] = t Extended slice assignment. del s[i] Item deletion. del s[i:j] Slice deletion. del s[i:j:stride] Extended slice deletion. s.append(x) Appends a new byte to the end. s.clear() Clears the byte array. s.copy() Makes a copy. s.extend(t) Extends s with bytes from t . s.insert(n, x) Inserts byte x at index n . s.pop([n]) Removes and returns byte at index n . s.remove(x) Removes first occurrence of byte x . s.reverse() Reverses the byte array in-place. callable(obj) Returns True if obj is callable as a function. chr(x) Converts the integer x representing a Unicode code-point into a single-character string. callable(obj) Returns True if obj is callable as a function. chr(x) Converts the integer x representing a Unicode code-point into a single-character string. classmethod(func) This decorator creates a class method for the function func . It is typically only used inside class definitions where it is implicitly invoked using @classmethod . Unlike a normal method, a class method receives the class as the first argument, not an instance. compile(string, filename, kind) Compiles string into a code object for use with exec() or eval() . string is a string containing valid Python code. If this code spans multiple lines, the lines must be terminated by a single newline ('\\n') and not platform-specific variants (for example, '\\r\\n' on Windows). filename is a string containing the name of the file in which the string was defined (if any). kind is 'exec' for a sequence of statements, 'eval' for a single expression, or 'single' for a single executable statement. The resulting code object that is returned can be directly passed to exec() or eval() in place of a string. complex([real [, imag]]) Type representing a complex number with real and imaginary components, real and imag , which can be supplied as any numeric type. If imag is omitted, the imaginary component is set to zero. If real is passed as a string, the string is parsed and converted to a complex number. In this case, imag should be omitted. If real is any other kind of object, the value of real.complex() is returned. If no arguments are given, 0j is returned. Table 10.3: Attributes of complex Attribute/Method Description z.real Real component z.imag Imaginary component z.conjugate() Conjugates as a complex number Click here to view code image. delattr(object, attr) Deletes an attribute of an object. attr is a string. Same as del object.attr . Click here to view code image. dict([m]) or dict(key1=value1, key2=value2, ...) Type representing a dictionary. If no argument is given, an empty dictionary is returned. If m is a mapping object (such as another dictionary), a new dictionary having the same keys and same values as m is returned. For example, if m is a dictionary, dict(m) makes a shallow copy of it. If m is not a mapping, it must support iteration in which a sequence of (key, value) pairs is produced. These pairs are used to populate the dictionary. dict() can also be called with keyword arguments. For example, dict(foo=3, bar=7) creates the dictionary {'foo': 3, 'bar': 7} . Table 10.4: Operations on Dictionaries Operation Description m | n Merges m and n into a single dictionary. len(m) Returns the number of items in m . m[k] Returns the item of m with key k . m[k]=x Sets m[k] to x . del m[k] Removes m[k] from m . k in m Returns True if k is a key in m . m.clear() Removes all items from m . m.copy() Makes a shallow copy of m . m.fromkeys(s [, value]) Creates a new dictionary with keys from sequence s and values all set to value . m.get(k [, v]) Returns m[k] if found; otherwise, returns v . m.items() Returns (key, value) pairs. m.keys() Returns the keys. m.pop(k [, default]) Returns m[k] if found and removes it from m ; otherwise, returns default if supplied or raises KeyError if not. m.popitem() Removes a random (key, value) pair from m and returns it as a tuple. m.setdefault(k [, v]) Returns m[k] if found; otherwise, returns v and sets m[k] = v . m.update(b) Adds all objects from b to m . m.values() Returns the values. dir([object]) Returns a sorted list of attribute names. If object is a module, it contains the list of symbols defined in that module. If object is a type or class object, it returns a list of attribute names. The names are typically obtained from the object\u2019s dict attribute if defined, but other sources may be used. If no argument is given, the names in the current local symbol table are returned. It should be noted that this function is primarily used for informational purposes (for example, used interactively at the command line). It should not be used for formal program analysis because the information obtained may be incomplete. Also, user-defined classes can define a special method dir() that alters the result of this function. divmod(a, b) Returns the quotient and remainder of long division as a tuple. For integers, the value (a // b, a % b) is returned. For floats, (math.floor(a / b), a % b) is returned. This function may not be called with complex numbers. Click here to view code image. enumerate(iter, start=0) Given an iterable object, iter , returns a new iterator (of type enumerate ) that produces tuples containing a count and the value produced from iter . For example, if iter produces a, b, c , then enumerate(iter) produces (0,a) , (1,b) , (2,c) . The optional start changes the initial value of the count. Click here to view code image. eval(expr [, globals [, locals]]) Evaluates an expression. expr is a string or a code object created by compile() . globals and locals are mapping objects that define the global and local namespaces, respectively, for the operation. If omitted, the expression is evaluated using the values of globals() and locals() as executed in the caller\u2019s environment. It is most common for globals and locals to be specified as dictionaries, but advanced applications can supply custom mapping objects. Click here to view code image. exec(code [, global [, locals]]) Executes Python statements. code is a string, bytes, or a code object created by compile() . globals and locals define the global and local namespaces, respectively, for the operation. If omitted, the code is executed using the values of globals() and locals() as executed in the caller\u2019s environment. Click here to view code image. filter(function, iterable) Creates an iterator that returns the items in iterable for which function(item) evaluates as True . float([x]) Type representing a floating-point number. If x is a number, it is converted to a float. If x is a string, it is parsed into a float. For all other objects, x.float() is invoked. If no argument is supplied, 0.0 is returned. Table 10.5: Methods and Attributes of Floats Attribute/Method Description x.real Real component when used as a complex. x.imag Imaginary component when used as a complex. x.conjugate() Conjugates as a complex number. x.as_integer_ratio() Converts to numerator/denominator pair. x.hex() Creates a hexadecimal representation. x.is_integer() Tests if an exact integer value. float.fromhex(s) Creates from a hexadecimal string. A class method. Python Built-in Functions bytes([source[, encoding[, errors]]]) Constructs a new bytes object. The source parameter can be used to initialize the bytes object from a sequence of integers or another object that implements the buffer protocol. If source is specified, the encoding and errors parameters must not be specified. callable(obj) Returns True if obj is callable as a function. chr(x) Converts the integer x representing a Unicode code-point into a single-character string. classmethod(func) This decorator creates a class method for the function func . It is typically only used inside class definitions where it is implicitly invoked using @classmethod . Unlike a normal method, a class method receives the class as the first argument, not an instance. compile(string, filename, kind) Compiles string into a code object for use with exec() or eval() . string is a string containing valid Python code. If this code spans multiple lines, the lines must be terminated by a single newline ( '\\n' ) and not platform-specific variants (for example, '\\r\\n' on Windows). filename is a string containing the name of the file in which the string was defined (if any). kind is 'exec' for a sequence of statements, 'eval' for a single expression, or 'single' for a single executable statement. The resulting code object that is returned can be directly passed to exec() or eval() in place of a string. complex([real [, imag]]) Type representing a complex number with real and imaginary components, real and imag , which can be supplied as any numeric type. If imag is omitted, the imaginary component is set to zero. If real is passed as a string, the string is parsed and converted to a complex number. In this case, imag should be omitted. If real is any other kind of object, the value of real.complex() is returned. If no arguments are given, 0j is returned. delattr(object, attr) Deletes an attribute of an object. attr is a string. Same as del object.attr . dict([m]) or dict(key1=value1, key2=value2, ...) Type representing a dictionary. If no argument is given, an empty dictionary is returned. If m is a mapping object (such as another dictionary), a new dictionary having the same keys and same values as m is returned. For example, if m is a dictionary, dict(m) makes a shallow copy of it. If m is not a mapping, it must support iteration in which a sequence of (key, value) pairs is produced. These pairs are used to populate the dictionary. dict() can also be called with keyword arguments. For example, dict(foo=3, bar=7) creates the dictionary {'foo': 3, 'bar': 7} . dir([object]) Returns a sorted list of attribute names. If object is a module, it contains the list of symbols defined in that module. If object is a type or class object, it returns a list of attribute names. The names are typically obtained from the object\u2019s dict attribute if defined, but other sources may be used. If no argument is given, the names in the current local symbol table are returned. It should be noted that this function is primarily used for informational purposes (for example, used interactively at a Python prompt). divmod(x, y) Returns a pair of numbers (q, r) such that x = y * q + r . If x and y are integers, the return value is also an integer. For example, divmod(7, 3) returns (2, 1) , where 2 is the quotient and 1 is the remainder. enumerate(iterable[, start]) Returns an iterator that generates pairs consisting of an index and an item from the iterable . The start parameter is an optional integer that specifies the starting value of the index. By default, it is 0 . eval(expression[, globals[, locals]]) Evaluates the expression in the given globals and locals namespaces. expression can be a string or a code object. If globals is specified, it must be a dictionary. If locals is specified, it can be any mapping object. If both globals and locals are omitted, the expression is evaluated in the context of the current global and local namespaces. exec(object[, globals[, locals]]) Evaluates the object as a Python expression or statement. object can be a string or a code object. If globals is specified, it must be a dictionary. If locals is specified, it can be any mapping object. If both globals and locals are omitted, the code is executed in the context of the current global and local namespaces. filter(function, iterable) Creates an iterator that produces the values from iterable for which function returns True . If function is None , the identity function is assumed, which returns True for all elements of the iterable. If iterable is a string, the resulting iterator produces the individual characters of the string. float([x]) Type representing a floating-point number. If no argument is given, 0.0 is returned. If x is a number, it is converted to a floating-point number. If x is a string, it is parsed and converted to a floating-point number. format(value[, format_spec]) Converts value to a formatted string according to the format specification string in format_spec . This operation invokes value.format() , which is free to interpret the format specification as it sees fit. For simple types of data, the format specifier typically includes an alignment character of < , > , or ^ , a number (which indicates the field width), and a character code of d , f , or s for integer, floating point, or string values, respectively. For example, a format specification of 'd' formats an integer, a specification of '8d' right-aligns an integer in an 8-character field, and '<8d' left-aligns an integer in an 8-character field. More details on format() and format specifiers can be found in Chapter 9. frozenset([iterable]) Type representing an immutable set object populated with values taken from iterable . The values must also be immutable. If no argument is given, an empty set is returned. A frozenset supports all of the operations found on sets except for any operations that mutate a set in-place. getattr(object, name[, default]) Returns the value of a named attribute of an object. name is a string containing the attribute name. default is an optional value to return if no such attribute exists; Table 10.8 shows operations on sets. Table 10.8 Set Operations and Methods Operation Description s | t Union s & t Intersection s - t Difference s ^ t Symmetric difference len(s) Returns number of items in s. s.add(item) Adds item to s. Has no effect if item is already in s. s.clear() Removes all items from s. s.copy() Makes a copy of s. s.difference(t) Set difference. Returns all the items in s, but not in t. s.difference_update(t) Removes all the items from s that are also in t. s.discard(item) Removes item from s. If item is not a member of s, nothing happens. s.intersection(t) Intersection. Returns all the items that are both in s and in t. s.intersection_update(t) Computes the intersection of s and t and leaves the result in s. s.isdisjoint(t) Returns True if s and t have no items in common. s.issubset(t) Returns True if s is a subset of t. s.issuperset(t) Returns True if s is a superset of t. s.pop() Returns an arbitrary set element and removes it from s. s.remove(item) Removes item from s. If item is not a member, KeyError is raised. s.symmetric_difference(t) Symmetric difference. Returns all the items that are in s or t, but not in both sets. s.symmetric_difference_update(t) Computes the symmetric difference of s and t and leaves the result in s. s.union(t) Union. Returns all items in s or t. s.update(t) Adds all the items in t to s. t may be another set, a sequence, or any object that supports iteration. Click here to view code image setattr(object, name, value) Sets an attribute of an object. name is a string. Same as object.name = value. Click here to view code image slice([start,] stop [, step]) Returns a slice object representing integers in the specified range. Slice objects are also generated by the extended slice syntax a[i:i:k]. Click here to view code image sorted(iterable, *, key=keyfunc, reverse=reverseflag) Creates a sorted list from items in iterable. The keyword argument key is a single-argument function that transforms values before they are compared. The keyword argument reverse is a Boolean flag that specifies whether or not the resulting list is sorted in reverse order. The key and reverse arguments must be specified using keywords\u2014for example, sorted(a, key=get_name). staticmethod(func) Creates a static method for use in classes. This function is usually used as a @staticmethod decorator. str([object]) Type representing a string. If object is supplied, a string representation of its value is created by calling its str() method. This is the same string that you see when you print the object. If no argument is given, an empty string is created. Table 10.9 shows methods defined on strings. Table 10.9 String Operators and Methods Operation Description s + t Concatenates strings if t is a string. s * n Replicates a string if n is an integer. s % x Formats a string. x is tuple. s[i] Returns element i of a string. s[i:j] Returns a slice. s[i:j:stride] Returns an extended slice. len(s) Number of elements in s. s.capitalize() Capitalizes the first character. s.casefold() Converts s to a string usable for a caseless comparison. s.center(width [, pad]) Centers the string in a field of length width. pad is a padding character. s.count(sub [, start [, end]]) Counts occurrences of the specified substring sub. s.decode([encoding [, errors]]) Decodes a byte string into text (bytes type only). s.encode([encoding [, errors]]) Returns an encoded version of the string (str type only). s.endswith(suffix [, start [, end]]) Checks the end of the string for a suffix. s.expandtabs([tabsize]) Replaces tabs with spaces. s.find(sub [, start [, end]]) Finds the first occurrence of the specified substring sub. s.format(args, *kwargs) Formats s (str type only). s.format_map(m) Formats s with substitutions taking from the mapping m (str type only). s.index(sub [, start [, end]]) Finds the first occurrence or error in the specified substring sub. s.isalnum() Checks whether all characters are alphanumeric. s.isalpha() Checks whether all characters are alphabetic. s.isascii() Checks whether all characters are ASCII. s.isdecimal() Checks whether all characters are decimal characters. Does not match superscript, subscripts, or other special digits. s.isdigit() Checks whether all characters are digits. Matches superscripts and superscripts, but not vulgar fractions. s.isidentifier() Checks whether s is a valid Python identifier. s.islower() Checks whether all characters are lowercase. s.isnumeric() Checks whether all characters are numeric. Matches all forms of numeric characters such as vulgar fractions, Roman numerals, etc. s.isprintable() Checks whether all characters are printable. s.isspace() Checks whether all characters are whitespace. s.istitle() Checks whether the string is a title-cased string (first letter of each word capitalized). s.isupper() Checks whether all characters are uppercase. s.join(t) Joins a sequence of strings t using a delimiter s. s.ljust(width [, fill]) Left-aligns s in a string of size width. s.lower() Converts to lowercase. s.lstrip([chrs]) Removes leading whitespace or characters supplied in chrs. s.maketrans(x [, y [, z]]) Makes a translation table for s.translate(). s.partition(sep) Partitions a string based on a separator string sep. Returns a tuple (head, sep, tail) or (s, '', '') if sep isn\u2019t found. s.removeprefix(prefix) Returns s with a given prefix removed if present. s.removesuffix(suffix) Returns s with a given suffix removed if present. s.replace(old, new [, maxreplace]) Replaces a substring. s.rfind(sub [, start [, end]]) Finds the last occurrence of a substring. s.rindex(sub [, start [, end]]) Finds the last occurrence or raises an error. s.rjust(width [, fill]) Right-aligns s in a string of length width. s.rpartition(sep) Partitions s based on a separator sep, but searches from the end of the string. s.rsplit([sep [, maxsplit]]) Splits a string from the end of the string using sep as a delimiter. maxsplit is the maximum number of splits to perform. If maxsplit is omitted, the result is identical to the split() method. s.rstrip([chrs]) Removes trailing whitespace or characters supplied in chrs. s.split([sep [, maxsplit]]) Splits a string using sep as a delimiter. maxsplit is the maximum number of splits to perform. s.splitlines([keepends]) Splits a string into a list of lines. If keepends is 1, trailing newlines are preserved. s.startswith(prefix [, start [, end]]) Checks whether a string starts with a prefix. s.strip([chrs]) Removes leading and trailing whitespace or characters supplied in chrs. s.swapcase() Converts uppercase to lowercase, and vice versa. s.title() Returns a title-cased version of the string. s.translate(table [, deletechars]) Translates a string using a character translation table table, removing characters in deletechars. s.upper() Converts a string to uppercase. s.zfill(width) Pads a string with zeros on the left up to the specified width. sum(items [,initial]) Computes the sum of a sequence of items taken from the iterable object items. initial provides the starting value and defaults to 0. This function usually only works with numbers. super() Returns an object that represents the collective superclasses of the class in which its used. The primary purpose of this object is to invoke methods in base classes. Here\u2019s an example: Click here to view code image class B(A): def foo(self): super().foo() # Invoke foo() defined by superclasses. tuple([items]) Type representing a tuple. If supplied, items is an iterable object that is used to populate the tuple. However, if items is already a tuple, it\u2019s returned unmodified. If no argument is given, an empty tuple is returned. Table 10.10 shows methods defined on tuples. Table 10.10 Tuple Operators and Methods Operation Description s + t Concatenation if t is a list. s * n Replication if n is an integer. s[i] Returns element i of a s. s[i:j] Returns a slice. s[i:j:stride] Returns an extended slice. len(s) Number of elements in s. s.append(x) Appends a new element, x, to the end of s. s.count(x) Counts occurrences of x in s. s.index(x [, start [, stop]]) Returns the smallest i where s[i] == x. start and stop optionally specify the starting and ending index for the search. type(object) The base class of all types in Python. When called as a function, returns the type of object. This type is the same as the object\u2019s class. For common types such as integers, floats, and lists, the type will refer to one of the other built-in classes such as int, float, list, and so forth. For user-defined objects, the type is the associated class. For objects related to Python\u2019s internals, you will typically get a reference to one of the classes defined in the types module. vars([object]) Returns the symbol table of object (usually found in its dict attribute). If no argument is given, a dictionary corresponding to the local namespace is returned. The dictionary returned by this function should be assumed to be read-only. It\u2019s not safe to modify its contents. zip([s1 [, s2 [, ... ]]]) Creates an iterator that produces tuples containing one item each from s1, s2, and so on. The nth tuple is (s1[n], s2[n], ... ). The resulting iterator stops when the shortest input is exhausted. If no arguments are given, the iterator produces no values. 10.2 Built-in Exceptions This section describes the built-in exceptions used to report different kinds of errors. 10.2.1 Exception Base Classes The following exceptions serve as base classes for all the other exceptions: BaseException The root class for all exceptions. All built-in exceptions are derived from this class. Exception The base class for all program-related exceptions. That includes all built-in exceptions except for SystemExit, GeneratorExit, and KeyboardInterrupt. User-defined exceptions should inherit from Exception. ArithmeticError The base class for arithmetic exceptions, including OverflowError, ZeroDivisionError, and FloatingPointError. LookupError The base class for indexing and key errors, including IndexError and KeyError. EnvironmentError The base class for errors that occur outside Python. Is a synonym for OSError. The preceding exceptions are never raised explicitly. However, they can be used to catch certain classes of errors. For instance, the following code would catch any sort of numerical error: Click here to view code image try: # Some operation ... except ArithmeticError as e: # Math error 10.2.2 Exception Attributes Instances of an exception e have a few standard attributes that can be useful to inspect and/or manipulate it in certain applications. e.args The tuple of arguments supplied when raising the exception. In most cases, this is a one-item tuple with a string describing the error. For EnvironmentError exceptions, the value is a 2-tuple or 3-tuple containing an integer error number, string error message, and an optional filename. The contents of this tuple might be useful if you need to recreate the exception in a different context\u2014for example, to raise an exception in a different Python interpreter process. e.cause Previous exception when using explicit chained exceptions. e.context Previous exception for implicitly chained exceptions. e.traceback Traceback object associated with the exception. 10.2.3 Predefined Exception Classes The following exceptions are raised by programs: AssertionError Failed assert statement. AttributeError Failed attribute reference or assignment. BufferError Memory buffer expected. EOFError End of file. Generated by the built-in functions input() and raw_input(). It should be noted that most other I/O operations such as the read() and readline() methods of files return an empty string to signal EOF instead of raising an exception. FloatingPointError Failed floating-point operation. It should be noted that floating-point exception handling is a tricky problem and this exception only gets raised if Python has been configured and built in a way that enables it. It is more common for floating-point errors to silently produce results such as float('nan') or float('inf'). A subclass of ArithmeticError. GeneratorExit Raised inside a generator function to signal termination. This happens when a generator is destroyed prematurely (before all generator values are consumed) or the close() method of a generator is called. If a generator ignores this exception, the generator is terminated, and the exception is silently ignored. IOError Failed I/O operation. The value is an IOError instance with the attributes errno, strerror, and filename. errno is an integer error number, strerror is a string error message, and filename is an optional filename. A subclass of EnvironmentError. ImportError Raised when an import statement can\u2019t find a module or when from can\u2019t find a name in a module. IndentationError Indentation error. A subclass of SyntaxError. IndexError Sequence subscript out of range. A subclass of LookupError. KeyError Key not found in a mapping. A subclass of LookupError. KeyboardInterrupt Raised when the user hits the interrupt key (usually Ctrl+C). MemoryError Recoverable out-of-memory error. ModuleNotFoundError Module can\u2019t be found by the import statement. NameError Name not found in local or global namespaces. NotImplementedError Unimplemented feature. Can be raised by base classes that require derived classes to implement certain methods. A subclass of RuntimeError. OSError Operating system error. Primarily raised by functions in the os module. The following exceptions are subclasses: BlockingIOError, BrokenPipeError, ChildProcessError, ConnectionAbortedError, ConnectionError, ConnectionRefusedError, ConnectionResetError, FileExistsError, FileNotFoundError, InterruptedError, IsADirectoryError, NotADirectoryError, PermissionError, ProcessLookupError, TimeoutError. OverflowError Result of an integer value being too large to be represented. This exception usually only arises if large integer values are passed to objects that internally rely upon fixed-precision machine integers in their implementation. For example, this error can arise with range or xrange objects if you specify starting or ending values that exceed 32 bits in size. A subclass of ArithmeticError. RecursionError Recursion limit exceeded. ReferenceError Result of accessing a weak reference after the underlying object has been destroyed (see the weakref module). RuntimeError A generic error not covered by any of the other categories. StopIteration Raised to signal the end of iteration. This normally happens in the next() method of an object or in a generator function. StopAsyncIteration Raised to signal the end of asynchronous iteration. Only applicable in the context of async functions and generators. SyntaxError Parser syntax error. Instances have the attributes filename, lineno, offset, and text, which can be used to gather more information. SystemError Internal error in the interpreter. The value is a string indicating the problem. SystemExit Raised by the sys.exit() function. The value is an integer indicating the return code. If it\u2019s necessary to exit immediately, os._exit() can be used. TabError Inconsistent tab usage. Generated when Python is run with the -tt option. A subclass of SyntaxError. TypeError Occurs when an operation or function is applied to an object of an inappropriate type. UnboundLocalError Unbound local variable referenced. This error occurs if a variable is referenced before it\u2019s defined in a function. A subclass of NameError. UnicodeError Unicode encoding or decoding error. A subclass of ValueError. The following exceptions are subclasses: UnicodeEncodeError, UnicodeDecodeError, UnicodeTranslateError. ValueError Generated when the argument to a function or operation is the right type but an inappropriate value. WindowsError Generated by failed system calls on Windows. A subclass of OSError. ZeroDivisionError Dividing by zero. A subclass of ArithmeticError. 10.3 Standard Library Python comes with a sizable standard library. Many of these modules have been previously described in the book. Reference material can be found at https://docs.python.org/library. That material is not repeated here. The modules listed below are notable because they are generally useful for a wide variety of applications and for Python programming in general. 10.3.1 collections Module The collections module supplements Python with a variety of additional container objects that can be quite useful for working with data\u2014such as a double-ended queue (deque), dictionaries that automatically initialize missing items (defaultdict), and counters for tabulation (Counter). 10.3.2 datetime Module The datetime module is where you find functions related to dates, times, and computations involving those things. 10.3.3 itertools Module The itertools module provides a variety of useful iteration patterns\u2014chaining iterables together, iterating over product sets, permutations, grouping, and similar operations. 10.3.4 inspect Module The inspect module provides functions for inspecting the internals of code-related elements such as functions, classes, generators, and coroutines. It\u2019s commonly used in metaprogramming by functions that define decorators and similar features. 10.3.5 math Module The math module provides common mathematical functions such as sqrt(), cos(), and sin(). 10.3.6 os Module The os module is where you find low-level functions related to the host operating system\u2014processes, files, pipes, permissions, and similar features. 10.3.7 random Module The random module provides various functions related to random number generation. 10.3.8 re Module The re module provides support for working with text via regular expression pattern matching. 10.3.9 shutil Module The shutil module has functions for performing common tasks related to the shell, such as copying files and directories. 10.3.10 statistics Module The statistics module provides functions for computing common statistical values such as means, medians, and standard deviation. 10.3.11 sys Module The sys module contains a variety of attributes and methods related to the runtime environment of Python itself. This includes command-line options, standard I/O streams, the import path, and similar features. 10.3.12 time Module The time module is where you find various functions related to system time, such as getting the value of the system clock, sleeping, and the number of elapsed CPU seconds. 10.3.13 turtle Module Turtle graphics. You know, for kids. 10.3.14 unittest Module The unittest module provides built-in support for writing unit tests. Python itself is tested using unittest. However, many programmers prefer using third-party libraries such as pytest for testing. This author concurs.","title":"Builtins"},{"location":"python/modules/builtins/#builtins","text":"","title":"Builtins"},{"location":"python/modules/builtins/#absx","text":"Returns the absolute value of x .","title":"abs(x)"},{"location":"python/modules/builtins/#alls","text":"Returns True if all of the values in the iterable s evaluate as True . Returns True if s is empty.","title":"all(s)"},{"location":"python/modules/builtins/#anys","text":"Returns True if any of the values in the iterable s evaluate as True . Returns False if s is empty.","title":"any(s)"},{"location":"python/modules/builtins/#asciix","text":"Creates a printable representation of the object x just like the repr() , but only uses ASCII characters in the result. Non-ASCII characters are turned into appropriate escape sequences. This can be used to view Unicode strings in a terminal or shell that doesn\u2019t support Unicode.","title":"ascii(x)"},{"location":"python/modules/builtins/#binx","text":"Returns a string with the binary representation of the integer x .","title":"bin(x)"},{"location":"python/modules/builtins/#boolx","text":"Type representing Boolean values True and False . If used to convert x , it returns True if x evaluates to true using the usual truth-testing semantics\u2014that is, nonzero number, nonempty list, and so on. Otherwise, False is returned. False is also the default value returned if bool() is called without any arguments. The bool class inherits from int , so the Boolean values True and False can be used as integers with values 1 and 0 in mathematical calculations.","title":"bool([x])"},{"location":"python/modules/builtins/#breakpoint","text":"Sets a manual debugger breakpoint. When encountered, control will transfer to pdb , the Python debugger.","title":"breakpoint()"},{"location":"python/modules/builtins/#bytearrayx","text":"A type representing a mutable array of bytes. When creating an instance, x may either be an iterable sequence of integers in the range 0 to 255, an 8-bit string or bytes literal, or an integer that specifies the size of the byte array (in which case every entry will be initialized to 0).","title":"bytearray([x])"},{"location":"python/modules/builtins/#bytearrays-encoding","text":"An alternative calling convention for creating a bytearray instance from characters in a string s where encoding specifies the character encoding to use in the conversion.","title":"bytearray(s, encoding)"},{"location":"python/modules/builtins/#bytesx","text":"A type representing an immutable array of bytes.","title":"bytes([x])"},{"location":"python/modules/builtins/#bytess-encoding","text":"An alternate calling convention for creating bytes from a string s where encoding specifies the encoding to use in conversion. Table 10.1 shows operations supported by both bytes and byte arrays. Table 10.1: Operations on Bytes and Bytearrays Operation Description s + t Concatenates if t is bytes. s * n Replicates if n is an integer. s % x Formats bytes. x is tuple. s[i] Returns element i as an integer. s[i:j] Returns a slice. s[i:j:stride] Returns an extended slice. len(s) Number of bytes in s . s.capitalize() Capitalizes the first character. s.center(width [, pad]) Centers the string in a field of length width . pad is a padding character. s.count(sub [, start [, end]]) Counts occurrences of the specified substring sub . s.decode([encoding [, errors]]) Decodes a byte string into text (bytes type only). s.endswith(suffix [, start [, end]]) Checks the end of the string for a suffix. s.expandtabs([tabsize]) Replaces tabs with spaces. s.find(sub [, start [, end]]) Finds the first occurrence of the specified substring sub . s.hex() Converts to a hexadecimal string. s.index(sub [, start [, end]]) Finds the first occurrence or error in the specified substring sub . s.isalnum() Checks whether all characters are alphanumeric. s.isalpha() Checks whether all characters are alphabetic. s.isascii() Checks whether all characters are ASCII. s.isdigit() Checks whether all characters are digits. s.islower() Checks whether all characters are lowercase. s.isspace() Checks whether all characters are whitespace. s.istitle() Checks whether the string is a title-cased string (first letter of each word capitalized). s.isupper() Checks whether all characters are uppercase. s.join(t) Joins a sequence of strings t using a delimiter s . s.ljust(width [, fill]) Left-aligns s in a string of size width . fill is a padding character. s.lower() Converts to lowercase. s.lstrip([chrs]) Removes leading whitespace or characters supplied in chrs . s.maketrans(x [, y [, z]]) Makes a translation table for s.translate() . s.partition(sep) Partitions a string based on a separator string sep . Returns a tuple (head, sep, tail) or (s, '', '') if sep isn\u2019t found. s.removeprefix(prefix) Returns s with a given prefix removed if present. s.removesuffix(suffix) Returns s with a given suffix removed if present. s.replace(old, new [, maxreplace]) Replaces a substring. s.rfind(sub [, start [, end]]) Finds the last occurrence of a substring. s.rindex(sub [, start [, end]]) Finds the last occurrence or raises an error. s.rjust(width [, fill]) Right-aligns s in a string of length width . fill is a padding character. s.rpartition(sep) Partitions s based on a separator sep , but searches from the end of the string. s.rsplit([sep [, maxsplit]]) Splits a string from the end of the string using sep as a delimiter. maxsplit is the maximum number of splits to perform. If maxsplit is omitted, the result is identical to the split() method. s.rstrip([chrs]) Removes trailing whitespace or characters supplied in chrs . s.split([sep [, maxsplit]]) Splits a string using sep as a delimiter. maxsplit is the maximum number of splits to perform. s.splitlines([keepends]) Splits a string into a list of lines. If keepends is 1 , trailing newlines are preserved. s.startswith(prefix [, start [, end]]) Checks whether a string starts with prefix . s.strip([chrs]) Removes leading and trailing whitespace or characters supplied in chrs . s.swapcase() Converts uppercase to lowercase, and vice versa. s.title() Returns a title-cased version of the string. s.translate(table [, deletechars]) Translates a string using a character translation table table , removing characters in deletechars . s.upper() Converts a string to uppercase. s.zfill(width) Pads a string with zeros on the left up to the specified width . Byte arrays additionally support the methods in Table 10.2. Table 10.2: Additional Operations on Byte Arrays Operation Description s[i] = v Item assignment. s[i:j] = t Slice assignment. s[i:j:stride] = t Extended slice assignment. del s[i] Item deletion. del s[i:j] Slice deletion. del s[i:j:stride] Extended slice deletion. s.append(x) Appends a new byte to the end. s.clear() Clears the byte array. s.copy() Makes a copy. s.extend(t) Extends s with bytes from t . s.insert(n, x) Inserts byte x at index n . s.pop([n]) Removes and returns byte at index n . s.remove(x) Removes first occurrence of byte x . s.reverse() Reverses the byte array in-place.","title":"bytes(s, encoding)"},{"location":"python/modules/builtins/#callableobj","text":"Returns True if obj is callable as a function.","title":"callable(obj)"},{"location":"python/modules/builtins/#chrx","text":"Converts the integer x representing a Unicode code-point into a single-character string.","title":"chr(x)"},{"location":"python/modules/builtins/#callableobj_1","text":"Returns True if obj is callable as a function.","title":"callable(obj)"},{"location":"python/modules/builtins/#chrx_1","text":"Converts the integer x representing a Unicode code-point into a single-character string.","title":"chr(x)"},{"location":"python/modules/builtins/#classmethodfunc","text":"This decorator creates a class method for the function func . It is typically only used inside class definitions where it is implicitly invoked using @classmethod . Unlike a normal method, a class method receives the class as the first argument, not an instance.","title":"classmethod(func)"},{"location":"python/modules/builtins/#compilestring-filename-kind","text":"Compiles string into a code object for use with exec() or eval() . string is a string containing valid Python code. If this code spans multiple lines, the lines must be terminated by a single newline ('\\n') and not platform-specific variants (for example, '\\r\\n' on Windows). filename is a string containing the name of the file in which the string was defined (if any). kind is 'exec' for a sequence of statements, 'eval' for a single expression, or 'single' for a single executable statement. The resulting code object that is returned can be directly passed to exec() or eval() in place of a string.","title":"compile(string, filename, kind)"},{"location":"python/modules/builtins/#complexreal-imag","text":"Type representing a complex number with real and imaginary components, real and imag , which can be supplied as any numeric type. If imag is omitted, the imaginary component is set to zero. If real is passed as a string, the string is parsed and converted to a complex number. In this case, imag should be omitted. If real is any other kind of object, the value of real.complex() is returned. If no arguments are given, 0j is returned. Table 10.3: Attributes of complex Attribute/Method Description z.real Real component z.imag Imaginary component z.conjugate() Conjugates as a complex number Click here to view code image.","title":"complex([real [, imag]])"},{"location":"python/modules/builtins/#delattrobject-attr","text":"Deletes an attribute of an object. attr is a string. Same as del object.attr . Click here to view code image.","title":"delattr(object, attr)"},{"location":"python/modules/builtins/#dictm-or-dictkey1value1-key2value2","text":"Type representing a dictionary. If no argument is given, an empty dictionary is returned. If m is a mapping object (such as another dictionary), a new dictionary having the same keys and same values as m is returned. For example, if m is a dictionary, dict(m) makes a shallow copy of it. If m is not a mapping, it must support iteration in which a sequence of (key, value) pairs is produced. These pairs are used to populate the dictionary. dict() can also be called with keyword arguments. For example, dict(foo=3, bar=7) creates the dictionary {'foo': 3, 'bar': 7} . Table 10.4: Operations on Dictionaries Operation Description m | n Merges m and n into a single dictionary. len(m) Returns the number of items in m . m[k] Returns the item of m with key k . m[k]=x Sets m[k] to x . del m[k] Removes m[k] from m . k in m Returns True if k is a key in m . m.clear() Removes all items from m . m.copy() Makes a shallow copy of m . m.fromkeys(s [, value]) Creates a new dictionary with keys from sequence s and values all set to value . m.get(k [, v]) Returns m[k] if found; otherwise, returns v . m.items() Returns (key, value) pairs. m.keys() Returns the keys. m.pop(k [, default]) Returns m[k] if found and removes it from m ; otherwise, returns default if supplied or raises KeyError if not. m.popitem() Removes a random (key, value) pair from m and returns it as a tuple. m.setdefault(k [, v]) Returns m[k] if found; otherwise, returns v and sets m[k] = v . m.update(b) Adds all objects from b to m . m.values() Returns the values.","title":"dict([m]) or dict(key1=value1, key2=value2, ...)"},{"location":"python/modules/builtins/#dirobject","text":"Returns a sorted list of attribute names. If object is a module, it contains the list of symbols defined in that module. If object is a type or class object, it returns a list of attribute names. The names are typically obtained from the object\u2019s dict attribute if defined, but other sources may be used. If no argument is given, the names in the current local symbol table are returned. It should be noted that this function is primarily used for informational purposes (for example, used interactively at the command line). It should not be used for formal program analysis because the information obtained may be incomplete. Also, user-defined classes can define a special method dir() that alters the result of this function.","title":"dir([object])"},{"location":"python/modules/builtins/#divmoda-b","text":"Returns the quotient and remainder of long division as a tuple. For integers, the value (a // b, a % b) is returned. For floats, (math.floor(a / b), a % b) is returned. This function may not be called with complex numbers. Click here to view code image.","title":"divmod(a, b)"},{"location":"python/modules/builtins/#enumerateiter-start0","text":"Given an iterable object, iter , returns a new iterator (of type enumerate ) that produces tuples containing a count and the value produced from iter . For example, if iter produces a, b, c , then enumerate(iter) produces (0,a) , (1,b) , (2,c) . The optional start changes the initial value of the count. Click here to view code image.","title":"enumerate(iter, start=0)"},{"location":"python/modules/builtins/#evalexpr-globals-locals","text":"Evaluates an expression. expr is a string or a code object created by compile() . globals and locals are mapping objects that define the global and local namespaces, respectively, for the operation. If omitted, the expression is evaluated using the values of globals() and locals() as executed in the caller\u2019s environment. It is most common for globals and locals to be specified as dictionaries, but advanced applications can supply custom mapping objects. Click here to view code image.","title":"eval(expr [, globals [, locals]])"},{"location":"python/modules/builtins/#execcode-global-locals","text":"Executes Python statements. code is a string, bytes, or a code object created by compile() . globals and locals define the global and local namespaces, respectively, for the operation. If omitted, the code is executed using the values of globals() and locals() as executed in the caller\u2019s environment. Click here to view code image.","title":"exec(code [, global [, locals]])"},{"location":"python/modules/builtins/#filterfunction-iterable","text":"Creates an iterator that returns the items in iterable for which function(item) evaluates as True .","title":"filter(function, iterable)"},{"location":"python/modules/builtins/#floatx","text":"Type representing a floating-point number. If x is a number, it is converted to a float. If x is a string, it is parsed into a float. For all other objects, x.float() is invoked. If no argument is supplied, 0.0 is returned. Table 10.5: Methods and Attributes of Floats Attribute/Method Description x.real Real component when used as a complex. x.imag Imaginary component when used as a complex. x.conjugate() Conjugates as a complex number. x.as_integer_ratio() Converts to numerator/denominator pair. x.hex() Creates a hexadecimal representation. x.is_integer() Tests if an exact integer value. float.fromhex(s) Creates from a hexadecimal string. A class method.","title":"float([x])"},{"location":"python/modules/builtins/#python-built-in-functions","text":"","title":"Python Built-in Functions"},{"location":"python/modules/builtins/#bytessource-encoding-errors","text":"Constructs a new bytes object. The source parameter can be used to initialize the bytes object from a sequence of integers or another object that implements the buffer protocol. If source is specified, the encoding and errors parameters must not be specified.","title":"bytes([source[, encoding[, errors]]])"},{"location":"python/modules/builtins/#callableobj_2","text":"Returns True if obj is callable as a function.","title":"callable(obj)"},{"location":"python/modules/builtins/#chrx_2","text":"Converts the integer x representing a Unicode code-point into a single-character string.","title":"chr(x)"},{"location":"python/modules/builtins/#classmethodfunc_1","text":"This decorator creates a class method for the function func . It is typically only used inside class definitions where it is implicitly invoked using @classmethod . Unlike a normal method, a class method receives the class as the first argument, not an instance.","title":"classmethod(func)"},{"location":"python/modules/builtins/#compilestring-filename-kind_1","text":"Compiles string into a code object for use with exec() or eval() . string is a string containing valid Python code. If this code spans multiple lines, the lines must be terminated by a single newline ( '\\n' ) and not platform-specific variants (for example, '\\r\\n' on Windows). filename is a string containing the name of the file in which the string was defined (if any). kind is 'exec' for a sequence of statements, 'eval' for a single expression, or 'single' for a single executable statement. The resulting code object that is returned can be directly passed to exec() or eval() in place of a string.","title":"compile(string, filename, kind)"},{"location":"python/modules/builtins/#complexreal-imag_1","text":"Type representing a complex number with real and imaginary components, real and imag , which can be supplied as any numeric type. If imag is omitted, the imaginary component is set to zero. If real is passed as a string, the string is parsed and converted to a complex number. In this case, imag should be omitted. If real is any other kind of object, the value of real.complex() is returned. If no arguments are given, 0j is returned.","title":"complex([real [, imag]])"},{"location":"python/modules/builtins/#delattrobject-attr_1","text":"Deletes an attribute of an object. attr is a string. Same as del object.attr .","title":"delattr(object, attr)"},{"location":"python/modules/builtins/#dictm-or-dictkey1value1-key2value2_1","text":"Type representing a dictionary. If no argument is given, an empty dictionary is returned. If m is a mapping object (such as another dictionary), a new dictionary having the same keys and same values as m is returned. For example, if m is a dictionary, dict(m) makes a shallow copy of it. If m is not a mapping, it must support iteration in which a sequence of (key, value) pairs is produced. These pairs are used to populate the dictionary. dict() can also be called with keyword arguments. For example, dict(foo=3, bar=7) creates the dictionary {'foo': 3, 'bar': 7} .","title":"dict([m]) or dict(key1=value1, key2=value2, ...)"},{"location":"python/modules/builtins/#dirobject_1","text":"Returns a sorted list of attribute names. If object is a module, it contains the list of symbols defined in that module. If object is a type or class object, it returns a list of attribute names. The names are typically obtained from the object\u2019s dict attribute if defined, but other sources may be used. If no argument is given, the names in the current local symbol table are returned. It should be noted that this function is primarily used for informational purposes (for example, used interactively at a Python prompt).","title":"dir([object])"},{"location":"python/modules/builtins/#divmodx-y","text":"Returns a pair of numbers (q, r) such that x = y * q + r . If x and y are integers, the return value is also an integer. For example, divmod(7, 3) returns (2, 1) , where 2 is the quotient and 1 is the remainder.","title":"divmod(x, y)"},{"location":"python/modules/builtins/#enumerateiterable-start","text":"Returns an iterator that generates pairs consisting of an index and an item from the iterable . The start parameter is an optional integer that specifies the starting value of the index. By default, it is 0 .","title":"enumerate(iterable[, start])"},{"location":"python/modules/builtins/#evalexpression-globals-locals","text":"Evaluates the expression in the given globals and locals namespaces. expression can be a string or a code object. If globals is specified, it must be a dictionary. If locals is specified, it can be any mapping object. If both globals and locals are omitted, the expression is evaluated in the context of the current global and local namespaces.","title":"eval(expression[, globals[, locals]])"},{"location":"python/modules/builtins/#execobject-globals-locals","text":"Evaluates the object as a Python expression or statement. object can be a string or a code object. If globals is specified, it must be a dictionary. If locals is specified, it can be any mapping object. If both globals and locals are omitted, the code is executed in the context of the current global and local namespaces.","title":"exec(object[, globals[, locals]])"},{"location":"python/modules/builtins/#filterfunction-iterable_1","text":"Creates an iterator that produces the values from iterable for which function returns True . If function is None , the identity function is assumed, which returns True for all elements of the iterable. If iterable is a string, the resulting iterator produces the individual characters of the string.","title":"filter(function, iterable)"},{"location":"python/modules/builtins/#floatx_1","text":"Type representing a floating-point number. If no argument is given, 0.0 is returned. If x is a number, it is converted to a floating-point number. If x is a string, it is parsed and converted to a floating-point number.","title":"float([x])"},{"location":"python/modules/builtins/#formatvalue-format_spec","text":"Converts value to a formatted string according to the format specification string in format_spec . This operation invokes value.format() , which is free to interpret the format specification as it sees fit. For simple types of data, the format specifier typically includes an alignment character of < , > , or ^ , a number (which indicates the field width), and a character code of d , f , or s for integer, floating point, or string values, respectively. For example, a format specification of 'd' formats an integer, a specification of '8d' right-aligns an integer in an 8-character field, and '<8d' left-aligns an integer in an 8-character field. More details on format() and format specifiers can be found in Chapter 9.","title":"format(value[, format_spec])"},{"location":"python/modules/builtins/#frozensetiterable","text":"Type representing an immutable set object populated with values taken from iterable . The values must also be immutable. If no argument is given, an empty set is returned. A frozenset supports all of the operations found on sets except for any operations that mutate a set in-place.","title":"frozenset([iterable])"},{"location":"python/modules/builtins/#getattrobject-name-default","text":"Returns the value of a named attribute of an object. name is a string containing the attribute name. default is an optional value to return if no such attribute exists; Table 10.8 shows operations on sets. Table 10.8 Set Operations and Methods Operation Description s | t Union s & t Intersection s - t Difference s ^ t Symmetric difference len(s) Returns number of items in s. s.add(item) Adds item to s. Has no effect if item is already in s. s.clear() Removes all items from s. s.copy() Makes a copy of s. s.difference(t) Set difference. Returns all the items in s, but not in t. s.difference_update(t) Removes all the items from s that are also in t. s.discard(item) Removes item from s. If item is not a member of s, nothing happens. s.intersection(t) Intersection. Returns all the items that are both in s and in t. s.intersection_update(t) Computes the intersection of s and t and leaves the result in s. s.isdisjoint(t) Returns True if s and t have no items in common. s.issubset(t) Returns True if s is a subset of t. s.issuperset(t) Returns True if s is a superset of t. s.pop() Returns an arbitrary set element and removes it from s. s.remove(item) Removes item from s. If item is not a member, KeyError is raised. s.symmetric_difference(t) Symmetric difference. Returns all the items that are in s or t, but not in both sets. s.symmetric_difference_update(t) Computes the symmetric difference of s and t and leaves the result in s. s.union(t) Union. Returns all items in s or t. s.update(t) Adds all the items in t to s. t may be another set, a sequence, or any object that supports iteration. Click here to view code image setattr(object, name, value) Sets an attribute of an object. name is a string. Same as object.name = value. Click here to view code image slice([start,] stop [, step]) Returns a slice object representing integers in the specified range. Slice objects are also generated by the extended slice syntax a[i:i:k]. Click here to view code image sorted(iterable, *, key=keyfunc, reverse=reverseflag) Creates a sorted list from items in iterable. The keyword argument key is a single-argument function that transforms values before they are compared. The keyword argument reverse is a Boolean flag that specifies whether or not the resulting list is sorted in reverse order. The key and reverse arguments must be specified using keywords\u2014for example, sorted(a, key=get_name). staticmethod(func) Creates a static method for use in classes. This function is usually used as a @staticmethod decorator. str([object]) Type representing a string. If object is supplied, a string representation of its value is created by calling its str() method. This is the same string that you see when you print the object. If no argument is given, an empty string is created. Table 10.9 shows methods defined on strings. Table 10.9 String Operators and Methods Operation Description s + t Concatenates strings if t is a string. s * n Replicates a string if n is an integer. s % x Formats a string. x is tuple. s[i] Returns element i of a string. s[i:j] Returns a slice. s[i:j:stride] Returns an extended slice. len(s) Number of elements in s. s.capitalize() Capitalizes the first character. s.casefold() Converts s to a string usable for a caseless comparison. s.center(width [, pad]) Centers the string in a field of length width. pad is a padding character. s.count(sub [, start [, end]]) Counts occurrences of the specified substring sub. s.decode([encoding [, errors]]) Decodes a byte string into text (bytes type only). s.encode([encoding [, errors]]) Returns an encoded version of the string (str type only). s.endswith(suffix [, start [, end]]) Checks the end of the string for a suffix. s.expandtabs([tabsize]) Replaces tabs with spaces. s.find(sub [, start [, end]]) Finds the first occurrence of the specified substring sub. s.format(args, *kwargs) Formats s (str type only). s.format_map(m) Formats s with substitutions taking from the mapping m (str type only). s.index(sub [, start [, end]]) Finds the first occurrence or error in the specified substring sub. s.isalnum() Checks whether all characters are alphanumeric. s.isalpha() Checks whether all characters are alphabetic. s.isascii() Checks whether all characters are ASCII. s.isdecimal() Checks whether all characters are decimal characters. Does not match superscript, subscripts, or other special digits. s.isdigit() Checks whether all characters are digits. Matches superscripts and superscripts, but not vulgar fractions. s.isidentifier() Checks whether s is a valid Python identifier. s.islower() Checks whether all characters are lowercase. s.isnumeric() Checks whether all characters are numeric. Matches all forms of numeric characters such as vulgar fractions, Roman numerals, etc. s.isprintable() Checks whether all characters are printable. s.isspace() Checks whether all characters are whitespace. s.istitle() Checks whether the string is a title-cased string (first letter of each word capitalized). s.isupper() Checks whether all characters are uppercase. s.join(t) Joins a sequence of strings t using a delimiter s. s.ljust(width [, fill]) Left-aligns s in a string of size width. s.lower() Converts to lowercase. s.lstrip([chrs]) Removes leading whitespace or characters supplied in chrs. s.maketrans(x [, y [, z]]) Makes a translation table for s.translate(). s.partition(sep) Partitions a string based on a separator string sep. Returns a tuple (head, sep, tail) or (s, '', '') if sep isn\u2019t found. s.removeprefix(prefix) Returns s with a given prefix removed if present. s.removesuffix(suffix) Returns s with a given suffix removed if present. s.replace(old, new [, maxreplace]) Replaces a substring. s.rfind(sub [, start [, end]]) Finds the last occurrence of a substring. s.rindex(sub [, start [, end]]) Finds the last occurrence or raises an error. s.rjust(width [, fill]) Right-aligns s in a string of length width. s.rpartition(sep) Partitions s based on a separator sep, but searches from the end of the string. s.rsplit([sep [, maxsplit]]) Splits a string from the end of the string using sep as a delimiter. maxsplit is the maximum number of splits to perform. If maxsplit is omitted, the result is identical to the split() method. s.rstrip([chrs]) Removes trailing whitespace or characters supplied in chrs. s.split([sep [, maxsplit]]) Splits a string using sep as a delimiter. maxsplit is the maximum number of splits to perform. s.splitlines([keepends]) Splits a string into a list of lines. If keepends is 1, trailing newlines are preserved. s.startswith(prefix [, start [, end]]) Checks whether a string starts with a prefix. s.strip([chrs]) Removes leading and trailing whitespace or characters supplied in chrs. s.swapcase() Converts uppercase to lowercase, and vice versa. s.title() Returns a title-cased version of the string. s.translate(table [, deletechars]) Translates a string using a character translation table table, removing characters in deletechars. s.upper() Converts a string to uppercase. s.zfill(width) Pads a string with zeros on the left up to the specified width. sum(items [,initial]) Computes the sum of a sequence of items taken from the iterable object items. initial provides the starting value and defaults to 0. This function usually only works with numbers. super() Returns an object that represents the collective superclasses of the class in which its used. The primary purpose of this object is to invoke methods in base classes. Here\u2019s an example: Click here to view code image class B(A): def foo(self): super().foo() # Invoke foo() defined by superclasses. tuple([items]) Type representing a tuple. If supplied, items is an iterable object that is used to populate the tuple. However, if items is already a tuple, it\u2019s returned unmodified. If no argument is given, an empty tuple is returned. Table 10.10 shows methods defined on tuples. Table 10.10 Tuple Operators and Methods Operation Description s + t Concatenation if t is a list. s * n Replication if n is an integer. s[i] Returns element i of a s. s[i:j] Returns a slice. s[i:j:stride] Returns an extended slice. len(s) Number of elements in s. s.append(x) Appends a new element, x, to the end of s. s.count(x) Counts occurrences of x in s. s.index(x [, start [, stop]]) Returns the smallest i where s[i] == x. start and stop optionally specify the starting and ending index for the search. type(object) The base class of all types in Python. When called as a function, returns the type of object. This type is the same as the object\u2019s class. For common types such as integers, floats, and lists, the type will refer to one of the other built-in classes such as int, float, list, and so forth. For user-defined objects, the type is the associated class. For objects related to Python\u2019s internals, you will typically get a reference to one of the classes defined in the types module. vars([object]) Returns the symbol table of object (usually found in its dict attribute). If no argument is given, a dictionary corresponding to the local namespace is returned. The dictionary returned by this function should be assumed to be read-only. It\u2019s not safe to modify its contents. zip([s1 [, s2 [, ... ]]]) Creates an iterator that produces tuples containing one item each from s1, s2, and so on. The nth tuple is (s1[n], s2[n], ... ). The resulting iterator stops when the shortest input is exhausted. If no arguments are given, the iterator produces no values. 10.2 Built-in Exceptions This section describes the built-in exceptions used to report different kinds of errors. 10.2.1 Exception Base Classes The following exceptions serve as base classes for all the other exceptions: BaseException The root class for all exceptions. All built-in exceptions are derived from this class. Exception The base class for all program-related exceptions. That includes all built-in exceptions except for SystemExit, GeneratorExit, and KeyboardInterrupt. User-defined exceptions should inherit from Exception. ArithmeticError The base class for arithmetic exceptions, including OverflowError, ZeroDivisionError, and FloatingPointError. LookupError The base class for indexing and key errors, including IndexError and KeyError. EnvironmentError The base class for errors that occur outside Python. Is a synonym for OSError. The preceding exceptions are never raised explicitly. However, they can be used to catch certain classes of errors. For instance, the following code would catch any sort of numerical error: Click here to view code image try: # Some operation ... except ArithmeticError as e: # Math error 10.2.2 Exception Attributes Instances of an exception e have a few standard attributes that can be useful to inspect and/or manipulate it in certain applications. e.args The tuple of arguments supplied when raising the exception. In most cases, this is a one-item tuple with a string describing the error. For EnvironmentError exceptions, the value is a 2-tuple or 3-tuple containing an integer error number, string error message, and an optional filename. The contents of this tuple might be useful if you need to recreate the exception in a different context\u2014for example, to raise an exception in a different Python interpreter process. e.cause Previous exception when using explicit chained exceptions. e.context Previous exception for implicitly chained exceptions. e.traceback Traceback object associated with the exception. 10.2.3 Predefined Exception Classes The following exceptions are raised by programs: AssertionError Failed assert statement. AttributeError Failed attribute reference or assignment. BufferError Memory buffer expected. EOFError End of file. Generated by the built-in functions input() and raw_input(). It should be noted that most other I/O operations such as the read() and readline() methods of files return an empty string to signal EOF instead of raising an exception. FloatingPointError Failed floating-point operation. It should be noted that floating-point exception handling is a tricky problem and this exception only gets raised if Python has been configured and built in a way that enables it. It is more common for floating-point errors to silently produce results such as float('nan') or float('inf'). A subclass of ArithmeticError. GeneratorExit Raised inside a generator function to signal termination. This happens when a generator is destroyed prematurely (before all generator values are consumed) or the close() method of a generator is called. If a generator ignores this exception, the generator is terminated, and the exception is silently ignored. IOError Failed I/O operation. The value is an IOError instance with the attributes errno, strerror, and filename. errno is an integer error number, strerror is a string error message, and filename is an optional filename. A subclass of EnvironmentError. ImportError Raised when an import statement can\u2019t find a module or when from can\u2019t find a name in a module. IndentationError Indentation error. A subclass of SyntaxError. IndexError Sequence subscript out of range. A subclass of LookupError. KeyError Key not found in a mapping. A subclass of LookupError. KeyboardInterrupt Raised when the user hits the interrupt key (usually Ctrl+C). MemoryError Recoverable out-of-memory error. ModuleNotFoundError Module can\u2019t be found by the import statement. NameError Name not found in local or global namespaces. NotImplementedError Unimplemented feature. Can be raised by base classes that require derived classes to implement certain methods. A subclass of RuntimeError. OSError Operating system error. Primarily raised by functions in the os module. The following exceptions are subclasses: BlockingIOError, BrokenPipeError, ChildProcessError, ConnectionAbortedError, ConnectionError, ConnectionRefusedError, ConnectionResetError, FileExistsError, FileNotFoundError, InterruptedError, IsADirectoryError, NotADirectoryError, PermissionError, ProcessLookupError, TimeoutError. OverflowError Result of an integer value being too large to be represented. This exception usually only arises if large integer values are passed to objects that internally rely upon fixed-precision machine integers in their implementation. For example, this error can arise with range or xrange objects if you specify starting or ending values that exceed 32 bits in size. A subclass of ArithmeticError. RecursionError Recursion limit exceeded. ReferenceError Result of accessing a weak reference after the underlying object has been destroyed (see the weakref module). RuntimeError A generic error not covered by any of the other categories. StopIteration Raised to signal the end of iteration. This normally happens in the next() method of an object or in a generator function. StopAsyncIteration Raised to signal the end of asynchronous iteration. Only applicable in the context of async functions and generators. SyntaxError Parser syntax error. Instances have the attributes filename, lineno, offset, and text, which can be used to gather more information. SystemError Internal error in the interpreter. The value is a string indicating the problem. SystemExit Raised by the sys.exit() function. The value is an integer indicating the return code. If it\u2019s necessary to exit immediately, os._exit() can be used. TabError Inconsistent tab usage. Generated when Python is run with the -tt option. A subclass of SyntaxError. TypeError Occurs when an operation or function is applied to an object of an inappropriate type. UnboundLocalError Unbound local variable referenced. This error occurs if a variable is referenced before it\u2019s defined in a function. A subclass of NameError. UnicodeError Unicode encoding or decoding error. A subclass of ValueError. The following exceptions are subclasses: UnicodeEncodeError, UnicodeDecodeError, UnicodeTranslateError. ValueError Generated when the argument to a function or operation is the right type but an inappropriate value. WindowsError Generated by failed system calls on Windows. A subclass of OSError. ZeroDivisionError Dividing by zero. A subclass of ArithmeticError. 10.3 Standard Library Python comes with a sizable standard library. Many of these modules have been previously described in the book. Reference material can be found at https://docs.python.org/library. That material is not repeated here. The modules listed below are notable because they are generally useful for a wide variety of applications and for Python programming in general. 10.3.1 collections Module The collections module supplements Python with a variety of additional container objects that can be quite useful for working with data\u2014such as a double-ended queue (deque), dictionaries that automatically initialize missing items (defaultdict), and counters for tabulation (Counter). 10.3.2 datetime Module The datetime module is where you find functions related to dates, times, and computations involving those things. 10.3.3 itertools Module The itertools module provides a variety of useful iteration patterns\u2014chaining iterables together, iterating over product sets, permutations, grouping, and similar operations. 10.3.4 inspect Module The inspect module provides functions for inspecting the internals of code-related elements such as functions, classes, generators, and coroutines. It\u2019s commonly used in metaprogramming by functions that define decorators and similar features. 10.3.5 math Module The math module provides common mathematical functions such as sqrt(), cos(), and sin(). 10.3.6 os Module The os module is where you find low-level functions related to the host operating system\u2014processes, files, pipes, permissions, and similar features. 10.3.7 random Module The random module provides various functions related to random number generation. 10.3.8 re Module The re module provides support for working with text via regular expression pattern matching. 10.3.9 shutil Module The shutil module has functions for performing common tasks related to the shell, such as copying files and directories. 10.3.10 statistics Module The statistics module provides functions for computing common statistical values such as means, medians, and standard deviation. 10.3.11 sys Module The sys module contains a variety of attributes and methods related to the runtime environment of Python itself. This includes command-line options, standard I/O streams, the import path, and similar features. 10.3.12 time Module The time module is where you find various functions related to system time, such as getting the value of the system clock, sleeping, and the number of elapsed CPU seconds. 10.3.13 turtle Module Turtle graphics. You know, for kids. 10.3.14 unittest Module The unittest module provides built-in support for writing unit tests. Python itself is tested using unittest. However, many programmers prefer using third-party libraries such as pytest for testing. This author concurs.","title":"getattr(object, name[, default])"},{"location":"python/modules/io/","text":"data representation Specify a bytes literal (note: b' prefix) a = b'hello' Specify bytes from a list of integers b = bytes([0x68, 0x65, 0x6c, 0x6c, 0x6f]) Create and populate a bytearray from parts c = bytearray() c.extend(b'world') # d = 'world' c.append(0x21) # d = 'world!' Access byte values print(a[0]) # --> prints 104 for x in b: # Outputs 104 101 108 108 111 print(x) a = b'hello' # bytes b = 'hello' # text c = 'world' # text print(a == b) # -> False d = a + c # TypeError: can't concat str to bytes e = b + c # -> 'helloworld' (both are strings) text encoding and decoding a = 'hello' # Text b = a.encode('utf-8') # Encode to bytes c = b'world' # Bytes d = c.decode('utf-8') # Decode to text 'ascii' Character values in the range [0x00, 0x7f]. 'latin1' Character values in the range [0x00, 0xff]. Also known as 'iso-8859-1'. 'utf-8' Variable-length encoding that allows all Unicode characters to be represented. 'cp1252' A common text encoding on Windows. 'macroman' A common text encoding on Macintosh. text and byte formatting x = 123.456 format(x, '0.2f') # '123.46' format(x, '10.4f') # ' 123.4560' format(x, '< 10.2f') # '123.46 ***' name = 'Elwood' r = format(name, '<10') # r = 'Elwood ' r = format(name, '>10') # r = ' Elwood' r = format(name, '^10') # r = ' Elwood ' r = format(name, ' ^10') # r = ' Elwood *' d Decimal integer or long integer. b Binary integer or long integer. o Octal integer or long integer. x Hexadecimal integer or long integer. X Hexadecimal integer (uppercase letters). f, F Floating point as [-]m.dddddd. e Floating point as [-]m.dddddde\u00b1xx. E Floating point as [-]m.ddddddE\u00b1xx. g, G Use e or E for exponents less than [nd]4 or greater than the precision; otherwise use f. n Same as g except that the current locale setting determines the decimal point character. % Multiplies a number by 100 and displays it using f format followed by a % sign. s String or any object. The formatting code uses str() to generate strings. c Single character. x = 42 r = format(x, '10d') # r = ' 42' r = format(x, '10x') # r = ' 2a' r = format(x, '10b') # r = ' 101010' r = format(x, '010b') # r = '0000101010' y = 3.1415926 r = format(y, '10.2f') # r = ' 3.14' r = format(y, '10.2e') # r = ' 3.14e+00' r = format(y, '+10.2f') # r = ' +3.14' r = format(y, '+010.2f') # r = '+000003.14' r = format(y, '+10.2%') # r = ' +314.16%' f'Value is {x:0.2f}' # 'Value is 123.46' f'Value is {x:10.4f}' # 'Value is 123.4560' f'Value is {2 x: <10.2f}' # 'Value is 246.91****' f'{x!r:spec}' # Calls (repr(x). format ('spec')) f'{x!s:spec}' # Calls (str(x). format ('spec')) 'Value is {:0.2f}' .format(x) # 'Value is 123.46' 'Value is {0:10.2f}' .format(x) # 'Value is 123.4560' 'Value is {val:< 10.2f}' .format(val=x) # 'Value is 123.46 ***' Unlike f-strings, the arg value of a specifier cannot be an arbitrary expression, so it\u2019s not quite as expressive. However, the format() method can perform limited attribute lookup, indexing, and nested substitutions. For example: y = 3.1415926 width = 8 precision=3 r = 'Value is {0:{1}.{2}f}'.format(y, width, precision) d = { 'name': 'IBM', 'shares': 50, 'price': 490.1 } r = '{0[shares]:d} shares of {0[name]} at {0[price]:0.2f}'.format(d) r = '50 shares of IBM at 490.10' command line arguments def main(argv): if len(argv) != 3: raise SystemExit( f'Usage : python {argv[0]} inputfile outputfile\\n') inputfile = argv[1] outputfile = argv[2] ... if name == ' main ': import sys main(sys.argv) import argparse def main(argv): p = argparse.ArgumentParser(description='This is some program') 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # A positional argument p.add_argument('infile') # An option taking an argument p.add_argument('-o','--output', action='store') # An option that sets a boolean flag p.add_argument('-d','--debug', action='store_true', default=False) # Parse the command line args = p.parse_args(args=argv) # Retrieve the option settings infile = args.infile output = args.output debugmode = args.debug print(infile, output, debugmode) if name == ' main ': import sys main(sys.argv[1:]) env variables import os path = os.environ['PATH'] user = os.environ['USER'] editor = os.environ['EDITOR'] val = os.environ['SOMEVAR'] buffering By default, files are opened with I/O buffering enabled. With I/O buffering, I/O operations are performed in larger chunks to avoid excessive system calls. For example, write operations would start filling an internal memory buffer and output would only actually occur when the buffer is filled up. This behavior can be changed by giving a buffering argument to open(). For example with open('data.bin', 'wb', buffering=0) as file: file.write(data) file.write(data) file.write(data) file.flush() # Make sure all data is written from buffers text mode enxoding with open('file.txt', 'rt', encoding='utf-8', errors='replace') as file: data = file.read() newline With text files, one complication is the encoding of newline characters. Newlines are encoded as '\\n', '\\r\\n', or '\\r' depending on the host operating system\u2014for example, '\\n' on UNIX and '\\r\\n' on Windows. By default, Python translates all of these line endings to a standard '\\n' character when reading. On writing, newline characters are translated back to the default line ending used on the system. The behavior is sometimes referred to as \u201cuniversal newline mode\u201d in Python documentation. file = open('somefile.txt', 'rt', newline='\\r\\n') behind in scenes The open() function serves as a kind of high-level factory function for creating instances of different I/O classes. These classes embody the different file modes, encodings, and buffering behaviors. They are also composed together in layers. The following classes are defined in the io module: Click here to view code image FileIO(filename, mode='r', closefd=True, opener=None) Opens a file for raw unbuffered binary I/O. filename is any valid filename accepted by the open() function. Other arguments have the same meaning as for open(). Click here to view code image BufferedReader(file [, buffer_size]) BufferedWriter(file [, buffer_size]) BufferedRandom(file,[, buffer_size]) Implements a buffered binary I/O layer for a file. file is an instance of FileIO. buffer_size specifies the internal buffer size to use. The choice of class depends on whether or not the file is reading, writing, or updating data. The optional buffer_size argument specifies the internal buffer size used. Click here to view code image TextIOWrapper(buffered, [encoding, [errors [, newline [, line_buffering [, write_through]]]]]) Implements text mode I/O. buffered is a buffered binary mode file, such as BufferedReader or BufferedWriter. The encoding, errors, and newline arguments have the same meaning as for open(). line_buffering is a Boolean flag that forces I/O to be flushed on newline characters (False by default). write_through is a Boolean flag that forces all writes to be flushed (False by default). Here is an example that shows how a text-mode file is constructed, layer-by-layer: Click here to view code image 1 raw = io.FileIO('filename.txt', 'r') # Raw-binary mode buffer = io.BufferedReader(raw) # Binary buffered reader file = io.TextIOWrapper(buffer, encoding='utf-8') # Text mode file methods f.readable() Returns True if file can be read. f.read([n]) Reads at most n bytes. f.readline([n]) Reads a single line of input up to n characters. If n is omitted, this method reads the entire line. f.readlines([size]) Reads all the lines and returns a list. size optionally specifies the approximate number of characters to read on the file before stopping. f.readinto(buffer) Reads data into a memory buffer. f.writable() Returns True if file can be written. f.write(s) Writes string s. f.writelines(lines) Writes all strings in iterable lines. f.close() Closes the file. f.seekable() Returns True if file supports random-access seeking. f.tell() Returns the current file pointer. f.seek(offset [, where]) Seeks to a new file position. f.isatty() Returns True if f is an interactive terminal. f.flush() Flushes the output buffers. f.truncate([size]) Truncates the file to at most size bytes. f.fileno() Returns an integer file descriptor. file attributes f.closed Boolean value indicates the file state: False if the file is open, True if closed. f.mode The I/O mode for the file. f.name Name of the file if created using open(). Otherwise, it will be a string indicating the source of the file. f.newlines The newline representation actually found in the file. The value is either None if no newlines have been encountered, a string containing '\\n', '\\r', or '\\r\\n', or a tuple containing all the different newline encodings seen. f.encoding A string that indicates file encoding, if any (for example, 'latin-1' or 'utf-8'). The value is None if no encoding is being used. f.errors The error handling policy. f.write_through Boolean value indicating if writes on a text file pass data directly to the underlying binary level file without buffering. stdin, stdout, stderr import sys sys.stdout.write('Enter your name : ') name = sys.stdin.readline() If necessary, the values of sys.stdout, sys.stdin, and sys.stderr can be replaced with other file objects, in which case the print() and input() functions will use the new values. Should it ever be necessary to restore the original value of sys.stdout, it should be saved first. The original values of sys.stdout, sys.stdin, and sys.stderr at interpreter startup are also available in sys.stdout, sys.stdin, and sys.stderr, respectively. directories import os names = os.listdir('dirname') for name in names: print(name) print print('The values are', x, y, z) Suppress the newline print('The values are', x, y, z, end='') To redirect the output to a file, use the file keyword argument: Redirect to file object f print('The values are', x, y, z, file=f) To change the separator character between items, use the sep keyword argument: Put commas between the values print('The values are', x, y, z, sep=',') consume input use advance generator for io def line_receiver(): data = bytearray() line = None linecount = 0 while True: part = yield line linecount += part.count(b'\\n') data.extend(part) if linecount > 0: index = data.index(b'\\n') line = bytes(data[:index+1]) data = data[index+1:] linecount -= 1 else: line = None r = line_receiver() r.send(None) # Necessary first step r.send(b'hello') r.send(b'world\\nit ') b'hello world\\n' r.send(b'works!') r.send(b'\\n') b'it works!\\n'' An interesting side effect of this approach is that it externalizes the actual I/O operations that must be performed to get the input data. Specifically, the implementation of line_receiver() contains no I/O operations at all! This means that it could be used in different contexts. For example, with sockets: r = line_receiver() data = None while True: while not (line:=r.send(data)): data = sock.recv(8192) 1 2 # Process the line ... r = line_receiver() data = None while True: while not (line:=r.send(data)): data = file.read(10000) 1 2 # Process the line ... async def reader(ch): r = line_receiver() data = None while True: while not (line:=r.send(data)): data = await ch.receive(8192) object serializations Sometimes it\u2019s necessary to serialize the representation of an object so it can be transmitted over the network, saved to a file, or stored in a database. One way to do this is to convert data into a standard encoding such as JSON or XML. There is also a common Python-specific data serialization format called Pickle. The pickle module serializes an object into a stream of bytes that can be used to reconstruct the object at a later point in time. The interface to pickle is simple, consisting of two operations, dump() and load(). For example, the following code writes an object to a file: import pickle obj = SomeObject() with open(filename, 'wb') as file: pickle.dump(obj, file) # Save object on f To restore the object, use: Click here to view code image with open(filename, 'rb') as file: obj = pickle.load(file) # Restore the object It is not normally necessary for user-defined objects to do anything extra to work with pickle. However, certain kinds of objects can\u2019t be pickled. These tend to be objects that incorporate runtime state\u2014open files, threads, closures, generators, and so on. To handle these tricky cases, a class can define the special methods getstate() and setstate(). The getstate() method, if defined, will be called to create a value representing the state of an object. The value returned by getstate() is typically a string, tuple, list, or dictionary. The setstate() method receives this value during unpickling and should restore the state of an object from it. do not unpickle unknow data blocking operations and concurency A fundamental aspect of I/O is the concept of blocking. By its very nature, I/O is connected to the real world. It often involves waiting for input or devices to be ready. For example, code that reads data on the network might perform a receive operation on a socket like this: data = sock.recv(8192) def reader1(sock): while (data := sock.recv(8192)): print('reader1 got:', data) def reader2(sock): while (data := sock.recv(8192)): print('reader2 got:', data) Problem: How to make reader1() and reader2() run at the same time? The rest of this section outlines a few different approaches to solving this problem. However, it is not meant to be a full tutorial on concurrency. For that, you will need to consult other resources. nonblocking io def run(sock1, sock2): sock1.setblocking(False) sock2.setblocking(False) while True: reader1(sock1) reader2(sock2) In practice, relying only on nonblocking I/O is clumsy and inefficient. For example, the core of this program is the run() function at the end. It will run in a inefficient busy loop as it constantly tries to read on the sockets. This works, but it is not a good design. IO polling Instead of relying upon exceptions and spinning, it is possible to poll I/O channels to see if data is available. The select or selectors module can be used for this purpose. For example, here\u2019s a slightly modified version of the run() function: from selectors import DefaultSelector, EVENT_READ, EVENT_WRITE def run(sock1, sock2): selector = DefaultSelector() selector.register(sock1, EVENT_READ, data=reader1) selector.register(sock2, EVENT_READ, data=reader2) # Wait for something to happen while True: for key, evt in selector.select(): func = key.data func(key.fileobj) In this code, the loop dispatches either reader1() or reader2() function as a callback whenever I/O is detected on the appropriate socket. The selector.select() operation itself blocks, waiting for I/O to occur. Thus, unlike the previous example, it won\u2019t make the CPU furiously spin. This approach to I/O is the foundation of many so-called \u201casync\u201d frameworks such as asyncio, although you usually don\u2019t see the inner workings of the event loop. threading import threading def reader1(sock): while (data := sock.recv(8192)): print('reader1 got:', data) def reader2(sock): while (data := sock.recv(8192)): print('reader2 got:', data) t1 = threading.Thread(target=reader1, args=[sock1]).start() t2 = threading.Thread(target=reader2, args=[sock2]).start() Start the threads t1.start() t2.start() Wait for the threads to finish t1.join() t2.join() asyncio import asyncio async def reader1(sock): loop = asyncio.get_event_loop() while (data := await loop.sock_recv(sock, 8192)): print('reader1 got:', data) async def reader2(sock): loop = asyncio.get_event_loop() while (data := await loop.sock_recv(sock, 8192)): print('reader2 got:', data) async def main(sock1, sock2): loop = asyncio.get_event_loop() t1 = loop.create_task(reader1(sock1)) t2 = loop.create_task(reader2(sock2)) 1 2 3 # Wait for the tasks to finish await t1 await t2 ... Run it asyncio.run(main(sock1, sock2)) asyncio tcp socket import asyncio from socket import * async def echo_server(address): loop = asyncio.get_event_loop() sock = socket(AF_INET, SOCK_STREAM) sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1) sock.bind(address) sock.listen(5) sock.setblocking(False) print('Server listening at', address) with sock: while True: client, addr = await loop.sock_accept(sock) print('Connection from', addr) loop.create_task(echo_client(loop, client)) async def echo_client(loop, client): with client: while True: data = await loop.sock_recv(client, 10000) if not data: break await loop.sock_sendall(client, b'Got:' + data) print('Connection closed') if name == ' main ': loop = asyncio.get_event_loop() loop.create_task(echo_server(loop, ('', 25000))) loop.run_forever() To test this code, use a program such as nc or telnet to connect to port 25000 on your machine. The code should echo back the text that you type. If you connect more than once using multiple terminal windows, you\u2019ll find that the code can handle all of the connections concurrently. Most applications using asyncio will probably operate at a higher level than sockets. However, in such applications, you will still have to make use of special async functions and interact with the underlying event loop in some manner. binascii converts binary data into text repr binascii.b2a_hex(b'hello') b'68656c6c6f' 1 binascii.a2b_hex() b'hello' binascii.b2a_base64(b'hello') b'aGVsbG8=\\n' binascii.a2b_base64() cgi module To register, please provide a contact name and email address. Your name: Your email: Here\u2019s a CGI script that receives the form data on the other end: Click here to view code image !/usr/bin/env python import cgi try: form = cgi.FieldStorage() name = form.getvalue('name') email = form.getvalue('email') # Validate the responses and do whatever ... # Produce an HTML result (or redirect) print('Status: 302 Moved\\r') print('Location: https://www.mywebsite.com/thanks.html\\r') print('\\r') except Exception as e: print('Status: 501 Error\\r') print('Content-type: text/plain\\r') print('\\r') print('Some kind of error occurred.\\r') Will writing such a CGI script get you a job at an Internet startup? Probably not. Will it solve your actual problem? Likely. configparser ; A comment [section1] name1 = value1 name2 = value2 [section2] ; Alternative syntax name1: value1 name2: value2 cfg = configparser.ConfigParser() cfg.read('conig.ini') Extract values a = cfg.get('section1', 'name1') b = cfg.get('section2', 'name2') errorno so much error handlerrs fcntl module low level io tool open file with lock to avoid concurent open import fcntl with open('somefile', 'r') as file: try: fcntl.flock(file.fileno(), fcntl.LOCK_EX) # Use the file ... finally: fcntl.flock(file.fileno(), fcntl.LOCK_UN) hashlib The hashlib module provides functions for computing cryptographic hash values such as MD5 and SHA-1. The following example illustrates how to use the module: Click here to view code image 1 h = hashlib.new('sha256') h.update(b'Hello') # Feed data h.update(b'World') h.digest() b'\\xa5\\x91\\xa6\\xd4\\x0b\\xf4 @J\\x01\\x173\\xcf\\xb7\\xb1\\x90\\xd6,e\\xbf\\x0b\\xcd\\xa3+W\\xb2w\\xd9\\xad\\x9f\\x14n h.hexdigest() 'a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e' h.digest_size 32 https package io The io module primarily contains the definitions of classes used to implement the file objects as returned by the open() function. It is not so common to access those classes directly. However, the module also contains a pair of classes that are useful for \u201cfaking\u201d a file in the form of strings and bytes. This can be useful for testing and other applications where you need to provide a \u201cfile\u201d but have obtained data in a different way. The StringIO() class provides a file-like interface on top of strings. For example, here is how you can write output to a string: import io file = io.StringIO() greeting(file) Get the resulting output output = file.getvalue() logging The logging module is the de facto standard module used for reporting program diagnostics and for print-style debugging. It can be used to route output to a log file and provides a large number of configuration options. A common practice is to write code that creates a Logger instance and issues messages on it like this: Click here to view code image import logging log = logging.getLogger( name ) Function that uses logging def func(args): log.debug('A debugging message') log.info('An informational message') log.warning('A warning message') log.error('An error message') log.critical('A critical message') Configuration of logging (occurs one at program startup) if name == ' main ': logging.basicConfig( level=logging.WARNING, filename='output.log' ) There are five built-in levels of logging ordered by increasing severity. When configuring the logging system, you specify a level that acts as a filter. Only messages at that level or greater severity are reported. Logging provides a large number of configuration options, mostly related to the back-end handling of the log messages. Usually you don\u2019t need to know about that when writing application code\u2014you use debug(), info(), warning(), and similar methods on some given Logger instance. Any special configuration takes place during program startup in a special location (such as a main() function or the main code block). pathlib from pathlib import Path filename = Path('/Users/beazley/old/data.csv') Once you have an instance filename of Path, you can perform various operations on it to manipulate the filename. For example: Click here to view code image filename.name 'data.csv' filename.parent Path('/Users/beazley/old') filename.parent / 'newfile.csv' Path('/Users/beazley/old/newfile.csv') filename.parts ('/', 'Users', 'beazley', 'old', 'data.csv') filename.with_suffix('.csv.clean') Path('/Users/beazley/old/data.csv.clean') import pathlib def compute_usage(filename): pathname = pathlib.Path(filename) if pathname.is_file(): return pathname.stat().st_size elif pathname.is_dir(): return sum(path.stat().st_size for path in pathname.rglob('*') if path.is_file()) return pathname.stat().st_size else: raise RuntimeError('Unsupported file kind') re regex shutil some shell commadns import shutil shutil.copy(srcfile, dstfile) To move a file: Click here to view code image shutil.move(srcfile, dstfile) To copy a directory tree: Click here to view code image shutil.copytree(srcdir, dstdir) To remove a directory tree: shutil.rmtree(pathname) The shutil module is often used as a safer and more portable alternative to directly executing shell commands with the os.system() function. select The select module is used for simple polling of multiple I/O streams. That is, it can be used to watch a collection of file descriptors for incoming data or for the ability to receive outgoing data. The following example shows typical usage: import select Collections of objects representing file descriptors. Must be integers or objects with a fileno() method. want_to_read = [ ... ] want_to_write = [ ... ] check_exceptions = [ ... ] Timeout (or None) timeout = None Poll for I/O can_read, can_write, have_exceptions = \\ select.select(want_to_read, want_to_write, check_exceptions, timeout) Perform I/O operations for file in can_read: do_read(file) for file in can_write: do_write(file) Handle exceptions for file in have_exceptions: handle_exception(file) smtlib import smtplib fromaddr = 'someone@some.com' toaddrs = ['recipient@other.com' ] amount = 123.45 msg = f'''From: {fromaddr}\\r \\r Pay {amount} bitcoin or else. We're watching.\\r ''' server = smtplib.SMTP('localhost') serv.sendmail(fromaddr, toaddrs, msg) serv.quit() socket use telnet or nc from socket import socket, AF_INET, SOCK_STREAM sock = socket(AF_INET, SOCK_STREAM) sock.connect(('python.org', 80)) sock.send(b'GET /index.html HTTP/1.0\\r\\n\\r\\n') parts = [] while True: part = sock.recv(10000) if not part: break parts.append(part) response = b''.join(part) print(part) struct The struct module is used to convert data between Python and binary data structures, represented as Python byte strings. These data structures are often used when interacting with functions written in C, binary file formats, network protocols, or binary communication over serial ports. As an example, suppose you need to construct a binary message with its format described by a C data structure: Click here to view code image Message format: All values are 'big endian' struct Message { unsigned short msgid; // 16 bit unsigned integer unsigned int sequence; // 32 bit sequence number float x; // 32 bit float float y; // 32 bit float } subprocess import subprocess Run the 'netstat -a' command and collect its output try: out = subprocess.check_output(['netstat', '-a']) except subprocess.CalledProcessError as e: print('It failed:', e) The data returned by check_output() is presented as bytes. If you want to convert it to text, make sure you apply a proper decoding: Click here to view code image text = out.decode('utf-8') It is also possible to set up a pipe and to interact with a subprocess in a more detailed manner. To do that, use the Popen class like this: Click here to view code image import subprocess p = subprocess.Popen(['wc'], stdin=subprocess.PIPE, stdout=subprocess.PIPE) Send data to the subprocess p.stdin.write(b'hello world\\nthis is a test\\n') p.stdin.close() Read data back out = p.stdout.read() print(out) tmpfile temp files textwrap wrapped = textwrap.wrap(text, width=81) print('\\n'.join(wrapped)) threading threads import threading import time def countdown(n): while n > 0: print('T-minus', n) n -= 1 time.sleep(1) t = threading.Thread(target=countdown, args=[10]) t.start() t.join() # Wait for the thread to finish If you\u2019re never going to wait for the thread to finish, make it daemonic by supplying an extra daemon flag like this: Click here to view code image t = threading.Thread(target=countdown, args=[10], daemon=True) to stop import threading import time must_stop = False def countdown(n): while n > 0 and not must_stop: print('T-minus', n) n -= 1 time.sleep(1) thread lock import threading class Counter: def init (self): self.value = 0 self.lock = threading.Lock() 1 2 3 4 5 6 7 def increment(self): with self.lock: self.value += 1 def decrement(self): with self.lock: self.value -= 1 threading event def step1(evt): print('Step 1') time.sleep(5) evt.set() def step2(evt): evt.wait() print('Step 2') evt = threading.Event() threading.Thread(target=step1, args=[evt]).start() threading.Thread(target=step2, args=[evt]).start() thread queue import threading import queue import time def producer(q): for i in range(10): print('Producing:', i) q.put(i) print('Done') q.put(None) def consumer(q): while True: item = q.get() if item is None: break print('Consuming:', item) print('Goodbye') q = queue.Queue() threading.Thread(target=producer, args=[q]).start() threading.Thread(target=consumer, args=[q]).start() time The time module is used to access system time-related functions. The following selected functions are the most useful: sleep(seconds) Make Python sleep for a given number of seconds, given as a floating point. time() Return the current system time in UTC as a floating-point number. This is the number of seconds since the epoch (usually January 1, 1970 for UNIX systems). Use localtime() to convert it into a data structure suitable for extracting useful information. localtime([secs]) Return a struct_time object representing the local time on the system or the time represented by the floating-point value secs passed as an argument. The resulting struct has attributes tm_year, tm_mon, tm_mday, tm_hour, tm_min, tm_sec, tm_wday, tm_yday, and tm_isdst. gmtime([secs]) The same as localtime() except that the resulting structure represents the time in UTC (or Greenwich Mean Time). ctime([secs]) Convert a time represented as seconds to a text string suitable for printing. Useful for debugging and logging. asctime(tm) Convert a time structure as represented by localtime() into a text string suitable for printing. The datetime module is more generally used for representing dates and times for the purpose of performing date-related computations and dealing with timezones. urllib from urllib.request import urlopen u = urlopen('http://www.python.org') data = u.read() If you want to encode form parameters, you can use urllib.parse.urlencode() as shown here: Click here to view code image from urllib.parse import urlencode from urllib.request import urlopen form = { 'name': 'Mary A. Python', 'email': 'mary123@python.org' } data = urlencode(form) u = urlopen('http://httpbin.org/post', data.encode('utf-8')) response = u.read() The urlopen() function works fine for basic webpages and APIs involving HTTP or HTTPS. However, it becomes quite awkward to use if access also involves cookies, advanced authentication schemes, and other layers. Frankly, most Python programmers would use a third-party library such as requests or httpx to handle these situations. You should too. The urllib.parse subpackage has additional functions for manipulating URLs themselves. For example, the urlparse() function can be used to pull apart a URL: Click here to view code image url = 'http://httpbin.org/get?name=Dave&n=42' from urllib.parse import urlparse urlparse(url) ParseResult(scheme='http', netloc='httpbin.org', path='/get', params='', query='name=Dave&n=42', fragment='') unicodedata for unicode strings unicodedata.normalize(option) xml from xml.etree.ElementTree import ElementTree doc = ElementTree(file='recipe.xml') title = doc.find('title') print(title.text) Alternative (just get element text) print(doc.findtext('description')) Iterate over multiple elements for item in doc.findall('ingredients/item'): num = item.get('num') units = item.get('units', '') text = item.text.strip() print(f'{num} {units} {text}') I/O is a fundamental part of writing any useful program. Given its popularity, Python is able to work with literally any data format, encoding, or document structure that\u2019s in use. Although the standard library might not support it, you will almost certainly find a third-party module to solve your problem. In the big picture, it may be more useful to think about the edges of your application. At the outer boundary between your program and reality, it\u2019s common to encounter issues related to data encoding. This is especially true for textual data and Unicode. Much of the complexity in Python\u2019s I/O handling\u2014supporting different encoding, error handling policies, and so on\u2014is aimed at this specific problem. It\u2019s also critical to keep in mind that textual data and binary data are strictly separated. Knowing what you\u2019re working with helps in understanding the big picture. A secondary consideration in I/O is the overall evaluation model. Python code is currently separated into two worlds\u2014normal synchronous code and asynchronous code usually associated with the asyncio module (characterized by the use of async functions and the async/await syntax). Asynchronous code almost always requires using dedicated libraries that are capable of operating in that environment. This, in turn, forces your hand on writing your application code in the \u201casync\u201d style as well. Honestly, you should probably avoid asynchronous coding unless you absolutely know that you need it\u2014and if you\u2019re not really sure, then you almost certainly don\u2019t. Most of the well-adjusted Python-speaking universe codes in a normal synchronous style that is far easier to reason about, debug, and test. You should choose that.","title":"IO"},{"location":"python/modules/io/#specify-a-bytes-literal-note-b-prefix","text":"a = b'hello'","title":"Specify a bytes literal (note: b' prefix)"},{"location":"python/modules/io/#specify-bytes-from-a-list-of-integers","text":"b = bytes([0x68, 0x65, 0x6c, 0x6c, 0x6f])","title":"Specify bytes from a list of integers"},{"location":"python/modules/io/#create-and-populate-a-bytearray-from-parts","text":"c = bytearray() c.extend(b'world') # d = 'world' c.append(0x21) # d = 'world!'","title":"Create and populate a bytearray from parts"},{"location":"python/modules/io/#access-byte-values","text":"print(a[0]) # --> prints 104 for x in b: # Outputs 104 101 108 108 111 print(x) a = b'hello' # bytes b = 'hello' # text c = 'world' # text print(a == b) # -> False d = a + c # TypeError: can't concat str to bytes e = b + c # -> 'helloworld' (both are strings) text encoding and decoding a = 'hello' # Text b = a.encode('utf-8') # Encode to bytes c = b'world' # Bytes d = c.decode('utf-8') # Decode to text 'ascii' Character values in the range [0x00, 0x7f]. 'latin1' Character values in the range [0x00, 0xff]. Also known as 'iso-8859-1'. 'utf-8' Variable-length encoding that allows all Unicode characters to be represented. 'cp1252' A common text encoding on Windows. 'macroman' A common text encoding on Macintosh. text and byte formatting x = 123.456 format(x, '0.2f') # '123.46' format(x, '10.4f') # ' 123.4560' format(x, '< 10.2f') # '123.46 ***' name = 'Elwood' r = format(name, '<10') # r = 'Elwood ' r = format(name, '>10') # r = ' Elwood' r = format(name, '^10') # r = ' Elwood ' r = format(name, ' ^10') # r = ' Elwood *' d Decimal integer or long integer. b Binary integer or long integer. o Octal integer or long integer. x Hexadecimal integer or long integer. X Hexadecimal integer (uppercase letters). f, F Floating point as [-]m.dddddd. e Floating point as [-]m.dddddde\u00b1xx. E Floating point as [-]m.ddddddE\u00b1xx. g, G Use e or E for exponents less than [nd]4 or greater than the precision; otherwise use f. n Same as g except that the current locale setting determines the decimal point character. % Multiplies a number by 100 and displays it using f format followed by a % sign. s String or any object. The formatting code uses str() to generate strings. c Single character. x = 42 r = format(x, '10d') # r = ' 42' r = format(x, '10x') # r = ' 2a' r = format(x, '10b') # r = ' 101010' r = format(x, '010b') # r = '0000101010' y = 3.1415926 r = format(y, '10.2f') # r = ' 3.14' r = format(y, '10.2e') # r = ' 3.14e+00' r = format(y, '+10.2f') # r = ' +3.14' r = format(y, '+010.2f') # r = '+000003.14' r = format(y, '+10.2%') # r = ' +314.16%' f'Value is {x:0.2f}' # 'Value is 123.46' f'Value is {x:10.4f}' # 'Value is 123.4560' f'Value is {2 x: <10.2f}' # 'Value is 246.91****' f'{x!r:spec}' # Calls (repr(x). format ('spec')) f'{x!s:spec}' # Calls (str(x). format ('spec')) 'Value is {:0.2f}' .format(x) # 'Value is 123.46' 'Value is {0:10.2f}' .format(x) # 'Value is 123.4560' 'Value is {val:< 10.2f}' .format(val=x) # 'Value is 123.46 ***' Unlike f-strings, the arg value of a specifier cannot be an arbitrary expression, so it\u2019s not quite as expressive. However, the format() method can perform limited attribute lookup, indexing, and nested substitutions. For example: y = 3.1415926 width = 8 precision=3 r = 'Value is {0:{1}.{2}f}'.format(y, width, precision) d = { 'name': 'IBM', 'shares': 50, 'price': 490.1 } r = '{0[shares]:d} shares of {0[name]} at {0[price]:0.2f}'.format(d)","title":"Access byte values"},{"location":"python/modules/io/#r-50-shares-of-ibm-at-49010","text":"command line arguments def main(argv): if len(argv) != 3: raise SystemExit( f'Usage : python {argv[0]} inputfile outputfile\\n') inputfile = argv[1] outputfile = argv[2] ... if name == ' main ': import sys main(sys.argv) import argparse def main(argv): p = argparse.ArgumentParser(description='This is some program') 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # A positional argument p.add_argument('infile') # An option taking an argument p.add_argument('-o','--output', action='store') # An option that sets a boolean flag p.add_argument('-d','--debug', action='store_true', default=False) # Parse the command line args = p.parse_args(args=argv) # Retrieve the option settings infile = args.infile output = args.output debugmode = args.debug print(infile, output, debugmode) if name == ' main ': import sys main(sys.argv[1:]) env variables import os path = os.environ['PATH'] user = os.environ['USER'] editor = os.environ['EDITOR'] val = os.environ['SOMEVAR'] buffering By default, files are opened with I/O buffering enabled. With I/O buffering, I/O operations are performed in larger chunks to avoid excessive system calls. For example, write operations would start filling an internal memory buffer and output would only actually occur when the buffer is filled up. This behavior can be changed by giving a buffering argument to open(). For example with open('data.bin', 'wb', buffering=0) as file: file.write(data) file.write(data) file.write(data) file.flush() # Make sure all data is written from buffers text mode enxoding with open('file.txt', 'rt', encoding='utf-8', errors='replace') as file: data = file.read() newline With text files, one complication is the encoding of newline characters. Newlines are encoded as '\\n', '\\r\\n', or '\\r' depending on the host operating system\u2014for example, '\\n' on UNIX and '\\r\\n' on Windows. By default, Python translates all of these line endings to a standard '\\n' character when reading. On writing, newline characters are translated back to the default line ending used on the system. The behavior is sometimes referred to as \u201cuniversal newline mode\u201d in Python documentation. file = open('somefile.txt', 'rt', newline='\\r\\n') behind in scenes The open() function serves as a kind of high-level factory function for creating instances of different I/O classes. These classes embody the different file modes, encodings, and buffering behaviors. They are also composed together in layers. The following classes are defined in the io module: Click here to view code image FileIO(filename, mode='r', closefd=True, opener=None) Opens a file for raw unbuffered binary I/O. filename is any valid filename accepted by the open() function. Other arguments have the same meaning as for open(). Click here to view code image BufferedReader(file [, buffer_size]) BufferedWriter(file [, buffer_size]) BufferedRandom(file,[, buffer_size]) Implements a buffered binary I/O layer for a file. file is an instance of FileIO. buffer_size specifies the internal buffer size to use. The choice of class depends on whether or not the file is reading, writing, or updating data. The optional buffer_size argument specifies the internal buffer size used. Click here to view code image TextIOWrapper(buffered, [encoding, [errors [, newline [, line_buffering [, write_through]]]]]) Implements text mode I/O. buffered is a buffered binary mode file, such as BufferedReader or BufferedWriter. The encoding, errors, and newline arguments have the same meaning as for open(). line_buffering is a Boolean flag that forces I/O to be flushed on newline characters (False by default). write_through is a Boolean flag that forces all writes to be flushed (False by default). Here is an example that shows how a text-mode file is constructed, layer-by-layer: Click here to view code image 1 raw = io.FileIO('filename.txt', 'r') # Raw-binary mode buffer = io.BufferedReader(raw) # Binary buffered reader file = io.TextIOWrapper(buffer, encoding='utf-8') # Text mode file methods f.readable() Returns True if file can be read. f.read([n]) Reads at most n bytes. f.readline([n]) Reads a single line of input up to n characters. If n is omitted, this method reads the entire line. f.readlines([size]) Reads all the lines and returns a list. size optionally specifies the approximate number of characters to read on the file before stopping. f.readinto(buffer) Reads data into a memory buffer. f.writable() Returns True if file can be written. f.write(s) Writes string s. f.writelines(lines) Writes all strings in iterable lines. f.close() Closes the file. f.seekable() Returns True if file supports random-access seeking. f.tell() Returns the current file pointer. f.seek(offset [, where]) Seeks to a new file position. f.isatty() Returns True if f is an interactive terminal. f.flush() Flushes the output buffers. f.truncate([size]) Truncates the file to at most size bytes. f.fileno() Returns an integer file descriptor. file attributes f.closed Boolean value indicates the file state: False if the file is open, True if closed. f.mode The I/O mode for the file. f.name Name of the file if created using open(). Otherwise, it will be a string indicating the source of the file. f.newlines The newline representation actually found in the file. The value is either None if no newlines have been encountered, a string containing '\\n', '\\r', or '\\r\\n', or a tuple containing all the different newline encodings seen. f.encoding A string that indicates file encoding, if any (for example, 'latin-1' or 'utf-8'). The value is None if no encoding is being used. f.errors The error handling policy. f.write_through Boolean value indicating if writes on a text file pass data directly to the underlying binary level file without buffering. stdin, stdout, stderr import sys sys.stdout.write('Enter your name : ') name = sys.stdin.readline() If necessary, the values of sys.stdout, sys.stdin, and sys.stderr can be replaced with other file objects, in which case the print() and input() functions will use the new values. Should it ever be necessary to restore the original value of sys.stdout, it should be saved first. The original values of sys.stdout, sys.stdin, and sys.stderr at interpreter startup are also available in sys.stdout, sys.stdin, and sys.stderr, respectively. directories import os names = os.listdir('dirname') for name in names: print(name) print print('The values are', x, y, z)","title":"r = '50 shares of IBM at 490.10'"},{"location":"python/modules/io/#suppress-the-newline","text":"print('The values are', x, y, z, end='') To redirect the output to a file, use the file keyword argument:","title":"Suppress the newline"},{"location":"python/modules/io/#redirect-to-file-object-f","text":"print('The values are', x, y, z, file=f) To change the separator character between items, use the sep keyword argument:","title":"Redirect to file object f"},{"location":"python/modules/io/#put-commas-between-the-values","text":"print('The values are', x, y, z, sep=',') consume input use advance generator for io def line_receiver(): data = bytearray() line = None linecount = 0 while True: part = yield line linecount += part.count(b'\\n') data.extend(part) if linecount > 0: index = data.index(b'\\n') line = bytes(data[:index+1]) data = data[index+1:] linecount -= 1 else: line = None r = line_receiver() r.send(None) # Necessary first step r.send(b'hello') r.send(b'world\\nit ') b'hello world\\n' r.send(b'works!') r.send(b'\\n') b'it works!\\n'' An interesting side effect of this approach is that it externalizes the actual I/O operations that must be performed to get the input data. Specifically, the implementation of line_receiver() contains no I/O operations at all! This means that it could be used in different contexts. For example, with sockets: r = line_receiver() data = None while True: while not (line:=r.send(data)): data = sock.recv(8192) 1 2 # Process the line ... r = line_receiver() data = None while True: while not (line:=r.send(data)): data = file.read(10000) 1 2 # Process the line ... async def reader(ch): r = line_receiver() data = None while True: while not (line:=r.send(data)): data = await ch.receive(8192) object serializations Sometimes it\u2019s necessary to serialize the representation of an object so it can be transmitted over the network, saved to a file, or stored in a database. One way to do this is to convert data into a standard encoding such as JSON or XML. There is also a common Python-specific data serialization format called Pickle. The pickle module serializes an object into a stream of bytes that can be used to reconstruct the object at a later point in time. The interface to pickle is simple, consisting of two operations, dump() and load(). For example, the following code writes an object to a file: import pickle obj = SomeObject() with open(filename, 'wb') as file: pickle.dump(obj, file) # Save object on f To restore the object, use: Click here to view code image with open(filename, 'rb') as file: obj = pickle.load(file) # Restore the object It is not normally necessary for user-defined objects to do anything extra to work with pickle. However, certain kinds of objects can\u2019t be pickled. These tend to be objects that incorporate runtime state\u2014open files, threads, closures, generators, and so on. To handle these tricky cases, a class can define the special methods getstate() and setstate(). The getstate() method, if defined, will be called to create a value representing the state of an object. The value returned by getstate() is typically a string, tuple, list, or dictionary. The setstate() method receives this value during unpickling and should restore the state of an object from it. do not unpickle unknow data blocking operations and concurency A fundamental aspect of I/O is the concept of blocking. By its very nature, I/O is connected to the real world. It often involves waiting for input or devices to be ready. For example, code that reads data on the network might perform a receive operation on a socket like this: data = sock.recv(8192) def reader1(sock): while (data := sock.recv(8192)): print('reader1 got:', data) def reader2(sock): while (data := sock.recv(8192)): print('reader2 got:', data)","title":"Put commas between the values"},{"location":"python/modules/io/#problem-how-to-make-reader1-and-reader2","text":"","title":"Problem: How to make reader1() and reader2()"},{"location":"python/modules/io/#run-at-the-same-time","text":"The rest of this section outlines a few different approaches to solving this problem. However, it is not meant to be a full tutorial on concurrency. For that, you will need to consult other resources. nonblocking io def run(sock1, sock2): sock1.setblocking(False) sock2.setblocking(False) while True: reader1(sock1) reader2(sock2) In practice, relying only on nonblocking I/O is clumsy and inefficient. For example, the core of this program is the run() function at the end. It will run in a inefficient busy loop as it constantly tries to read on the sockets. This works, but it is not a good design. IO polling Instead of relying upon exceptions and spinning, it is possible to poll I/O channels to see if data is available. The select or selectors module can be used for this purpose. For example, here\u2019s a slightly modified version of the run() function: from selectors import DefaultSelector, EVENT_READ, EVENT_WRITE def run(sock1, sock2): selector = DefaultSelector() selector.register(sock1, EVENT_READ, data=reader1) selector.register(sock2, EVENT_READ, data=reader2) # Wait for something to happen while True: for key, evt in selector.select(): func = key.data func(key.fileobj) In this code, the loop dispatches either reader1() or reader2() function as a callback whenever I/O is detected on the appropriate socket. The selector.select() operation itself blocks, waiting for I/O to occur. Thus, unlike the previous example, it won\u2019t make the CPU furiously spin. This approach to I/O is the foundation of many so-called \u201casync\u201d frameworks such as asyncio, although you usually don\u2019t see the inner workings of the event loop. threading import threading def reader1(sock): while (data := sock.recv(8192)): print('reader1 got:', data) def reader2(sock): while (data := sock.recv(8192)): print('reader2 got:', data) t1 = threading.Thread(target=reader1, args=[sock1]).start() t2 = threading.Thread(target=reader2, args=[sock2]).start()","title":"run at the same time?"},{"location":"python/modules/io/#start-the-threads","text":"t1.start() t2.start()","title":"Start the threads"},{"location":"python/modules/io/#wait-for-the-threads-to-finish","text":"t1.join() t2.join() asyncio import asyncio async def reader1(sock): loop = asyncio.get_event_loop() while (data := await loop.sock_recv(sock, 8192)): print('reader1 got:', data) async def reader2(sock): loop = asyncio.get_event_loop() while (data := await loop.sock_recv(sock, 8192)): print('reader2 got:', data) async def main(sock1, sock2): loop = asyncio.get_event_loop() t1 = loop.create_task(reader1(sock1)) t2 = loop.create_task(reader2(sock2)) 1 2 3 # Wait for the tasks to finish await t1 await t2 ...","title":"Wait for the threads to finish"},{"location":"python/modules/io/#run-it","text":"asyncio.run(main(sock1, sock2)) asyncio tcp socket import asyncio from socket import * async def echo_server(address): loop = asyncio.get_event_loop() sock = socket(AF_INET, SOCK_STREAM) sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1) sock.bind(address) sock.listen(5) sock.setblocking(False) print('Server listening at', address) with sock: while True: client, addr = await loop.sock_accept(sock) print('Connection from', addr) loop.create_task(echo_client(loop, client)) async def echo_client(loop, client): with client: while True: data = await loop.sock_recv(client, 10000) if not data: break await loop.sock_sendall(client, b'Got:' + data) print('Connection closed') if name == ' main ': loop = asyncio.get_event_loop() loop.create_task(echo_server(loop, ('', 25000))) loop.run_forever() To test this code, use a program such as nc or telnet to connect to port 25000 on your machine. The code should echo back the text that you type. If you connect more than once using multiple terminal windows, you\u2019ll find that the code can handle all of the connections concurrently. Most applications using asyncio will probably operate at a higher level than sockets. However, in such applications, you will still have to make use of special async functions and interact with the underlying event loop in some manner. binascii converts binary data into text repr binascii.b2a_hex(b'hello') b'68656c6c6f' 1 binascii.a2b_hex() b'hello' binascii.b2a_base64(b'hello') b'aGVsbG8=\\n' binascii.a2b_base64() cgi module To register, please provide a contact name and email address. Your name: Your email: Here\u2019s a CGI script that receives the form data on the other end: Click here to view code image","title":"Run it"},{"location":"python/modules/io/#usrbinenv-python","text":"import cgi try: form = cgi.FieldStorage() name = form.getvalue('name') email = form.getvalue('email') # Validate the responses and do whatever ... # Produce an HTML result (or redirect) print('Status: 302 Moved\\r') print('Location: https://www.mywebsite.com/thanks.html\\r') print('\\r') except Exception as e: print('Status: 501 Error\\r') print('Content-type: text/plain\\r') print('\\r') print('Some kind of error occurred.\\r') Will writing such a CGI script get you a job at an Internet startup? Probably not. Will it solve your actual problem? Likely. configparser ; A comment [section1] name1 = value1 name2 = value2 [section2] ; Alternative syntax name1: value1 name2: value2 cfg = configparser.ConfigParser() cfg.read('conig.ini')","title":"!/usr/bin/env python"},{"location":"python/modules/io/#extract-values","text":"a = cfg.get('section1', 'name1') b = cfg.get('section2', 'name2') errorno so much error handlerrs fcntl module low level io tool open file with lock to avoid concurent open import fcntl with open('somefile', 'r') as file: try: fcntl.flock(file.fileno(), fcntl.LOCK_EX) # Use the file ... finally: fcntl.flock(file.fileno(), fcntl.LOCK_UN) hashlib The hashlib module provides functions for computing cryptographic hash values such as MD5 and SHA-1. The following example illustrates how to use the module: Click here to view code image 1 h = hashlib.new('sha256') h.update(b'Hello') # Feed data h.update(b'World') h.digest() b'\\xa5\\x91\\xa6\\xd4\\x0b\\xf4 @J\\x01\\x173\\xcf\\xb7\\xb1\\x90\\xd6,e\\xbf\\x0b\\xcd\\xa3+W\\xb2w\\xd9\\xad\\x9f\\x14n h.hexdigest() 'a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e' h.digest_size 32 https package io The io module primarily contains the definitions of classes used to implement the file objects as returned by the open() function. It is not so common to access those classes directly. However, the module also contains a pair of classes that are useful for \u201cfaking\u201d a file in the form of strings and bytes. This can be useful for testing and other applications where you need to provide a \u201cfile\u201d but have obtained data in a different way. The StringIO() class provides a file-like interface on top of strings. For example, here is how you can write output to a string: import io file = io.StringIO() greeting(file)","title":"Extract values"},{"location":"python/modules/io/#get-the-resulting-output","text":"output = file.getvalue() logging The logging module is the de facto standard module used for reporting program diagnostics and for print-style debugging. It can be used to route output to a log file and provides a large number of configuration options. A common practice is to write code that creates a Logger instance and issues messages on it like this: Click here to view code image import logging log = logging.getLogger( name )","title":"Get the resulting output"},{"location":"python/modules/io/#function-that-uses-logging","text":"def func(args): log.debug('A debugging message') log.info('An informational message') log.warning('A warning message') log.error('An error message') log.critical('A critical message')","title":"Function that uses logging"},{"location":"python/modules/io/#configuration-of-logging-occurs-one-at-program-startup","text":"if name == ' main ': logging.basicConfig( level=logging.WARNING, filename='output.log' ) There are five built-in levels of logging ordered by increasing severity. When configuring the logging system, you specify a level that acts as a filter. Only messages at that level or greater severity are reported. Logging provides a large number of configuration options, mostly related to the back-end handling of the log messages. Usually you don\u2019t need to know about that when writing application code\u2014you use debug(), info(), warning(), and similar methods on some given Logger instance. Any special configuration takes place during program startup in a special location (such as a main() function or the main code block). pathlib from pathlib import Path filename = Path('/Users/beazley/old/data.csv') Once you have an instance filename of Path, you can perform various operations on it to manipulate the filename. For example: Click here to view code image filename.name 'data.csv' filename.parent Path('/Users/beazley/old') filename.parent / 'newfile.csv' Path('/Users/beazley/old/newfile.csv') filename.parts ('/', 'Users', 'beazley', 'old', 'data.csv') filename.with_suffix('.csv.clean') Path('/Users/beazley/old/data.csv.clean') import pathlib def compute_usage(filename): pathname = pathlib.Path(filename) if pathname.is_file(): return pathname.stat().st_size elif pathname.is_dir(): return sum(path.stat().st_size for path in pathname.rglob('*') if path.is_file()) return pathname.stat().st_size else: raise RuntimeError('Unsupported file kind') re regex shutil some shell commadns import shutil shutil.copy(srcfile, dstfile) To move a file: Click here to view code image shutil.move(srcfile, dstfile) To copy a directory tree: Click here to view code image shutil.copytree(srcdir, dstdir) To remove a directory tree: shutil.rmtree(pathname) The shutil module is often used as a safer and more portable alternative to directly executing shell commands with the os.system() function. select The select module is used for simple polling of multiple I/O streams. That is, it can be used to watch a collection of file descriptors for incoming data or for the ability to receive outgoing data. The following example shows typical usage: import select","title":"Configuration of logging (occurs one at program startup)"},{"location":"python/modules/io/#collections-of-objects-representing-file-descriptors-must-be","text":"","title":"Collections of objects representing file descriptors.  Must be"},{"location":"python/modules/io/#integers-or-objects-with-a-fileno-method","text":"want_to_read = [ ... ] want_to_write = [ ... ] check_exceptions = [ ... ]","title":"integers or objects with a fileno() method."},{"location":"python/modules/io/#timeout-or-none","text":"timeout = None","title":"Timeout (or None)"},{"location":"python/modules/io/#poll-for-io","text":"can_read, can_write, have_exceptions = \\ select.select(want_to_read, want_to_write, check_exceptions, timeout)","title":"Poll for I/O"},{"location":"python/modules/io/#perform-io-operations","text":"for file in can_read: do_read(file) for file in can_write: do_write(file)","title":"Perform I/O operations"},{"location":"python/modules/io/#handle-exceptions","text":"for file in have_exceptions: handle_exception(file) smtlib import smtplib fromaddr = 'someone@some.com' toaddrs = ['recipient@other.com' ] amount = 123.45 msg = f'''From: {fromaddr}\\r \\r Pay {amount} bitcoin or else. We're watching.\\r ''' server = smtplib.SMTP('localhost') serv.sendmail(fromaddr, toaddrs, msg) serv.quit() socket use telnet or nc from socket import socket, AF_INET, SOCK_STREAM sock = socket(AF_INET, SOCK_STREAM) sock.connect(('python.org', 80)) sock.send(b'GET /index.html HTTP/1.0\\r\\n\\r\\n') parts = [] while True: part = sock.recv(10000) if not part: break parts.append(part) response = b''.join(part) print(part) struct The struct module is used to convert data between Python and binary data structures, represented as Python byte strings. These data structures are often used when interacting with functions written in C, binary file formats, network protocols, or binary communication over serial ports. As an example, suppose you need to construct a binary message with its format described by a C data structure: Click here to view code image Message format: All values are 'big endian' struct Message { unsigned short msgid; // 16 bit unsigned integer unsigned int sequence; // 32 bit sequence number float x; // 32 bit float float y; // 32 bit float } subprocess import subprocess","title":"Handle exceptions"},{"location":"python/modules/io/#run-the-netstat-a-command-and-collect-its-output","text":"try: out = subprocess.check_output(['netstat', '-a']) except subprocess.CalledProcessError as e: print('It failed:', e) The data returned by check_output() is presented as bytes. If you want to convert it to text, make sure you apply a proper decoding: Click here to view code image text = out.decode('utf-8') It is also possible to set up a pipe and to interact with a subprocess in a more detailed manner. To do that, use the Popen class like this: Click here to view code image import subprocess p = subprocess.Popen(['wc'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)","title":"Run the 'netstat -a' command and collect its output"},{"location":"python/modules/io/#send-data-to-the-subprocess","text":"p.stdin.write(b'hello world\\nthis is a test\\n') p.stdin.close()","title":"Send data to the subprocess"},{"location":"python/modules/io/#read-data-back","text":"out = p.stdout.read() print(out) tmpfile temp files textwrap wrapped = textwrap.wrap(text, width=81) print('\\n'.join(wrapped)) threading threads import threading import time def countdown(n): while n > 0: print('T-minus', n) n -= 1 time.sleep(1) t = threading.Thread(target=countdown, args=[10]) t.start() t.join() # Wait for the thread to finish If you\u2019re never going to wait for the thread to finish, make it daemonic by supplying an extra daemon flag like this: Click here to view code image t = threading.Thread(target=countdown, args=[10], daemon=True)","title":"Read data back"},{"location":"python/modules/io/#to-stop","text":"import threading import time must_stop = False def countdown(n): while n > 0 and not must_stop: print('T-minus', n) n -= 1 time.sleep(1) thread lock import threading class Counter: def init (self): self.value = 0 self.lock = threading.Lock() 1 2 3 4 5 6 7 def increment(self): with self.lock: self.value += 1 def decrement(self): with self.lock: self.value -= 1 threading event def step1(evt): print('Step 1') time.sleep(5) evt.set() def step2(evt): evt.wait() print('Step 2') evt = threading.Event() threading.Thread(target=step1, args=[evt]).start() threading.Thread(target=step2, args=[evt]).start() thread queue import threading import queue import time def producer(q): for i in range(10): print('Producing:', i) q.put(i) print('Done') q.put(None) def consumer(q): while True: item = q.get() if item is None: break print('Consuming:', item) print('Goodbye') q = queue.Queue() threading.Thread(target=producer, args=[q]).start() threading.Thread(target=consumer, args=[q]).start() time The time module is used to access system time-related functions. The following selected functions are the most useful: sleep(seconds) Make Python sleep for a given number of seconds, given as a floating point. time() Return the current system time in UTC as a floating-point number. This is the number of seconds since the epoch (usually January 1, 1970 for UNIX systems). Use localtime() to convert it into a data structure suitable for extracting useful information. localtime([secs]) Return a struct_time object representing the local time on the system or the time represented by the floating-point value secs passed as an argument. The resulting struct has attributes tm_year, tm_mon, tm_mday, tm_hour, tm_min, tm_sec, tm_wday, tm_yday, and tm_isdst. gmtime([secs]) The same as localtime() except that the resulting structure represents the time in UTC (or Greenwich Mean Time). ctime([secs]) Convert a time represented as seconds to a text string suitable for printing. Useful for debugging and logging. asctime(tm) Convert a time structure as represented by localtime() into a text string suitable for printing. The datetime module is more generally used for representing dates and times for the purpose of performing date-related computations and dealing with timezones. urllib from urllib.request import urlopen u = urlopen('http://www.python.org') data = u.read() If you want to encode form parameters, you can use urllib.parse.urlencode() as shown here: Click here to view code image from urllib.parse import urlencode from urllib.request import urlopen form = { 'name': 'Mary A. Python', 'email': 'mary123@python.org' } data = urlencode(form) u = urlopen('http://httpbin.org/post', data.encode('utf-8')) response = u.read() The urlopen() function works fine for basic webpages and APIs involving HTTP or HTTPS. However, it becomes quite awkward to use if access also involves cookies, advanced authentication schemes, and other layers. Frankly, most Python programmers would use a third-party library such as requests or httpx to handle these situations. You should too. The urllib.parse subpackage has additional functions for manipulating URLs themselves. For example, the urlparse() function can be used to pull apart a URL: Click here to view code image url = 'http://httpbin.org/get?name=Dave&n=42' from urllib.parse import urlparse urlparse(url) ParseResult(scheme='http', netloc='httpbin.org', path='/get', params='', query='name=Dave&n=42', fragment='') unicodedata for unicode strings unicodedata.normalize(option) xml from xml.etree.ElementTree import ElementTree doc = ElementTree(file='recipe.xml') title = doc.find('title') print(title.text)","title":"to stop"},{"location":"python/modules/io/#alternative-just-get-element-text","text":"print(doc.findtext('description'))","title":"Alternative (just get element text)"},{"location":"python/modules/io/#iterate-over-multiple-elements","text":"for item in doc.findall('ingredients/item'): num = item.get('num') units = item.get('units', '') text = item.text.strip() print(f'{num} {units} {text}') I/O is a fundamental part of writing any useful program. Given its popularity, Python is able to work with literally any data format, encoding, or document structure that\u2019s in use. Although the standard library might not support it, you will almost certainly find a third-party module to solve your problem. In the big picture, it may be more useful to think about the edges of your application. At the outer boundary between your program and reality, it\u2019s common to encounter issues related to data encoding. This is especially true for textual data and Unicode. Much of the complexity in Python\u2019s I/O handling\u2014supporting different encoding, error handling policies, and so on\u2014is aimed at this specific problem. It\u2019s also critical to keep in mind that textual data and binary data are strictly separated. Knowing what you\u2019re working with helps in understanding the big picture. A secondary consideration in I/O is the overall evaluation model. Python code is currently separated into two worlds\u2014normal synchronous code and asynchronous code usually associated with the asyncio module (characterized by the use of async functions and the async/await syntax). Asynchronous code almost always requires using dedicated libraries that are capable of operating in that environment. This, in turn, forces your hand on writing your application code in the \u201casync\u201d style as well. Honestly, you should probably avoid asynchronous coding unless you absolutely know that you need it\u2014and if you\u2019re not really sure, then you almost certainly don\u2019t. Most of the well-adjusted Python-speaking universe codes in a normal synchronous style that is far easier to reason about, debug, and test. You should choose that.","title":"Iterate over multiple elements"},{"location":"python/modules/modules/","text":"module d.py - is module Module caching import works only one time, but you can reload if needed u can import global variables with from the from d import GLOABL_VARIABLE control * import define all=[\"func\",\"SomeClass\"] circula import moda.py import modb def func_a(): modb.func_b() class Base: pass ---------------------------- modb.py import moda def func_b(): print('B') class Child(moda.Base): pass There is a strange import order dependency in this code. Using import modb first works fine, but if you put import moda first, it blows up with an error about moda.Base being undefined. To understand what is happening, you have to follow the control flow. import moda starts executing the file moda.py. The first statement it encounters is import modb. Thus, control switches over to modb.py. The first statement in that file is import moda. Instead of entering a recursive cycle, that import is satisfied by the module cache and control continues on to the next statement in modb.py. This is good\u2014circular imports don\u2019t cause Python to deadlock or enter a new spacetime dimension. However, at this point in execution, module moda has only been partially evaluated. When control reaches the class Child(moda.Base) statement, it blows up. The required Base class hasn\u2019t been defined yet. One way to fix this problem is to move the import modb statement someplace else. For example, you could move the import into func_a() where the definition is actually needed: module reloading importlib.realod(requests) no c/ c++ extenstions compilation pychache When modules are first imported, they are compiled into an interpreter bytecode. This code is written to a .pyc file within a special pycache directory. This directory is usually found in the same directory as the original .py file. When the same import occurs again on a different run of the program, the compiled bytecode is loaded instead. This significantly speeds up the import process. The caching of bytecode is an automatic process that you almost never need to worry about. Files are automatically regenerated if the original source code changes. It just works. That said, there are still reasons to know about this caching and compilation process. First, sometimes Python files get installed (often accidentally) in an environment where users don\u2019t have operating system permissions to create the required pycache directory. Python will still work, but every import now loads the original source code and compiles it to bytecode. Program loading will be a lot slower than it needs to be. Similarly, in deploying or packaging a Python application, it may be advantageous to include the compiled bytecode, as that may significantly speed up program startup. The other good reason to know about module caching is that some programming techniques interfere with it. Advanced metaprogramming techniques involving dynamic code generation and the exec() function defeat the benefits of bytecode caching. A notable example is the use of dataclasses: Click here to view code image from dataclasses import dataclass @dataclass class Point: x: float y: float Dataclasses work by generating method functions as text fragments and executing them using exec(). None of this generated code is cached by the import system. For a single class definition, you won\u2019t notice. However, if you have a module consisting of 100 dataclasses, you might find that it imports nearly 20 times slower than a comparable module where you just wrote out the classes in the normal, if less compact, way. module search env PYTHONPATH=/some/path python3 script.py import sys sys.path.append('mymodules.zip') import foo, bar python execute directory myapp/ foo.py bar.py main.py You can run Python on it by typing python3 myapp. Execution will start in the main.py file. This also works if you turn the myapp directory into a ZIP archive. Typing python3 myapp.zip will look for a top-level main.py file and execute it if found. package graphics/ init.py primitive/ init.py lines.py fill.py text.py ... graph2d/ init.py plot2d.py ... graph3d/ init.py plot3d.py ... formats/ init.py gif.py png.py tiff.py jpeg.py Whenever any part of a package is first imported, code in the init.py file executes first (if it exists). As noted, this file may be empty, but it can also contain code to perform package-specific initializations. If importing a deeply nested submodule, all init.py files encountered in traversal of the directory structure are executed. Thus, the statement import graphics.primitive.fill would first execute the init.py file in the graphics/ directory followed by the init.py file in the primitive/ directory. Astute Python users might observe that a package still seems to work if init.py files are omitted. This is true\u2014you can use a directory of Python code as a package even if it contains no init.py. However, what\u2019s not obvious is that a directory with a missing init.py file actually defines a different kind of package known as namespace package. This is an advanced feature sometimes used by very large libraries and frameworks to implement broken plugin systems. In the opinion of the author, this is rarely what you want\u2014you should always create proper init.py files when creating a package. runninc packgage submodule as script from ..primitive import lines, text class Plot2D: ... if name == ' main ': print('Testing Plot2D') p = Plot2D() ... If you try to run it directly, you get a crash complaining about relative import statements: Click here to view code image bash $ python3 graphics/graph2d/plot2d.py Traceback (most recent call last): File 'graphics/graph2d/plot2d.py', line 1, in from ..primitive import line, text ValueError: attempted relative import beyond top-level package bash $ You can\u2019t move into the package directory and run it there either: Click here to view code image bash $ cd graphics/graph2d/ bash $ python3 plot2d.py Traceback (most recent call last): File 'plot2d.py', line 1, in from ..primitive import line, text ValueError: attempted relative import beyond top-level package bash $ bash $ python3 -m graphics.graph2d.plot2d Testing Plot2D bash $ -m specifies a module or package as the main program. Python will run the module with the proper environment to make sure that imports work. Many of Python\u2019s built-in packages have \u201csecret\u201d features that can be used via -m. One of the most well-known is using python3 -m http.server to run a web server from the current directory. You can provide similar functionality with your own packages. If the name supplied to python -m name corresponds to a package directory, Python looks for the presence of a main.py in that directory and runs that as the scrip control package namespace The primary purpose of a package is to serve as a top-level container for code. Sometimes users will import the top-level name and nothing else. For example: import graphics This import doesn\u2019t specify any particular submodule. Nor does it make any other part of the package accessible. For example, you\u2019ll find that code like this fails: Click here to view code image import graphics graphics.primitive.fill.floodfill(img,x,y,color) # Fails! When only a top-level package import is given, the only file that imports is the associated init.py file. In this example, it\u2019s the file graphics/init.py file. The primary purpose of an init.py file is to build and/or manage the contents of the top-level package namespace. Often, this involves importing selected functions, classes, and other objects from lower-level submodules. For example, if the graphics package in this example consists of hundreds of low-level functions but most of those details are encapsulated into a handful of high-level classes, then the init.py file might choose to expose just those classes: Click here to view code image graphics/init.py from .graph2d.plot2d import Plot2D from .graph3d.plot3d import Plot3D With this init.py file, the names Plot2D and Plot3D would appear at the top level of the package. A user could then use those names as if graphics were a simple module: Click here to view code image from graphics import Plot2D plt = Plot2D(100, 100) plt.clear() ... This is often much more convenient for the user because they don\u2019t have to know how you\u2019ve actually organized your code. In some sense, you\u2019re putting a higher layer of abstraction on top of your code structure. Many of the modules in the Python standard library are constructed in this manner. For example, the popular collections module is actually a package. The collections/init.py file consolidates definitions from a few different places and presents them to the user as a single consolidated namespace. package exports One issue concerns the interaction between an init .py file and low-level submodules. For example, the user of a package might only want to concern themselves with objects and functions that live in the top-level package namespace. However, the implementor of a package might be concerned with the problem of organizing code into maintainable submodules. To better manage this organizational complexity, package submodules often declare an explicit list of exports by defining an all variable. This is a list of names that should be pushed up one level in the package namespace. For example: Click here to view code image graphics/graph2d/plot2d.py all = ['Plot2D'] class Plot2D: ... The associated init .py file then imports its submodules using an * import like this: Click here to view code image graphics/graph2d/ init .py Only loads names explicitly listed in all variables from .plot2d import * Propagate the all up to next level (if desired) all = plot2d. all This lifting process then continues all the way to the top-level package init .py. for example: Click here to view code image graphics/ init .py from .graph2d import * from .graph3d import * Consolidate exports all = [ graph2d. all , graph3d. all ] The gist is that every component of a package explicitly states its exports using the all variable. The init.py files then propagate the exports upwards. In practice, it can get complicated, but this approach avoids the problem of hard-wiring specific export names into the init.py file. Instead, if a submodule wants to export something, its name gets listed in just one place\u2014the all variable. Then, by magic, it propagates up to its proper place in the package namespace. It is worth noting that although using * imports in user code is frowned upon, it is widespread practice in package init.py files. The reason it works in packages is that it is usually much more controlled and contained\u2014being driven by the contents of the all variables and not a free-wheeling attitude of \u201clet\u2019s just import everything.\u201d module objects name Full module name doc Documentation string dict Module dictionary file Filename where defined package Name of enclosing package (if any) path List of subdirectories to search for submodules of a package. annotations Module-level type hints 8.16 Deploying Python Packages The final frontier of modules and packages is the problem of giving your code to others. This is a large topic that has been the focus of active ongoing development over many years. I won\u2019t try to document a process that\u2019s bound to be out-of-date by the time you read this. Instead, direct your attention to the documentation at https://packaging.python.org/tutorials/packaging-projects. For the purposes of day-to-day development, the most important thing is to keep your code isolated as a self-contained project. All of your code should live in a proper package. Try to give your package a unique name so that it doesn\u2019t conflict with other possible dependencies. Consult the Python package index at https://pypi.org to pick a name. In structuring your code, try to keep things simple. As you\u2019ve seen, there are many highly sophisticated things that can be done with the module and package system. There is a time and place for that, but it should not be your starting point. With absolute simplicity in mind, the most minimalistic way to distribute pure Python code is to use the setuptools module or the built-in distutils module. Suppose you have written some code and it\u2019s in a project that looks like this: Click here to view code image spam-project/ README.txt Documentation.txt spam/ # A package of code init.py foo.py bar.py runspam.py # A script to run as: python runspam.py To create a distribution, create a file setup.py in the topmost directory (spam-project/ in this example). In this file, put the following code: Click here to view code image setup.py from setuptools import setup setup(name = 'spam', version = '0.0' packages = ['spam'], scripts = ['runspam.py'], ) In the setup() call, packages is a list of all package directories, and scripts is a list of script files. Any of these arguments may be omitted if your software does not have them (for example, if there are no scripts). name is the name of your package, and version is the version number as a string. The call to setup() supports a variety of other parameters that supply various metadata about your package. See the full list at https://docs.python.org/3/distutils/apiref.html. Creating a setup.py file is enough to create a source distribution of your software. Type the following shell command to make a source distribution: Click here to view code image bash $ python setup.py sdist ... bash $ This creates an archive file, such as spam-1.0.tar.gz or spam-1.0.zip, in the directory spam/dist. This is the file you would give to others to install your software. To install, a user can use a command such as pip. For example: Click here to view code image shell $ python3 -m pip install spam-1.0.tar.gz This installs the software into the local Python distribution and makes it available for general use. The code will normally be installed into a directory called site-packages in the Python library. To find the exact location of this directory, inspect the value of sys.path. Scripts are normally installed into the same directory as the Python interpreter itself. If the first line of a script starts with #! and contains the text python, the installer will rewrite the line to point to the local installation of Python. Thus, if your scripts have been hardcoded to a specific Python location, such as /usr/local/bin/python, they should still work when installed on other systems where Python is in a different location. It must be stressed that the use of setuptools as described here is absolutely minimal. Larger projects may involve C/C++ extensions, complicated package structures, examples, and more. Covering all of the tools and possible ways to deploy such code is beyond the scope of this book. You should consult various resources on https://python.org and https://pypi.org for the most up-to-date advice. 8.17 The Penultimate Word: Start with a Package When first starting a new program, it is easy to start with a simple single Python file. For example, you might write a script called program.py and start with that. Although this will work fine for throwaway programs and short tasks, your \u201cscript\u201d may start growing and adding features. Eventually, you might want to split it into multiple files. It\u2019s at that point that problems often arise. In light of this, it makes sense to get in the habit of starting all programs as a package from the onset. For example, instead of making a file called program.py, you should make a program package directory called program: program/ init.py main.py Put your starting code in main.py and run your program using a command such as python -m program. As you need more code, add new files to your package and use package-relative imports. An advantage of using a package is that all of your code remains isolated. You can name the files whatever you want and not worry about collisions with other packages, standard library modules, or code written by your coworkers. Although setting up a package requires a bit more work at the start, it will likely save you a lot of headaches later. 8.18 The Final Word: Keep It Simple There is a lot of more advanced wizardry associated with the module and package system than what has been shown here. Consult the tutorial \u201cModules and Packages: Live and Let Die!\u201d at https://dabeaz.com/modulepackage/index.html to get an idea of what\u2019s possible. All things considered, however, you\u2019re probably better off not doing any advanced module hacking. Managing modules, packages, and software distribution has always been a source of pain in the Python community. Much of the pain is a direct consequence of people applying hacks to the module system. Don\u2019t do that. Keep it simple and find the power to just say \u201cno\u201d when your coworkers propose to modify import to work with the blockchain.","title":"Modules"},{"location":"python/modules/modules/#modapy","text":"import modb def func_a(): modb.func_b() class Base: pass","title":"moda.py"},{"location":"python/modules/modules/#-","text":"","title":"----------------------------"},{"location":"python/modules/modules/#modbpy","text":"import moda def func_b(): print('B') class Child(moda.Base): pass There is a strange import order dependency in this code. Using import modb first works fine, but if you put import moda first, it blows up with an error about moda.Base being undefined. To understand what is happening, you have to follow the control flow. import moda starts executing the file moda.py. The first statement it encounters is import modb. Thus, control switches over to modb.py. The first statement in that file is import moda. Instead of entering a recursive cycle, that import is satisfied by the module cache and control continues on to the next statement in modb.py. This is good\u2014circular imports don\u2019t cause Python to deadlock or enter a new spacetime dimension. However, at this point in execution, module moda has only been partially evaluated. When control reaches the class Child(moda.Base) statement, it blows up. The required Base class hasn\u2019t been defined yet. One way to fix this problem is to move the import modb statement someplace else. For example, you could move the import into func_a() where the definition is actually needed: module reloading importlib.realod(requests) no c/ c++ extenstions compilation pychache When modules are first imported, they are compiled into an interpreter bytecode. This code is written to a .pyc file within a special pycache directory. This directory is usually found in the same directory as the original .py file. When the same import occurs again on a different run of the program, the compiled bytecode is loaded instead. This significantly speeds up the import process. The caching of bytecode is an automatic process that you almost never need to worry about. Files are automatically regenerated if the original source code changes. It just works. That said, there are still reasons to know about this caching and compilation process. First, sometimes Python files get installed (often accidentally) in an environment where users don\u2019t have operating system permissions to create the required pycache directory. Python will still work, but every import now loads the original source code and compiles it to bytecode. Program loading will be a lot slower than it needs to be. Similarly, in deploying or packaging a Python application, it may be advantageous to include the compiled bytecode, as that may significantly speed up program startup. The other good reason to know about module caching is that some programming techniques interfere with it. Advanced metaprogramming techniques involving dynamic code generation and the exec() function defeat the benefits of bytecode caching. A notable example is the use of dataclasses: Click here to view code image from dataclasses import dataclass @dataclass class Point: x: float y: float Dataclasses work by generating method functions as text fragments and executing them using exec(). None of this generated code is cached by the import system. For a single class definition, you won\u2019t notice. However, if you have a module consisting of 100 dataclasses, you might find that it imports nearly 20 times slower than a comparable module where you just wrote out the classes in the normal, if less compact, way. module search env PYTHONPATH=/some/path python3 script.py import sys sys.path.append('mymodules.zip') import foo, bar python execute directory myapp/ foo.py bar.py main.py You can run Python on it by typing python3 myapp. Execution will start in the main.py file. This also works if you turn the myapp directory into a ZIP archive. Typing python3 myapp.zip will look for a top-level main.py file and execute it if found. package graphics/ init.py primitive/ init.py lines.py fill.py text.py ... graph2d/ init.py plot2d.py ... graph3d/ init.py plot3d.py ... formats/ init.py gif.py png.py tiff.py jpeg.py Whenever any part of a package is first imported, code in the init.py file executes first (if it exists). As noted, this file may be empty, but it can also contain code to perform package-specific initializations. If importing a deeply nested submodule, all init.py files encountered in traversal of the directory structure are executed. Thus, the statement import graphics.primitive.fill would first execute the init.py file in the graphics/ directory followed by the init.py file in the primitive/ directory. Astute Python users might observe that a package still seems to work if init.py files are omitted. This is true\u2014you can use a directory of Python code as a package even if it contains no init.py. However, what\u2019s not obvious is that a directory with a missing init.py file actually defines a different kind of package known as namespace package. This is an advanced feature sometimes used by very large libraries and frameworks to implement broken plugin systems. In the opinion of the author, this is rarely what you want\u2014you should always create proper init.py files when creating a package. runninc packgage submodule as script from ..primitive import lines, text class Plot2D: ... if name == ' main ': print('Testing Plot2D') p = Plot2D() ... If you try to run it directly, you get a crash complaining about relative import statements: Click here to view code image bash $ python3 graphics/graph2d/plot2d.py Traceback (most recent call last): File 'graphics/graph2d/plot2d.py', line 1, in from ..primitive import line, text ValueError: attempted relative import beyond top-level package bash $ You can\u2019t move into the package directory and run it there either: Click here to view code image bash $ cd graphics/graph2d/ bash $ python3 plot2d.py Traceback (most recent call last): File 'plot2d.py', line 1, in from ..primitive import line, text ValueError: attempted relative import beyond top-level package bash $ bash $ python3 -m graphics.graph2d.plot2d Testing Plot2D bash $ -m specifies a module or package as the main program. Python will run the module with the proper environment to make sure that imports work. Many of Python\u2019s built-in packages have \u201csecret\u201d features that can be used via -m. One of the most well-known is using python3 -m http.server to run a web server from the current directory. You can provide similar functionality with your own packages. If the name supplied to python -m name corresponds to a package directory, Python looks for the presence of a main.py in that directory and runs that as the scrip control package namespace The primary purpose of a package is to serve as a top-level container for code. Sometimes users will import the top-level name and nothing else. For example: import graphics This import doesn\u2019t specify any particular submodule. Nor does it make any other part of the package accessible. For example, you\u2019ll find that code like this fails: Click here to view code image import graphics graphics.primitive.fill.floodfill(img,x,y,color) # Fails! When only a top-level package import is given, the only file that imports is the associated init.py file. In this example, it\u2019s the file graphics/init.py file. The primary purpose of an init.py file is to build and/or manage the contents of the top-level package namespace. Often, this involves importing selected functions, classes, and other objects from lower-level submodules. For example, if the graphics package in this example consists of hundreds of low-level functions but most of those details are encapsulated into a handful of high-level classes, then the init.py file might choose to expose just those classes: Click here to view code image graphics/init.py from .graph2d.plot2d import Plot2D from .graph3d.plot3d import Plot3D With this init.py file, the names Plot2D and Plot3D would appear at the top level of the package. A user could then use those names as if graphics were a simple module: Click here to view code image from graphics import Plot2D plt = Plot2D(100, 100) plt.clear() ... This is often much more convenient for the user because they don\u2019t have to know how you\u2019ve actually organized your code. In some sense, you\u2019re putting a higher layer of abstraction on top of your code structure. Many of the modules in the Python standard library are constructed in this manner. For example, the popular collections module is actually a package. The collections/init.py file consolidates definitions from a few different places and presents them to the user as a single consolidated namespace. package exports One issue concerns the interaction between an init .py file and low-level submodules. For example, the user of a package might only want to concern themselves with objects and functions that live in the top-level package namespace. However, the implementor of a package might be concerned with the problem of organizing code into maintainable submodules. To better manage this organizational complexity, package submodules often declare an explicit list of exports by defining an all variable. This is a list of names that should be pushed up one level in the package namespace. For example: Click here to view code image","title":"modb.py"},{"location":"python/modules/modules/#graphicsgraph2dplot2dpy","text":"all = ['Plot2D'] class Plot2D: ... The associated init .py file then imports its submodules using an * import like this: Click here to view code image","title":"graphics/graph2d/plot2d.py"},{"location":"python/modules/modules/#graphicsgraph2dinitpy","text":"","title":"graphics/graph2d/init.py"},{"location":"python/modules/modules/#only-loads-names-explicitly-listed-in-all-variables","text":"from .plot2d import *","title":"Only loads names explicitly listed in all variables"},{"location":"python/modules/modules/#propagate-the-all-up-to-next-level-if-desired","text":"all = plot2d. all This lifting process then continues all the way to the top-level package init .py. for example: Click here to view code image","title":"Propagate the all up to next level (if desired)"},{"location":"python/modules/modules/#graphicsinitpy","text":"from .graph2d import * from .graph3d import *","title":"graphics/init.py"},{"location":"python/modules/modules/#consolidate-exports","text":"all = [ graph2d. all , graph3d. all ] The gist is that every component of a package explicitly states its exports using the all variable. The init.py files then propagate the exports upwards. In practice, it can get complicated, but this approach avoids the problem of hard-wiring specific export names into the init.py file. Instead, if a submodule wants to export something, its name gets listed in just one place\u2014the all variable. Then, by magic, it propagates up to its proper place in the package namespace. It is worth noting that although using * imports in user code is frowned upon, it is widespread practice in package init.py files. The reason it works in packages is that it is usually much more controlled and contained\u2014being driven by the contents of the all variables and not a free-wheeling attitude of \u201clet\u2019s just import everything.\u201d module objects name Full module name doc Documentation string dict Module dictionary file Filename where defined package Name of enclosing package (if any) path List of subdirectories to search for submodules of a package. annotations Module-level type hints 8.16 Deploying Python Packages The final frontier of modules and packages is the problem of giving your code to others. This is a large topic that has been the focus of active ongoing development over many years. I won\u2019t try to document a process that\u2019s bound to be out-of-date by the time you read this. Instead, direct your attention to the documentation at https://packaging.python.org/tutorials/packaging-projects. For the purposes of day-to-day development, the most important thing is to keep your code isolated as a self-contained project. All of your code should live in a proper package. Try to give your package a unique name so that it doesn\u2019t conflict with other possible dependencies. Consult the Python package index at https://pypi.org to pick a name. In structuring your code, try to keep things simple. As you\u2019ve seen, there are many highly sophisticated things that can be done with the module and package system. There is a time and place for that, but it should not be your starting point. With absolute simplicity in mind, the most minimalistic way to distribute pure Python code is to use the setuptools module or the built-in distutils module. Suppose you have written some code and it\u2019s in a project that looks like this: Click here to view code image spam-project/ README.txt Documentation.txt spam/ # A package of code init.py foo.py bar.py runspam.py # A script to run as: python runspam.py To create a distribution, create a file setup.py in the topmost directory (spam-project/ in this example). In this file, put the following code: Click here to view code image setup.py from setuptools import setup setup(name = 'spam', version = '0.0' packages = ['spam'], scripts = ['runspam.py'], ) In the setup() call, packages is a list of all package directories, and scripts is a list of script files. Any of these arguments may be omitted if your software does not have them (for example, if there are no scripts). name is the name of your package, and version is the version number as a string. The call to setup() supports a variety of other parameters that supply various metadata about your package. See the full list at https://docs.python.org/3/distutils/apiref.html. Creating a setup.py file is enough to create a source distribution of your software. Type the following shell command to make a source distribution: Click here to view code image bash $ python setup.py sdist ... bash $ This creates an archive file, such as spam-1.0.tar.gz or spam-1.0.zip, in the directory spam/dist. This is the file you would give to others to install your software. To install, a user can use a command such as pip. For example: Click here to view code image shell $ python3 -m pip install spam-1.0.tar.gz This installs the software into the local Python distribution and makes it available for general use. The code will normally be installed into a directory called site-packages in the Python library. To find the exact location of this directory, inspect the value of sys.path. Scripts are normally installed into the same directory as the Python interpreter itself. If the first line of a script starts with #! and contains the text python, the installer will rewrite the line to point to the local installation of Python. Thus, if your scripts have been hardcoded to a specific Python location, such as /usr/local/bin/python, they should still work when installed on other systems where Python is in a different location. It must be stressed that the use of setuptools as described here is absolutely minimal. Larger projects may involve C/C++ extensions, complicated package structures, examples, and more. Covering all of the tools and possible ways to deploy such code is beyond the scope of this book. You should consult various resources on https://python.org and https://pypi.org for the most up-to-date advice. 8.17 The Penultimate Word: Start with a Package When first starting a new program, it is easy to start with a simple single Python file. For example, you might write a script called program.py and start with that. Although this will work fine for throwaway programs and short tasks, your \u201cscript\u201d may start growing and adding features. Eventually, you might want to split it into multiple files. It\u2019s at that point that problems often arise. In light of this, it makes sense to get in the habit of starting all programs as a package from the onset. For example, instead of making a file called program.py, you should make a program package directory called program: program/ init.py main.py Put your starting code in main.py and run your program using a command such as python -m program. As you need more code, add new files to your package and use package-relative imports. An advantage of using a package is that all of your code remains isolated. You can name the files whatever you want and not worry about collisions with other packages, standard library modules, or code written by your coworkers. Although setting up a package requires a bit more work at the start, it will likely save you a lot of headaches later. 8.18 The Final Word: Keep It Simple There is a lot of more advanced wizardry associated with the module and package system than what has been shown here. Consult the tutorial \u201cModules and Packages: Live and Let Die!\u201d at https://dabeaz.com/modulepackage/index.html to get an idea of what\u2019s possible. All things considered, however, you\u2019re probably better off not doing any advanced module hacking. Managing modules, packages, and software distribution has always been a source of pain in the Python community. Much of the pain is a direct consequence of people applying hacks to the module system. Don\u2019t do that. Keep it simple and find the power to just say \u201cno\u201d when your coworkers propose to modify import to work with the blockchain.","title":"Consolidate exports"},{"location":"python/structure/","text":"What is an Object Every piece of data stored in a program is an object. Each object has an identity, a type (also known as its class), and a value. For example, when you write a = 42 , an integer object is created with the value of 42. The identity of the object is a number representing its location in memory, and a is a label that refers to this specific location. The type of an object defines its internal data representation and supported methods. An object can be mutable or immutable, and it can hold references to other objects. Objects are characterized by their attributes, which are accessed using the dot operator ( . ). An attribute can be a simple data value or a function called a method. Inheritance allows the creation of subtype objects that inherit features from the original type and can have additional or redefined methods. Type checks in a program may not always be useful due to performance impact and complex object hierarchies. For example, the isinstance(items, list) statement may not work for objects that have a list-like interface but don't directly inherit from the built-in list type. First-Class Object All objects in Python are considered first-class objects. This means they can be assigned to names, stored as variables, passed as arguments, returned from functions, compared with other objects, and more. They can be treated as data and manipulated in various ways. Reference Counting and Garbage Collection Python manages objects through automatic garbage collection. Objects are reference-counted, meaning their reference count increases when they are assigned to names or placed in data structures. The reference count decreases when references go out of scope, are reassigned, or deleted. When an object's reference count reaches zero, it is garbage-collected. In some cases, circular dependencies among objects can lead to delayed destruction. The cyclic garbage collector detects and deletes these inaccessible objects periodically. Manual deletion of objects may be necessary in certain situations, and the gc module provides functions to control the garbage collection process. Object Protocol Python's behavior is determined by dynamic processes involving special methods known as \"magic\" methods. These methods are automatically triggered by the interpreter during program execution. Special methods are denoted by double underscores ( __ ) before and after the method name. Different categories of objects have associated special methods called \"protocols.\" For example, container objects define methods like __len__() , __getitem__() , __setitem__() , and __delitem__() to implement container operations such as indexing and slicing. Iterators implement the __iter__() and __next__() methods to enable iteration. Other protocols include class attribute protocol, function protocol, context manager protocol, repr and doc protocol, and spread with * . Container Protocols Container objects implement various special methods to support container operations: 1 2 3 4 5 6 a = [ 1 , 2 , 3 , 4 , 5 , 6 ] len ( a ) # a.__len__() x = a [ 2 ] # x = a.__getitem__(2) a [ 1 ] = 7 # a.__setitem__(1,7) del a [ 2 ] # a.__delitem__(2) 5 in a # a.__contains__(5) Slicing operations are implemented using __getitem__() , __setitem__() , and __delitem__() methods. Slices are represented by special slice instances. Iterator Protocol Objects that support iteration implement the iterator protocol: 1 2 obj = iter ( iterable ) # obj = iterable.__iter__() next ( obj ) # obj.__next__() The iter() method returns an iterator object, which has a __next__() method to retrieve the next object in the iteration. The for statement implicitly performs iteration using these methods. Class Attribute Protocol Objects define class attribute methods for accessing, setting, and deleting attributes: 1 2 3 4 obj . __getattribute__ ( self , name ) # Returns the attribute self.name obj . __getattr__ ( self , name ) # Returns the attribute self.name (if not found through __getattribute__()) obj . __setattr__ ( self , name , value ) # Sets the attribute self.name = value obj . __delattr__ ( self , name ) # Deletes the attribute self.name Function Protocol Objects can emulate functions by implementing the __call__() method. When an object provides this method, it can be invoked like a function: 1 obj ( arg1 , arg2 , ... ) # obj.__call__(arg1, arg2, ...) Many built-in types and libraries support function calls by implementing __call__() . Context Manager Protocol Context managers define the methods __enter__() and __exit__() (or __aenter__() and __aexit__ for async context managers). These methods are used for resource management and provide a convenient way to set up and clean up resources within a block of code. Repr and Doc Objects can define the __repr__() method to control how they are represented when using print() or str() . The __doc__ attribute stores docstrings associated with the object. Spread with * The * operator can be used to pass sequences or mappings as arguments to functions: 1 2 3 4 5 6 7 8 def func ( x , y , z ): ... s = ( 1 , 2 , 3 ) result = func ( * s ) # Pass a sequence as arguments d = { 'x' : 1 , 'y' : 2 , 'z' : 3 } result = func ( ** d ) # Pass a mapping as keyword arguments","title":"Index"},{"location":"python/structure/#what-is-an-object","text":"Every piece of data stored in a program is an object. Each object has an identity, a type (also known as its class), and a value. For example, when you write a = 42 , an integer object is created with the value of 42. The identity of the object is a number representing its location in memory, and a is a label that refers to this specific location. The type of an object defines its internal data representation and supported methods. An object can be mutable or immutable, and it can hold references to other objects. Objects are characterized by their attributes, which are accessed using the dot operator ( . ). An attribute can be a simple data value or a function called a method. Inheritance allows the creation of subtype objects that inherit features from the original type and can have additional or redefined methods. Type checks in a program may not always be useful due to performance impact and complex object hierarchies. For example, the isinstance(items, list) statement may not work for objects that have a list-like interface but don't directly inherit from the built-in list type.","title":"What is an Object"},{"location":"python/structure/#first-class-object","text":"All objects in Python are considered first-class objects. This means they can be assigned to names, stored as variables, passed as arguments, returned from functions, compared with other objects, and more. They can be treated as data and manipulated in various ways.","title":"First-Class Object"},{"location":"python/structure/#reference-counting-and-garbage-collection","text":"Python manages objects through automatic garbage collection. Objects are reference-counted, meaning their reference count increases when they are assigned to names or placed in data structures. The reference count decreases when references go out of scope, are reassigned, or deleted. When an object's reference count reaches zero, it is garbage-collected. In some cases, circular dependencies among objects can lead to delayed destruction. The cyclic garbage collector detects and deletes these inaccessible objects periodically. Manual deletion of objects may be necessary in certain situations, and the gc module provides functions to control the garbage collection process.","title":"Reference Counting and Garbage Collection"},{"location":"python/structure/#object-protocol","text":"Python's behavior is determined by dynamic processes involving special methods known as \"magic\" methods. These methods are automatically triggered by the interpreter during program execution. Special methods are denoted by double underscores ( __ ) before and after the method name. Different categories of objects have associated special methods called \"protocols.\" For example, container objects define methods like __len__() , __getitem__() , __setitem__() , and __delitem__() to implement container operations such as indexing and slicing. Iterators implement the __iter__() and __next__() methods to enable iteration. Other protocols include class attribute protocol, function protocol, context manager protocol, repr and doc protocol, and spread with * .","title":"Object Protocol"},{"location":"python/structure/#container-protocols","text":"Container objects implement various special methods to support container operations: 1 2 3 4 5 6 a = [ 1 , 2 , 3 , 4 , 5 , 6 ] len ( a ) # a.__len__() x = a [ 2 ] # x = a.__getitem__(2) a [ 1 ] = 7 # a.__setitem__(1,7) del a [ 2 ] # a.__delitem__(2) 5 in a # a.__contains__(5) Slicing operations are implemented using __getitem__() , __setitem__() , and __delitem__() methods. Slices are represented by special slice instances.","title":"Container Protocols"},{"location":"python/structure/#iterator-protocol","text":"Objects that support iteration implement the iterator protocol: 1 2 obj = iter ( iterable ) # obj = iterable.__iter__() next ( obj ) # obj.__next__() The iter() method returns an iterator object, which has a __next__() method to retrieve the next object in the iteration. The for statement implicitly performs iteration using these methods.","title":"Iterator Protocol"},{"location":"python/structure/#class-attribute-protocol","text":"Objects define class attribute methods for accessing, setting, and deleting attributes: 1 2 3 4 obj . __getattribute__ ( self , name ) # Returns the attribute self.name obj . __getattr__ ( self , name ) # Returns the attribute self.name (if not found through __getattribute__()) obj . __setattr__ ( self , name , value ) # Sets the attribute self.name = value obj . __delattr__ ( self , name ) # Deletes the attribute self.name","title":"Class Attribute Protocol"},{"location":"python/structure/#function-protocol","text":"Objects can emulate functions by implementing the __call__() method. When an object provides this method, it can be invoked like a function: 1 obj ( arg1 , arg2 , ... ) # obj.__call__(arg1, arg2, ...) Many built-in types and libraries support function calls by implementing __call__() .","title":"Function Protocol"},{"location":"python/structure/#context-manager-protocol","text":"Context managers define the methods __enter__() and __exit__() (or __aenter__() and __aexit__ for async context managers). These methods are used for resource management and provide a convenient way to set up and clean up resources within a block of code.","title":"Context Manager Protocol"},{"location":"python/structure/#repr-and-doc","text":"Objects can define the __repr__() method to control how they are represented when using print() or str() . The __doc__ attribute stores docstrings associated with the object.","title":"Repr and Doc"},{"location":"python/structure/#spread-with","text":"The * operator can be used to pass sequences or mappings as arguments to functions: 1 2 3 4 5 6 7 8 def func ( x , y , z ): ... s = ( 1 , 2 , 3 ) result = func ( * s ) # Pass a sequence as arguments d = { 'x' : 1 , 'y' : 2 , 'z' : 3 } result = func ( ** d ) # Pass a mapping as keyword arguments","title":"Spread with *"},{"location":"python/structure/code_design/","text":"Code Design Design by Contract Design by Contract is a programming approach that focuses on enforcing rules and constraints during the communication of software components. It involves the use of contracts that define preconditions, postconditions, invariants, and side effects. Preconditions: Checks performed before running a function to ensure that the requirements are met. Postconditions: Checks performed after the execution of a function to validate if the expected result is achieved. Invariants: Rules or constraints that remain true throughout the execution of the code. Side Effects: Mentioned in the code, they describe any changes or actions that occur beyond the return value of a function. Defensive Programming Defensive programming involves writing code that protects itself from invalid inputs or unexpected behavior. It includes error handling techniques such as: Value substitution: Using default values or environment variables ( os.getenv(\"DPORT\", 5432) ). Error logging: Capturing and logging errors for debugging and analysis. Exception handling: Properly handling exceptions with well-defined scopes to reduce the impact of errors. Best practices for error handling include avoiding traceback to end users, avoiding empty except blocks, and including the original exception for better debugging. Cohesion and Coupling Cohesion and coupling are concepts related to how objects or components in a codebase depend on each other. Cohesion: Describes the degree to which a component or class focuses on a single responsibility or functionality. High cohesion means that a component is focused and has a clear purpose. Coupling: Refers to the interdependence between components or classes. High coupling indicates tight dependencies, which can lead to issues such as limited code reuse, ripple effects of changes, and a low level of abstraction. DRY and OAOO DRY (Don't Repeat Yourself) and OAOO (Once and Only Once) are principles that promote code efficiency and maintainability. DRY: Encourages avoiding code duplication by abstracting common functionality into reusable components or functions. OAOO: Advocates for implementing a particular behavior or logic in a single place to ensure consistency and reduce the chance of introducing errors through duplicated code. YAGNI and KIS YAGNI: Stands for \"You Ain't Gonna Need It.\" It advises developers to avoid over-engineering or adding unnecessary features to their codebase. Only implement what is needed at the present moment to avoid complexity and potential issues. KIS: Stands for \"Keep It Simple.\" It emphasizes simplicity in design and implementation. When designing a software component, aim for the minimal solution that effectively solves the problem without introducing unnecessary complexity. EAFP and LBYL EAFP: Stands for \"Easier to Ask Forgiveness than Permission.\" This programming approach suggests trying an operation and handling any resulting exceptions rather than checking for preconditions or permissions before executing the operation. LBYL: Stands for \"Look Before You Leap.\" It involves checking preconditions or permissions before executing an operation to avoid exceptions or errors. An example is checking if a file exists before attempting to open it. Example: EAFP 1 2 3 4 5 try : with open ( filename ) as f : # Code for file processing except FileNotFoundError as e : logger . error ( e ) LBYL 1 2 3 if os . path . exists ( filename ): with open ( filename ) as f : # Code for file processing","title":"Code Design"},{"location":"python/structure/code_design/#code-design","text":"","title":"Code Design"},{"location":"python/structure/code_design/#design-by-contract","text":"Design by Contract is a programming approach that focuses on enforcing rules and constraints during the communication of software components. It involves the use of contracts that define preconditions, postconditions, invariants, and side effects. Preconditions: Checks performed before running a function to ensure that the requirements are met. Postconditions: Checks performed after the execution of a function to validate if the expected result is achieved. Invariants: Rules or constraints that remain true throughout the execution of the code. Side Effects: Mentioned in the code, they describe any changes or actions that occur beyond the return value of a function.","title":"Design by Contract"},{"location":"python/structure/code_design/#defensive-programming","text":"Defensive programming involves writing code that protects itself from invalid inputs or unexpected behavior. It includes error handling techniques such as: Value substitution: Using default values or environment variables ( os.getenv(\"DPORT\", 5432) ). Error logging: Capturing and logging errors for debugging and analysis. Exception handling: Properly handling exceptions with well-defined scopes to reduce the impact of errors. Best practices for error handling include avoiding traceback to end users, avoiding empty except blocks, and including the original exception for better debugging.","title":"Defensive Programming"},{"location":"python/structure/code_design/#cohesion-and-coupling","text":"Cohesion and coupling are concepts related to how objects or components in a codebase depend on each other. Cohesion: Describes the degree to which a component or class focuses on a single responsibility or functionality. High cohesion means that a component is focused and has a clear purpose. Coupling: Refers to the interdependence between components or classes. High coupling indicates tight dependencies, which can lead to issues such as limited code reuse, ripple effects of changes, and a low level of abstraction.","title":"Cohesion and Coupling"},{"location":"python/structure/code_design/#dry-and-oaoo","text":"DRY (Don't Repeat Yourself) and OAOO (Once and Only Once) are principles that promote code efficiency and maintainability. DRY: Encourages avoiding code duplication by abstracting common functionality into reusable components or functions. OAOO: Advocates for implementing a particular behavior or logic in a single place to ensure consistency and reduce the chance of introducing errors through duplicated code.","title":"DRY and OAOO"},{"location":"python/structure/code_design/#yagni-and-kis","text":"YAGNI: Stands for \"You Ain't Gonna Need It.\" It advises developers to avoid over-engineering or adding unnecessary features to their codebase. Only implement what is needed at the present moment to avoid complexity and potential issues. KIS: Stands for \"Keep It Simple.\" It emphasizes simplicity in design and implementation. When designing a software component, aim for the minimal solution that effectively solves the problem without introducing unnecessary complexity.","title":"YAGNI and KIS"},{"location":"python/structure/code_design/#eafp-and-lbyl","text":"EAFP: Stands for \"Easier to Ask Forgiveness than Permission.\" This programming approach suggests trying an operation and handling any resulting exceptions rather than checking for preconditions or permissions before executing the operation. LBYL: Stands for \"Look Before You Leap.\" It involves checking preconditions or permissions before executing an operation to avoid exceptions or errors. An example is checking if a file exists before attempting to open it. Example:","title":"EAFP and LBYL"},{"location":"python/structure/code_design/#eafp","text":"1 2 3 4 5 try : with open ( filename ) as f : # Code for file processing except FileNotFoundError as e : logger . error ( e )","title":"EAFP"},{"location":"python/structure/code_design/#lbyl","text":"1 2 3 if os . path . exists ( filename ): with open ( filename ) as f : # Code for file processing","title":"LBYL"},{"location":"python/structure/dessign_patterns/","text":"","title":"Design Patterns"},{"location":"python/structure/practices/","text":"BAD try to not use global statement.","title":"Practices"},{"location":"python/structure/practices/#bad","text":"try to not use global statement.","title":"BAD"},{"location":"python/structure/solid/","text":"","title":"SOLID"},{"location":"python/structure/structure/","text":"Memorize Some Tips Literals Literals are used to represent fixed values in Python. Here are some examples: Integer literals: 42 , 0b101010 (binary), 0o52 (octal), 0x2a (hexadecimal) Numeric literals can also include underscores for readability: 123_456_789 , 0x1234_5678 , 0b111_00_101 , 123.789_012 Operations for Iterables Iteration: for vars in s: Variable unpacking: v1, v2, ... = s Membership: x in s , x not in s Expansion in list, tuple, or set literals: [a, *s, b] , (a, *s, b) , {a, *s, b} Throw variable: throw variable like that Example: a,_,b=[1,2,3] Set Operations Set operations allow manipulating sets in Python: 1 2 3 4 5 6 7 8 names1 = { 'IBM' , 'MSFT' , 'AA' } names2 = set ([ 'IBM' , 'MSFT' , 'HPE' , 'IBM' , 'CAT' ]) a = names1 | names2 # Union: {'IBM', 'MSFT', 'HPE', 'AA', 'CAT'} b = names1 & names2 # Intersection: {'IBM', 'MSFT'} c = names1 - names2 # Difference: {'AA'} d = names2 - names1 # Difference: {'HPE', 'CAT'} e = names1 ^ names2 # Symmetric Difference: {'HPE', 'AA', 'CAT'} Discard() The discard() method is used to remove an item from a set: 1 s . discard ( 'SCOX' ) # Remove 'SCOX' if it exists. Dictionary Operations Dictionaries offer various operations for manipulating key-value pairs: Get with default value: p = prices.get('IBM', 0.0) Delete: del prices['GOOG'] Keys can be tuples: prices[('IBM', '2015-02-03')] = 91.23 List Comprehension List comprehension provides a concise way to create lists based on existing lists or other iterables: 1 2 3 4 [ expression for item1 in iterable1 if condition1 for item2 in iterable2 if condition2 ... for itemN in iterableN if conditionN ] This syntax is equivalent to the following code: 1 2 3 4 5 6 7 8 9 result = [] for item1 in iterable1 : if condition1 : for item2 in iterable2 : if condition2 : ... for itemN in iterableN : if conditionN : result . append ( expression ) Generator Expression Generator expressions are used to create generator objects, which generate values on the fly without storing them in memory: 1 2 3 4 5 6 7 8 9 nums = [ 1 , 2 , 3 , 4 ] squares = ( x * x for x in nums ) >>> squares < generator object at 0x590a8 > >>> next ( squares ) 1 >>> next ( squares ) 4 Python Enumerate The enumerate() function is used to iterate over a sequence while keeping track of the index: 1 2 for i , x in enumerate ( s , start = 100 ): statements Zip The zip() function is used to iterate over multiple sequences simultaneously: 1 2 for x , y in zip ( s , t ): statements The zip() function returns an iterable of tuples. Exception Base Roots BaseException : The root class for all exceptions. Exception : Base class for all program-related errors. ArithmeticError : Base class for all math-related errors. ImportError : Base class for import-related errors. LookupError : Base class for all container lookup errors. OSError : Base class for all system-related errors. IOError and EnvironmentError are aliases. ValueError : Base class for value-related errors, including Unicode-related errors. UnicodeError : Base class for Unicode string encoding-related errors. AssertionError : Raised when an assert statement fails. AttributeError : Raised when a bad attribute lookup is performed on an object. EOFError : Raised when the end of a file is reached. MemoryError : Raised when a recoverable out-of-memory error occurs. NameError : Raised when a name is not found in the local or global namespace. NotImplementedError : Raised for an unimplemented feature. RuntimeError : A generic \"something bad happened\" error. TypeError : Raised when an operation is applied to an object of the wrong type. UnboundLocalError : Raised when a local variable is used before a value is assigned. SystemExit : Raised to indicate program exit. KeyboardInterrupt : Raised when a program is interrupted via Control-C. StopIteration : Raised to signal the end of iteration. New Exception You can create your own custom exceptions by defining a new class that inherits from the Exception class. Here's an example: 1 2 3 4 class NetworkError ( Exception ): pass raise NetworkError ( 'Cannot find host' ) Chained Exception You can raise a different exception while preserving the original exception using the from keyword. Here's an example: 1 2 3 4 try : # Some code that may raise an exception except Exception as e : raise ValueError ( 'An error occurred' ) from e This creates a new ValueError exception with the original exception e chained to it. Exception Name Description BaseException The root class for all exceptions. Exception Base class for all program-related errors. ArithmeticError Base class for all math-related errors. ImportError Base class for import-related errors. LookupError Base class for all container lookup errors. OSError Base class for all system-related errors. ValueError Base class for value-related errors. UnicodeError Base class for Unicode string encoding errors. AssertionError Raised when an assert statement fails. AttributeError Raised when a bad attribute lookup is performed. EOFError Raised when the end of a file is reached. MemoryError Raised when a recoverable out-of-memory error occurs. NameError Raised when a name is not found in the local or global namespace. NotImplementedError Raised for an unimplemented feature. RuntimeError A generic \"something bad happened\" error. TypeError Raised when an operation is applied to an object of the wrong type. UnboundLocalError Raised when a local variable is used before a value is assigned. SystemExit Raised to indicate program exit. KeyboardInterrupt Raised when a program is interrupted via Control-C. StopIteration Raised to signal the end of iteration. 1 2 3 4 5 6 7 class ApplicationError ( Exception ): pass def do_something (): x = int ( 'N/A' ) # raises ValueError def spam (): try : do_something () except Exception as e : raise ApplicationError ( 'It failed' ) from e ## Exception handling advice e.args The tuple of arguments supplied when raising the exception. In most cases, this is a one-item tuple with a string describing the error. For OSError exceptions, the value is a 2-tuple or 3-tuple containing an integer error number, string error message, and an optional filename. e. cause Previous exception if the exception was intentionally raised in response to handling another exception. See the later section on chained exceptions. e. context Previous exception if the exception was raised while handling another exception. e. traceback Stack traceback object associated with the exception. 1 2 3 4 try : # do something except ( TypeError , ValueError ) as e : # Handle Type or Value errors 1 2 3 4 5 6 7 8 try : file = open ( 'foo.txt' , 'rt' ) except FileNotFoundError as e : print ( f 'Unable to open foo: { e } ' ) data = '' else : data = file . read () file . close () 1 2 3 4 5 6 file = open ( 'foo.txt' , 'rt' ) try : # Do some stuff ... finally : file . close () Exception handling is one of the most difficult things to get right in larger programs. However, there are a few rules of thumb that make it easier. The first rule is to not catch exceptions that can\u2019t be handled at that specific location in the code. Consider a function like this: 1 2 3 4 5 6 7 def read_data ( filename ): with open ( filename , 'rt' ) as file : rows = [] for line in file : row = line . split () rows . append (( row [ 0 ], int ( row [ 1 ]), float ( row [ 2 ]))) return rows Suppose the open() function fails due to a bad filename. Is this an error that should be caught with a try-except statement in this function? Probably not. If the caller gives a bad filename, there is no sensible way to recover. There is no file to open, no data to read, and nothing else that\u2019s possible. It\u2019s better to let the operation fail and report an exception back to the caller. Avoiding an error check in read_data() doesn\u2019t mean that the exception would never be handled anywhere\u2014it just means that it\u2019s not the role of read_data() to do it. Perhaps the code that prompted a user for a filename would handle this exception. This advice might seem contrary to the experience of programmers accustomed to languages that rely upon special error codes or wrapped result types. In those languages, great care is made to make sure you always check return codes for errors on all operations. You don\u2019t do this in Python. If an operation can fail and there\u2019s nothing you can do to recover, it\u2019s better to just let it fail. The exception will propagate to upper levels of the program where it is usually the responsibility of some other code to handle it. On the other hand, a function might be able to recover from bad data. For example: 1 2 3 4 5 6 7 8 9 10 11 def read_data ( filename ): with open ( filename , 'rt' ) as file : rows = [] for line in file : row = line . split () try : rows . append (( row [ 0 ], int ( row [ 1 ]), float ( row [ 2 ]))) except ValueError as e : print ( 'Bad row:' , row ) print ( 'Reason:' , e ) return rows When catching errors, try to make your except clauses as narrow as reasonable. The above code could have been written to catch all errors by using except Exception . However, doing that would make the code catch legitimate programming errors that probably shouldn\u2019t be ignored. Don\u2019t do that\u2014it will make debugging difficult. Finally, if you\u2019re explicitly raising an exception, consider making your own exception types. For example: 1 2 3 4 5 6 7 # Code Termination # exit code # can be used instead of exit() raise SystemExit () # Exit with no error message raise SystemExit ( \"Something is wrong\" ) # Exit with error Exception Hierarchy BaseException SystemExit KeyboardInterrupt GeneratorExit Exception StopIteration StopAsyncIteration ArithmeticError FloatingPointError OverflowError ZeroDivisionError AssertionError AttributeError BufferError EOFError ImportError ModuleNotFoundError LookupError IndexError KeyError MemoryError NameError UnboundLocalError OSError BlockingIOError ChildProcessError ConnectionError BrokenPipeError ConnectionAbortedError ConnectionRefusedError ConnectionResetError FileExistsError FileNotFoundError InterruptedError IsADirectoryError NotADirectoryError PermissionError ProcessLookupError TimeoutError ReferenceError RuntimeError NotImplementedError RecursionError SyntaxError IndentationError TabError SystemError TypeError ValueError UnicodeError UnicodeDecodeError UnicodeEncodeError UnicodeTranslateError Warning DeprecationWarning PendingDeprecationWarning RuntimeWarning SyntaxWarning UserWarning FutureWarning ImportWarning UnicodeWarning BytesWarning EncodingWarning ResourceWarning 1 2 3 4 5 6 7 8 9 10 11 ## Class Definitions ```python class NetworkError(Exception): pass class DeviceError(Exception): def __init__(self, errno, msg): self.args = (errno, msg) self.errno = errno self.errmsg = msg Context Manager 1 2 3 4 5 6 7 8 9 10 11 12 class ListTransaction : def __init__ ( self , thelist ): self . thelist = thelist def __enter__ ( self ): self . workingcopy = list ( self . thelist ) return self . workingcopy def __exit__ ( self , type , value , tb ): if type is None : self . thelist [:] = self . workingcopy return False This class allows you to make a sequence of modifications to an existing list. However, the modifications only take effect if no exceptions occur. Otherwise, the original list is left unmodified. 1 2 3 4 5 6 7 8 9 10 11 12 items = [ 1 , 2 , 3 ] with ListTransaction ( items ) as working : working . append ( 4 ) working . append ( 5 ) print ( items ) # Produces [1, 2, 3, 4, 5] try : with ListTransaction ( items ) as working : working . append ( 6 ) working . append ( 7 ) raise RuntimeError ( \"We're hosed!\" ) Python Optimized Mode If you run Python with the -o option, it will run in optimized mode, but it won't check assertions. What is Object in Python Every piece of data stored in a program is an object. Each object has an identity, a type (also known as its class), and a value. For example, when you write a = 42 , an integer object is created with the value of 42. The identity of the object is a number representing its location in memory. a is a label that refers to this specific location, although the label is not part of the object itself. The type of an object, also known as the object's class, defines the object's internal data representation as well as supported methods. When an object of a particular type is created, that object is called an instance of that type. After an instance is created, its identity does not change. If an object's value can be modified, the object is said to be mutable. If the value cannot be modified, the object is said to be immutable. An object that holds references to other objects is said to be a container. Objects are characterized by their attributes. An attribute is a value associated with an object that is accessed using the dot operator ( . ). An attribute might be a simple data value, such as a number. However, an attribute could also be a function that is invoked to carry out some operation. Such functions are called methods. The following example illustrates access to attributes: 1 obj . attribute A subtype is a type defined by inheritance. It carries all of the features of the original type plus additional and/or redefined methods. Inheritance is discussed in more detail in Chapter 7. Although type checks can be added to a program, this is often not as useful as you might imagine. For one, excessive checking impacts performance. Second, programs don't always define objects that neatly fit into a nice type hierarchy. For instance, if the purpose of the isinstance(items, list) statement above is to test whether items is \"list-like,\" it won't work with objects that have the same programming interface as a list but don't directly inherit from the built-in list type (one example is deque from the collections module). Reference Counting and Garbage Collection Python manages objects through automatic garbage collection. All objects are reference-counted. An object's reference count is increased whenever it's assigned to a new name or placed in a data structure that references it. An object's reference count is decreased by the del statement or whenever a reference goes out of scope or is reassigned. When an object's reference count reaches zero, it is garbage-collected. However, in some cases, a circular dependency may exist in a collection of objects that are no longer in use. In such cases, the destruction of the objects will be delayed until a cycle detector executes to find and delete the inaccessible objects. The exact behavior can be fine-tuned and controlled using functions in the gc standard library module. The gc.collect() function can be used to immediately invoke the cyclic garbage collector. First-Class Object All objects in Python are said to be first-class. This means that all objects that can be assigned to a name can also be treated as data. As data, objects can be stored as variables, passed as arguments, returned from functions, compared against other objects, and more. Object Protocol and Data Abstraction Unlike a compiler for a static language, Python does not verify correct program behavior in advance. Instead, the behavior of an object is determined by a dynamic process that involves the dispatch of so-called \"special\" or \"magic\" methods. The names of these special methods are always preceded and followed by double underscores ( __ ). The methods are automatically triggered by the interpreter as a program executes. For example, the operation x * y is carried out by a method x.__mul(y) . The names of these methods and their corresponding operators are hard-wired. The behavior of any given object depends entirely on the set of special methods that it implements. The next few sections describe the special methods associated with different categories of core interpreter features. These categories are sometimes called \"protocols.\" An object, including a user-defined class, may define any combination of these features to make the object behave in different ways. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ### Generate Markdown Table for Dunder | Method | Description | |-----------------------------|---------------------------------------------| | `__init__(self, *args, **kwargs)` | Initializes an instance. | | `__del__(self)` | Called when an instance is being destroyed.| | `__repr__(self)` | Creates a string representation. | | `__new__(self)` | Creates a new instance. | ### Object Management Methods | Method | Description | |------------------------|------------------------------| | `__add__(self, other)` | Adds two objects together. | | `__sub__(self, other)` | Subtracts one object from another. | | `__mul__(self, other)` | Multiplies two objects. | | `__truediv__(self, other)` | Divides one object by another. | | `__floordiv__(self, other)` | Performs floor division. | | `__mod__(self, other)` | Performs modulo operation. | | `__matmul__(self, other)` | Performs matrix multiplication. | If `__bool__()` is undefined, then `__len__()` is used as a fallback. If both `__bool__()` and `__len__()` are undefined, an object is simply considered to be True. The `__eq__()` method is used to determine basic equality for use with the `==` and `!=` operators. The default implementation of `__eq__()` compares objects by identity using the `is` operator. The `__ne__()` method, if present, can be used to implement special processing for `!=`, but is usually not required as long as `__eq__()` is implemented. Matrices, returning a matrix with the results. If comparison is not possible, the methods should return the built-in object `NotImplemented`. This is not the same as the `NotImplementedError`. It is not necessary for an ordered object to implement all of the comparison operations in Table 4.3. If you want to be able to sort objects or use functions such as `min()` or `max()`, then `__lt__()` must be minimally defined. If you are adding comparison operators to a user-defined class, the `@total_ordering` class decorator in the `functools` module may be of some use. It can generate all of the methods as long as you minimally implement `__eq__()` and one of the other comparisons. The `__hash__()` method is defined on instances that are to be placed into a set or be used as keys in a mapping (dictionary). The value returned is an integer that should be the same for two instances that compare as equal. Moreover, `__eq__()` should always be defined together with `__hash__()` because the two methods work together. The value returned by `__hash__()` is typically used as an internal implementation detail of various data structures. However, it\u2019s possible for two different objects to have the same hash value. Therefore, `__eq__()` is necessary to resolve potential collisions. Conversion Protocols - `__str__(self)`: Conversion to a string - `__bytes__(self)`: Conversion to bytes - `__format__(self, format_spec)`: Creates a formatted representation - `__bool__(self)`: bool(self) - `__int__(self)`: int(self) - `__float__(self)`: float(self) - `__complex__(self)`: __index__(self) Conversion to an integer index [self] Examples of formatting: - `f'{x:spec}'`: Calls `x.__format__('spec')` - `format(x, 'spec')`: Calls `x.__format__('spec')` - `'x is {0:spec}'.format(x)`: Calls `x.__format__('spec')` The `__index__()` method performs an integer conversion of an object when it\u2019s used in an operation that requires an integer value. This includes indexing in sequence operations. For example, if `items` is a list, performing an operation such as `items[x]` will attempt to execute `items[x.__index__()]` if `x` is... Container Protocols - `__len__(self)`: Returns length - `__getitem__(self, key)`: Returns `self[key]` - `__setitem__(self, key, value)`: Sets `self[key] = value` - `__delitem__(self, key)`: Deletes `self[key]` - `__contains__(self, obj)`: `obj in self` Here\u2019s an example: ```python a = [1, 2, 3, 4, 5, 6] len(a) # a.__len__() x = a[2] # x = a.__getitem__(2) a[1] = 7 # a.__setitem__(1, 7) del a[2] # a.__delitem__(2) 5 in a # a.__contains__(5) Slicing operations such as x = s[i:j] are also implemented using __getitem__() , __setitem__() , and __delitem__() . For slices, a special slice instance is passed as the key. This instance has attributes that describe the range of the slice being requested. For example: 1 2 3 4 a = [ 1 , 2 , 3 , 4 , 5 , 6 ] x = a [ 1 : 5 ] # x = a.__getitem__(slice(1, 5, None)) a [ 1 : 3 ] = [ 10 , 11 , 12 ] # a.__setitem__(slice(1, 3, None), [10, 11, 12]) del a [ 1 : 4 ] # a.__delitem__(slice(1, 4, None)) The slicing features of Python are more powerful than many programmers realize. For example, the following variations of extended slicing are all supported and may be useful for working with multidimensional data structures such as matrices and arrays: 1 2 3 4 5 a = m [ 0 : 100 : 10 ] # Strided slice (step=10) b = m [ 1 : 10 , 3 : 20 ] # Multidimensional slice c = m [ 0 : 100 : 10 , 50 : 75 : 5 ] # Multiple dimensions with strides m [ 0 : 5 , 5 : 10 ] = n # Extended slice assignment del m [: 10 , 15 :] # Extended slice deletion Iterator Protocol If an instance, obj , supports iteration, it provides a method, obj.iter() , that returns an iterator. An iterator iter , in turn, implements a single method, iter.next() , that returns the next object or raises StopIteration to signal the end of iteration. These methods are used by the implementation of the for statement as well as other operations that implicitly perform iteration. For example, the statement for x in s is carried out by performing these steps: 1 2 3 4 5 6 7 _iter = s . __iter__ () while True : try : x = _iter . __next__ () except StopIteration : break # Do statements in body of for loop Sample Iterator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class FRange : def __init__ ( self , start , stop , step ): self . start = start self . stop = stop self . step = step def __iter__ ( self ): x = self . start while x < self . stop : yield x x += self . step # Example use: nums = FRange ( 0.0 , 1.0 , 0.1 ) for x in nums : print ( x ) # 0.0, 0.1, 0.2, 0.3, ... Attribute Access __getattribute__(self, name) : Returns the attribute self.name __getattr__(self, name) : Returns the attribute self.name if it\u2019s not found through __getattribute__() __setattr__(self, name, value) : Sets the attribute self.name = value __delattr__(self, name) Function Protocol An object can emulate a function by providing the __call__() method. If an object, x , provides this method, it can be invoked like a function. That is, x(arg1, arg2, ...) invokes x.__call__(arg1, arg2, ...) . There are many built-in types that support function calls. For example, types implement __call__() to create new instances. Bound methods implement __call__() to pass the self argument to instance methods. Library functions such as functools.partial() also create objects that emulate functions. Context Manager Protocol The with statement allows a sequence of statements to execute under the control of an instance known as a context manager. The general syntax is as follows: 1 2 with context [ as var ]: statements A context object shown here is expected to implement the Use repr Just use repr it's good for debugging in the REPL. Docs Docstring is stored in the __doc__ attribute. The documentation string is stored in the doc attribute of the function. It\u2019s often accessed by IDEs to provide interactive help. Functions can also be annotated with type hints. For example: Passing Arguments You can pass arguments like this: 1 2 3 4 5 6 7 8 def func ( x , y , z ): ... s = ( 1 , 2 , 3 ) # Pass a sequence as arguments result = func ( * s ) # Pass a mapping as keyword arguments d = { 'x' : 1 , 'y' : 2 , 'z' : 3 } result = func ( ** d ) Tuple Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import NamedTuple class ParseResult ( NamedTuple ): name : str value : str def parse_value ( text ): ''' Split text of the form name=val into (name, val) ''' parts = text . split ( '=' , 1 ) return ParseResult ( parts [ 0 ] . strip (), parts [ 1 ] . strip ()) r = parse_value ( 'url=http://www.python.org' ) print ( r . name , r . value ) Avoid Using Global Statement It should be noted that use of the global statement is usually considered poor Python style. If you\u2019re writing code where a function needs to mutate state behind the scenes, consider using a class definition and modify state by mutating an instance or class variable instead. For example: 1 2 3 4 5 class Config : x = 42 def func (): Config . x = 13 Python allows nested function definitions. Here\u2019s an example: Inner Functions nonlocal cannot be used to refer to a global variable\u2014it must reference a local variable in an outer scope. Thus, if a function is assigning to a global, you should still use the global declaration. Use of nested functions and nonlocal declarations is not a common programming style. For example, inner functions have no outside visibility, which can complicate testing and debugging. Nevertheless, nested functions are sometimes useful for breaking recursion. Current limit: sys.getrecursionlimit() default is 1000 Set limit: sys.setrecursionlimit() Lambda Functions 1 2 3 4 5 6 x = 2 f = lambda y : x * y x = 3 g = lambda y : x * y print ( f ( 10 )) # --> prints 30 print ( g ( 10 )) # --> prints 30 This is called late binding. 1 2 3 4 x = 2 f = lambda y , x = x : x * y x = 3 g = lambda y , x = x : x * y Higher-Order Functions Python supports the concept of higher-order functions. This means that functions can be passed as arguments to other functions, placed in data structures, and returned by a function as a result. Functions are said to be first-class objects, meaning there is no difference between how you might handle a function and any other kind of data. Function as Callback with Parameters 1 2 3 4 after ( 10 , lambda : add ( 2 , 3 )) from functools import partial after ( 10 , partial ( add , 2 , 3 )) Since partials are fully evaluated, the callables created by partial() are objects that can be serialized into bytes, saved in files, and even transmitted across network connections (for example, using the pickle standard library module). This is not possible with a lambda function. Thus, in applications where functions are passed around, possibly to Python interpreters running in different processes or on different machines, you\u2019ll find partial() to be a bit more adaptable. As an aside, partial function application is closely related to a Decorators Shorthand of Decorators 1 func = decorate ( func ) 1 2 3 4 5 6 7 8 from functools import wraps def trace ( func ): @wraps ( func ) def call ( * args , ** kwargs ): print ( 'Calling' , func . __name__ ) return func ( * args , ** kwargs ) return call The @wraps() decorator copies various function metadata to the replacement function. In this case, metadata from the given function func() is copied to the returned wrapper function call() . Multiple Decorators 1 2 3 4 @decorator1 @decorator2 def func ( x ): pass The above code is equivalent to: 1 func = decorator1 ( decorator2 ( func )) Function Inspections f.__name__ : Function name f.__qualname__ : Fully qualified name (if nested) f.__module__ : Name of module in which defined f.__doc__ : Documentation string f.__annotations__ : Type hints f.__globals__ : Dictionary that is the global namespace f.__closure__ : Closure variables (if any) f.__code__ : Underlying code object Check if Two Function Parameters are the Same 1 2 3 4 5 6 7 8 import inspect def func ( x : int , y : float , debug = False ) -> float : pass sig = inspect . signature ( func ) assert inspect . signature ( func1 ) == inspect . signature ( func2 ) Attributes are not visible within the function body\u2014they are not local variables and do not appear as names in the execution environment. The main use of function attributes is to store extra metadata. Sometimes frameworks or various metaprogramming techniques utilize function tagging\u2014that is, attaching attributes to functions. One example is the @abstractmethod decorator that\u2019s used on methods within abstract base classes. 1 2 3 4 5 def func (): statements func . secure = 1 func . private = 1 Frame Attributes f.f_back : Previous stack frame (toward the caller) f.f_code : Code object being executed f.f_locals : Dictionary of local variables ( locals() ) f.f_globals : Dictionary used for global variables ( globals() ) f.f_builtins : Dictionary used for built-in names f.f_lineno : Line number f.f_lasti : Current instruction. This is an index into the bytecode string of f_code . f.f_trace : Function called at the start of each source code line 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import inspect from collections import ChainMap def debug ( * varnames ): f = inspect . currentframe () . f_back # Previous stack vars = ChainMap ( f . f_locals , f . f_globals ) print ( f ' { f . f_code . co_filename } : { f . f_lineno } ' ) for name in varnames : print ( f ' { name } = { vars [ name ] !r} ' ) # Example use def func ( x , y ): z = x + y debug ( 'x' , 'y' ) # Shows x and y along with file/line Dynamic Code Execution 1 exec ( str [, globals [, locals ]]) 1 2 3 4 5 6 7 globs = { 'x' : 7 , 'y' : 10 , 'birds' : [ 'Parrot' , 'Swallow' , 'Albatross' ] } locs = {} exec ( 'z = 3 * x + 4 * y' , globs , locs ) exec ( 'for b in birds: print(b)' , globs , locs ) 1 2 3 4 5 6 7 8 9 10 11 12 def make_init ( * names ): parms = ',' . join ( names ) code = f 'def __init__(self, { parms } ): \\n ' for name in names : code += f ' self. { name } = { name } \\n ' d = {} exec ( code , d ) return d [ '__init__' ] # Example use class Vector : __init__ = make_init ( 'x' , 'y' , 'z' ) Positional and Named Arguments 1 2 3 4 5 def func ( x , y , / ): pass func ( 1 , 2 ) # Ok func ( 1 , y = 2 ) # Error Name and Docstring __name__ __doc__ Argument Passing Everything is passed by reference, but extra care is needed only for mutable types. Pass ready parameters to functions. 1 2 3 4 5 6 7 8 9 def func ( x , y , z ): ... s = ( 1 , 2 , 3 ) # Pass a sequence as arguments result = func ( * s ) # Pass a mapping as keyword arguments d = { 'x' : 1 , 'y' : 2 , 'z' : 3 } result = func ( ** d ) NamedTuple 1 2 3 4 5 6 7 8 9 10 11 12 from typing import NamedTuple class ParseResult ( NamedTuple ): name : str value : str def parse_value ( text ): ''' Split text of the form name=val into (name, val) ''' parts = text . split ( '=' , 1 ) return ParseResult ( parts [ 0 ] . strip (), parts [ 1 ] . strip ()) Late Binding 1 2 def func (): n += 1 # Error: UnboundLocalError 1 2 3 4 5 x = 42 def func (): print ( x ) # Fails. UnboundLocalError x = 13 Async Function Use of await is only valid within an enclosing async function definition. It\u2019s also a required part of making async functions execute. If you leave off the await , you\u2019ll find that the code breaks. The requirement of using await hints at a general usage issue with asynchronous functions. Namely, their different evaluation model prevents them from being used in combination with other parts of Python. Specifically, it is never possible to write code like print(await twice(2)) \u2014at least not without an intervening await or async keyword. 1 2 3 4 5 6 async def twice ( x ): return 2 * x def main (): print ( twice ( 2 )) # Error. Doesn't execute the function print ( await twice ( 2 )) # Error. Can't use await here. yield and return 1 2 3 4 5 6 def func (): try : next ( f ) except StopIteration as e : yield 37 return 42 1 2 3 4 5 6 7 8 def countdown ( n ): print ( 'Counting down from' , n ) try : while n > 0 : yield n n = n - 1 finally : print ( 'Only made it to' , n ) Generators are guaranteed to execute the finally block code even if the generator is not fully consumed\u2014it will execute when the abandoned generator is garbage-collected. Similarly, any cleanup code involving a context manager is also guaranteed to execute. yield from 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def countup ( stop ): n = 1 while n <= stop : yield n n += 1 def countdown ( start ): n = start while n > 0 : yield n n -= 1 def up_and_down ( n ): yield from countup ( n ) yield from countdown ( n ) yield from is especially useful when writing code that must recursively iterate through nested iterables. 1 2 3 4 5 6 def flatten ( items ): for i in items : if isinstance ( i , list ): yield from flatten ( i ) else : yield i Avoiding Recursion Limit 1 2 3 4 5 6 7 8 9 10 11 def flatten ( items ): stack = [ iter ( items )] while stack : try : item = next ( stack [ - 1 ]) if isinstance ( item , list ): stack . append ( iter ( item )) else : yield item except StopIteration : stack . pop () Sending Values to Enhanced Generators (Coroutines) 1 2 3 4 5 def receiver (): print ( 'Ready to receive' ) while True : n = yield print ( 'Got' , n ) 1 2 3 4 5 r = receiver () r . send ( None ) # Advances to the first yield print ( r . send ( 1 )) print ( r . send ( 2 )) print ( r . send ( 'Hello' )) Check throw() and close() Method in Internet Enhanced Generators Enhanced generators are an odd programming construct. Unlike a simple generator which naturally feeds a for loop, there is no core language feature that drives an enhanced generator. Why, then, would you ever want a function that needs values to be sent to it? Is it purely academic? Historically, enhanced generators have been used in the context of concurrency libraries\u2014especially those based on asynchronous I/O. In that context, they\u2019re usually referred to as coroutines or generator-based coroutines . However, much of that functionality has been folded into the async and await features of Python. There is little practical reason to use yield for that specific use case. That said, there are still some practical applications. Like generators, an enhanced generator can be used to implement different kinds of evaluation and control flow. One example is the @contextmanager decorator found in the contextlib module. 1 2 3 4 5 6 7 8 class Manager : def __enter__ ( self ): return somevalue def __exit__ ( self , ty , val , tb ): if ty : # An exception occurred ... # Return True/ if handled. False otherwise With the @contextmanager generator, everything prior to the yield statement executes when the manager enters (via the enter() method). Everything after the yield executes when the manager exits (via the exit() method). If an error took place, it is reported as an exception on the yield statement. Here's a book on the internet where you can find more information about this topic. Final Words: A Brief History of Generators and Looking Forward Generators are one of Python\u2019s more interesting success stories. They are also part of a greater story concerning iteration. Iteration is one of the most common programming tasks of all. In early versions of Python, iteration was implemented via sequence indexing and the __getitem__() method. This later evolved into the current iteration protocol based on __iter__() and __next__() methods. Generators appeared shortly thereafter as a more convenient way to implement an iterator. In modern Python, there is almost no reason to ever implement an iterator using anything other than a generator. Even on iterable objects that you might define yourself, the __iter__() method itself is conveniently implemented in this way. In later versions of Python, generators took on a new role as they evolved enhanced features related to coroutines\u2014the send() and throw() methods. These were no longer limited to iteration but opened up possibilities for using generators in other contexts. Most notably, this formed the basis of many so-called async frameworks used for network programming and concurrency. However, as asynchronous programming has evolved, most of this has transformed into later features that use the async / await syntax. Thus, it\u2019s not so common to see generator functions used outside of the context of iteration\u2014their original purpose. In fact, if you find yourself defining a generator function and you\u2019re not sure why, it\u2019s worth questioning whether or not it\u2019s necessary. Function Introspection Here are some useful function introspection attributes: f.__name__ : Function name f.__qualname__ : Fully qualified name f.__module__ : Module name f.__doc__ : Docstring f.__annotations__ : Type hints f.__globals__ : Dictionary of global namespace f.__closure__ : Closure variables f.__code__ : Code object","title":"Structure"},{"location":"python/structure/structure/#memorize-some-tips","text":"","title":"Memorize Some Tips"},{"location":"python/structure/structure/#literals","text":"Literals are used to represent fixed values in Python. Here are some examples: Integer literals: 42 , 0b101010 (binary), 0o52 (octal), 0x2a (hexadecimal) Numeric literals can also include underscores for readability: 123_456_789 , 0x1234_5678 , 0b111_00_101 , 123.789_012","title":"Literals"},{"location":"python/structure/structure/#operations-for-iterables","text":"Iteration: for vars in s: Variable unpacking: v1, v2, ... = s Membership: x in s , x not in s Expansion in list, tuple, or set literals: [a, *s, b] , (a, *s, b) , {a, *s, b} Throw variable: throw variable like that Example: a,_,b=[1,2,3]","title":"Operations for Iterables"},{"location":"python/structure/structure/#set-operations","text":"Set operations allow manipulating sets in Python: 1 2 3 4 5 6 7 8 names1 = { 'IBM' , 'MSFT' , 'AA' } names2 = set ([ 'IBM' , 'MSFT' , 'HPE' , 'IBM' , 'CAT' ]) a = names1 | names2 # Union: {'IBM', 'MSFT', 'HPE', 'AA', 'CAT'} b = names1 & names2 # Intersection: {'IBM', 'MSFT'} c = names1 - names2 # Difference: {'AA'} d = names2 - names1 # Difference: {'HPE', 'CAT'} e = names1 ^ names2 # Symmetric Difference: {'HPE', 'AA', 'CAT'}","title":"Set Operations"},{"location":"python/structure/structure/#discard","text":"The discard() method is used to remove an item from a set: 1 s . discard ( 'SCOX' ) # Remove 'SCOX' if it exists.","title":"Discard()"},{"location":"python/structure/structure/#dictionary-operations","text":"Dictionaries offer various operations for manipulating key-value pairs: Get with default value: p = prices.get('IBM', 0.0) Delete: del prices['GOOG'] Keys can be tuples: prices[('IBM', '2015-02-03')] = 91.23","title":"Dictionary Operations"},{"location":"python/structure/structure/#list-comprehension","text":"List comprehension provides a concise way to create lists based on existing lists or other iterables: 1 2 3 4 [ expression for item1 in iterable1 if condition1 for item2 in iterable2 if condition2 ... for itemN in iterableN if conditionN ] This syntax is equivalent to the following code: 1 2 3 4 5 6 7 8 9 result = [] for item1 in iterable1 : if condition1 : for item2 in iterable2 : if condition2 : ... for itemN in iterableN : if conditionN : result . append ( expression )","title":"List Comprehension"},{"location":"python/structure/structure/#generator-expression","text":"Generator expressions are used to create generator objects, which generate values on the fly without storing them in memory: 1 2 3 4 5 6 7 8 9 nums = [ 1 , 2 , 3 , 4 ] squares = ( x * x for x in nums ) >>> squares < generator object at 0x590a8 > >>> next ( squares ) 1 >>> next ( squares ) 4","title":"Generator Expression"},{"location":"python/structure/structure/#python-enumerate","text":"The enumerate() function is used to iterate over a sequence while keeping track of the index: 1 2 for i , x in enumerate ( s , start = 100 ): statements","title":"Python Enumerate"},{"location":"python/structure/structure/#zip","text":"The zip() function is used to iterate over multiple sequences simultaneously: 1 2 for x , y in zip ( s , t ): statements The zip() function returns an iterable of tuples.","title":"Zip"},{"location":"python/structure/structure/#exception-base-roots","text":"BaseException : The root class for all exceptions. Exception : Base class for all program-related errors. ArithmeticError : Base class for all math-related errors. ImportError : Base class for import-related errors. LookupError : Base class for all container lookup errors. OSError : Base class for all system-related errors. IOError and EnvironmentError are aliases. ValueError : Base class for value-related errors, including Unicode-related errors. UnicodeError : Base class for Unicode string encoding-related errors. AssertionError : Raised when an assert statement fails. AttributeError : Raised when a bad attribute lookup is performed on an object. EOFError : Raised when the end of a file is reached. MemoryError : Raised when a recoverable out-of-memory error occurs. NameError : Raised when a name is not found in the local or global namespace. NotImplementedError : Raised for an unimplemented feature. RuntimeError : A generic \"something bad happened\" error. TypeError : Raised when an operation is applied to an object of the wrong type. UnboundLocalError : Raised when a local variable is used before a value is assigned. SystemExit : Raised to indicate program exit. KeyboardInterrupt : Raised when a program is interrupted via Control-C. StopIteration : Raised to signal the end of iteration.","title":"Exception Base Roots"},{"location":"python/structure/structure/#new-exception","text":"You can create your own custom exceptions by defining a new class that inherits from the Exception class. Here's an example: 1 2 3 4 class NetworkError ( Exception ): pass raise NetworkError ( 'Cannot find host' )","title":"New Exception"},{"location":"python/structure/structure/#chained-exception","text":"You can raise a different exception while preserving the original exception using the from keyword. Here's an example: 1 2 3 4 try : # Some code that may raise an exception except Exception as e : raise ValueError ( 'An error occurred' ) from e This creates a new ValueError exception with the original exception e chained to it. Exception Name Description BaseException The root class for all exceptions. Exception Base class for all program-related errors. ArithmeticError Base class for all math-related errors. ImportError Base class for import-related errors. LookupError Base class for all container lookup errors. OSError Base class for all system-related errors. ValueError Base class for value-related errors. UnicodeError Base class for Unicode string encoding errors. AssertionError Raised when an assert statement fails. AttributeError Raised when a bad attribute lookup is performed. EOFError Raised when the end of a file is reached. MemoryError Raised when a recoverable out-of-memory error occurs. NameError Raised when a name is not found in the local or global namespace. NotImplementedError Raised for an unimplemented feature. RuntimeError A generic \"something bad happened\" error. TypeError Raised when an operation is applied to an object of the wrong type. UnboundLocalError Raised when a local variable is used before a value is assigned. SystemExit Raised to indicate program exit. KeyboardInterrupt Raised when a program is interrupted via Control-C. StopIteration Raised to signal the end of iteration. 1 2 3 4 5 6 7 class ApplicationError ( Exception ): pass def do_something (): x = int ( 'N/A' ) # raises ValueError def spam (): try : do_something () except Exception as e : raise ApplicationError ( 'It failed' ) from e ## Exception handling advice","title":"Chained Exception"},{"location":"python/structure/structure/#eargs","text":"The tuple of arguments supplied when raising the exception. In most cases, this is a one-item tuple with a string describing the error. For OSError exceptions, the value is a 2-tuple or 3-tuple containing an integer error number, string error message, and an optional filename.","title":"e.args"},{"location":"python/structure/structure/#ecause","text":"Previous exception if the exception was intentionally raised in response to handling another exception. See the later section on chained exceptions.","title":"e.cause"},{"location":"python/structure/structure/#econtext","text":"Previous exception if the exception was raised while handling another exception.","title":"e.context"},{"location":"python/structure/structure/#etraceback","text":"Stack traceback object associated with the exception. 1 2 3 4 try : # do something except ( TypeError , ValueError ) as e : # Handle Type or Value errors 1 2 3 4 5 6 7 8 try : file = open ( 'foo.txt' , 'rt' ) except FileNotFoundError as e : print ( f 'Unable to open foo: { e } ' ) data = '' else : data = file . read () file . close () 1 2 3 4 5 6 file = open ( 'foo.txt' , 'rt' ) try : # Do some stuff ... finally : file . close () Exception handling is one of the most difficult things to get right in larger programs. However, there are a few rules of thumb that make it easier. The first rule is to not catch exceptions that can\u2019t be handled at that specific location in the code. Consider a function like this: 1 2 3 4 5 6 7 def read_data ( filename ): with open ( filename , 'rt' ) as file : rows = [] for line in file : row = line . split () rows . append (( row [ 0 ], int ( row [ 1 ]), float ( row [ 2 ]))) return rows Suppose the open() function fails due to a bad filename. Is this an error that should be caught with a try-except statement in this function? Probably not. If the caller gives a bad filename, there is no sensible way to recover. There is no file to open, no data to read, and nothing else that\u2019s possible. It\u2019s better to let the operation fail and report an exception back to the caller. Avoiding an error check in read_data() doesn\u2019t mean that the exception would never be handled anywhere\u2014it just means that it\u2019s not the role of read_data() to do it. Perhaps the code that prompted a user for a filename would handle this exception. This advice might seem contrary to the experience of programmers accustomed to languages that rely upon special error codes or wrapped result types. In those languages, great care is made to make sure you always check return codes for errors on all operations. You don\u2019t do this in Python. If an operation can fail and there\u2019s nothing you can do to recover, it\u2019s better to just let it fail. The exception will propagate to upper levels of the program where it is usually the responsibility of some other code to handle it. On the other hand, a function might be able to recover from bad data. For example: 1 2 3 4 5 6 7 8 9 10 11 def read_data ( filename ): with open ( filename , 'rt' ) as file : rows = [] for line in file : row = line . split () try : rows . append (( row [ 0 ], int ( row [ 1 ]), float ( row [ 2 ]))) except ValueError as e : print ( 'Bad row:' , row ) print ( 'Reason:' , e ) return rows When catching errors, try to make your except clauses as narrow as reasonable. The above code could have been written to catch all errors by using except Exception . However, doing that would make the code catch legitimate programming errors that probably shouldn\u2019t be ignored. Don\u2019t do that\u2014it will make debugging difficult. Finally, if you\u2019re explicitly raising an exception, consider making your own exception types. For example: 1 2 3 4 5 6 7 # Code Termination # exit code # can be used instead of exit() raise SystemExit () # Exit with no error message raise SystemExit ( \"Something is wrong\" ) # Exit with error","title":"e.traceback"},{"location":"python/structure/structure/#exception-hierarchy","text":"BaseException SystemExit KeyboardInterrupt GeneratorExit Exception StopIteration StopAsyncIteration ArithmeticError FloatingPointError OverflowError ZeroDivisionError AssertionError AttributeError BufferError EOFError ImportError ModuleNotFoundError LookupError IndexError KeyError MemoryError NameError UnboundLocalError OSError BlockingIOError ChildProcessError ConnectionError BrokenPipeError ConnectionAbortedError ConnectionRefusedError ConnectionResetError FileExistsError FileNotFoundError InterruptedError IsADirectoryError NotADirectoryError PermissionError ProcessLookupError TimeoutError ReferenceError RuntimeError NotImplementedError RecursionError SyntaxError IndentationError TabError SystemError TypeError ValueError UnicodeError UnicodeDecodeError UnicodeEncodeError UnicodeTranslateError Warning DeprecationWarning PendingDeprecationWarning RuntimeWarning SyntaxWarning UserWarning FutureWarning ImportWarning UnicodeWarning BytesWarning EncodingWarning ResourceWarning 1 2 3 4 5 6 7 8 9 10 11 ## Class Definitions ```python class NetworkError(Exception): pass class DeviceError(Exception): def __init__(self, errno, msg): self.args = (errno, msg) self.errno = errno self.errmsg = msg","title":"Exception Hierarchy"},{"location":"python/structure/structure/#context-manager","text":"1 2 3 4 5 6 7 8 9 10 11 12 class ListTransaction : def __init__ ( self , thelist ): self . thelist = thelist def __enter__ ( self ): self . workingcopy = list ( self . thelist ) return self . workingcopy def __exit__ ( self , type , value , tb ): if type is None : self . thelist [:] = self . workingcopy return False This class allows you to make a sequence of modifications to an existing list. However, the modifications only take effect if no exceptions occur. Otherwise, the original list is left unmodified. 1 2 3 4 5 6 7 8 9 10 11 12 items = [ 1 , 2 , 3 ] with ListTransaction ( items ) as working : working . append ( 4 ) working . append ( 5 ) print ( items ) # Produces [1, 2, 3, 4, 5] try : with ListTransaction ( items ) as working : working . append ( 6 ) working . append ( 7 ) raise RuntimeError ( \"We're hosed!\" )","title":"Context Manager"},{"location":"python/structure/structure/#python-optimized-mode","text":"If you run Python with the -o option, it will run in optimized mode, but it won't check assertions.","title":"Python Optimized Mode"},{"location":"python/structure/structure/#what-is-object-in-python","text":"Every piece of data stored in a program is an object. Each object has an identity, a type (also known as its class), and a value. For example, when you write a = 42 , an integer object is created with the value of 42. The identity of the object is a number representing its location in memory. a is a label that refers to this specific location, although the label is not part of the object itself. The type of an object, also known as the object's class, defines the object's internal data representation as well as supported methods. When an object of a particular type is created, that object is called an instance of that type. After an instance is created, its identity does not change. If an object's value can be modified, the object is said to be mutable. If the value cannot be modified, the object is said to be immutable. An object that holds references to other objects is said to be a container. Objects are characterized by their attributes. An attribute is a value associated with an object that is accessed using the dot operator ( . ). An attribute might be a simple data value, such as a number. However, an attribute could also be a function that is invoked to carry out some operation. Such functions are called methods. The following example illustrates access to attributes: 1 obj . attribute A subtype is a type defined by inheritance. It carries all of the features of the original type plus additional and/or redefined methods. Inheritance is discussed in more detail in Chapter 7. Although type checks can be added to a program, this is often not as useful as you might imagine. For one, excessive checking impacts performance. Second, programs don't always define objects that neatly fit into a nice type hierarchy. For instance, if the purpose of the isinstance(items, list) statement above is to test whether items is \"list-like,\" it won't work with objects that have the same programming interface as a list but don't directly inherit from the built-in list type (one example is deque from the collections module).","title":"What is Object in Python"},{"location":"python/structure/structure/#reference-counting-and-garbage-collection","text":"Python manages objects through automatic garbage collection. All objects are reference-counted. An object's reference count is increased whenever it's assigned to a new name or placed in a data structure that references it. An object's reference count is decreased by the del statement or whenever a reference goes out of scope or is reassigned. When an object's reference count reaches zero, it is garbage-collected. However, in some cases, a circular dependency may exist in a collection of objects that are no longer in use. In such cases, the destruction of the objects will be delayed until a cycle detector executes to find and delete the inaccessible objects. The exact behavior can be fine-tuned and controlled using functions in the gc standard library module. The gc.collect() function can be used to immediately invoke the cyclic garbage collector.","title":"Reference Counting and Garbage Collection"},{"location":"python/structure/structure/#first-class-object","text":"All objects in Python are said to be first-class. This means that all objects that can be assigned to a name can also be treated as data. As data, objects can be stored as variables, passed as arguments, returned from functions, compared against other objects, and more.","title":"First-Class Object"},{"location":"python/structure/structure/#object-protocol-and-data-abstraction","text":"Unlike a compiler for a static language, Python does not verify correct program behavior in advance. Instead, the behavior of an object is determined by a dynamic process that involves the dispatch of so-called \"special\" or \"magic\" methods. The names of these special methods are always preceded and followed by double underscores ( __ ). The methods are automatically triggered by the interpreter as a program executes. For example, the operation x * y is carried out by a method x.__mul(y) . The names of these methods and their corresponding operators are hard-wired. The behavior of any given object depends entirely on the set of special methods that it implements. The next few sections describe the special methods associated with different categories of core interpreter features. These categories are sometimes called \"protocols.\" An object, including a user-defined class, may define any combination of these features to make the object behave in different ways. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ### Generate Markdown Table for Dunder | Method | Description | |-----------------------------|---------------------------------------------| | `__init__(self, *args, **kwargs)` | Initializes an instance. | | `__del__(self)` | Called when an instance is being destroyed.| | `__repr__(self)` | Creates a string representation. | | `__new__(self)` | Creates a new instance. | ### Object Management Methods | Method | Description | |------------------------|------------------------------| | `__add__(self, other)` | Adds two objects together. | | `__sub__(self, other)` | Subtracts one object from another. | | `__mul__(self, other)` | Multiplies two objects. | | `__truediv__(self, other)` | Divides one object by another. | | `__floordiv__(self, other)` | Performs floor division. | | `__mod__(self, other)` | Performs modulo operation. | | `__matmul__(self, other)` | Performs matrix multiplication. | If `__bool__()` is undefined, then `__len__()` is used as a fallback. If both `__bool__()` and `__len__()` are undefined, an object is simply considered to be True. The `__eq__()` method is used to determine basic equality for use with the `==` and `!=` operators. The default implementation of `__eq__()` compares objects by identity using the `is` operator. The `__ne__()` method, if present, can be used to implement special processing for `!=`, but is usually not required as long as `__eq__()` is implemented. Matrices, returning a matrix with the results. If comparison is not possible, the methods should return the built-in object `NotImplemented`. This is not the same as the `NotImplementedError`. It is not necessary for an ordered object to implement all of the comparison operations in Table 4.3. If you want to be able to sort objects or use functions such as `min()` or `max()`, then `__lt__()` must be minimally defined. If you are adding comparison operators to a user-defined class, the `@total_ordering` class decorator in the `functools` module may be of some use. It can generate all of the methods as long as you minimally implement `__eq__()` and one of the other comparisons. The `__hash__()` method is defined on instances that are to be placed into a set or be used as keys in a mapping (dictionary). The value returned is an integer that should be the same for two instances that compare as equal. Moreover, `__eq__()` should always be defined together with `__hash__()` because the two methods work together. The value returned by `__hash__()` is typically used as an internal implementation detail of various data structures. However, it\u2019s possible for two different objects to have the same hash value. Therefore, `__eq__()` is necessary to resolve potential collisions. Conversion Protocols - `__str__(self)`: Conversion to a string - `__bytes__(self)`: Conversion to bytes - `__format__(self, format_spec)`: Creates a formatted representation - `__bool__(self)`: bool(self) - `__int__(self)`: int(self) - `__float__(self)`: float(self) - `__complex__(self)`: __index__(self) Conversion to an integer index [self] Examples of formatting: - `f'{x:spec}'`: Calls `x.__format__('spec')` - `format(x, 'spec')`: Calls `x.__format__('spec')` - `'x is {0:spec}'.format(x)`: Calls `x.__format__('spec')` The `__index__()` method performs an integer conversion of an object when it\u2019s used in an operation that requires an integer value. This includes indexing in sequence operations. For example, if `items` is a list, performing an operation such as `items[x]` will attempt to execute `items[x.__index__()]` if `x` is... Container Protocols - `__len__(self)`: Returns length - `__getitem__(self, key)`: Returns `self[key]` - `__setitem__(self, key, value)`: Sets `self[key] = value` - `__delitem__(self, key)`: Deletes `self[key]` - `__contains__(self, obj)`: `obj in self` Here\u2019s an example: ```python a = [1, 2, 3, 4, 5, 6] len(a) # a.__len__() x = a[2] # x = a.__getitem__(2) a[1] = 7 # a.__setitem__(1, 7) del a[2] # a.__delitem__(2) 5 in a # a.__contains__(5) Slicing operations such as x = s[i:j] are also implemented using __getitem__() , __setitem__() , and __delitem__() . For slices, a special slice instance is passed as the key. This instance has attributes that describe the range of the slice being requested. For example: 1 2 3 4 a = [ 1 , 2 , 3 , 4 , 5 , 6 ] x = a [ 1 : 5 ] # x = a.__getitem__(slice(1, 5, None)) a [ 1 : 3 ] = [ 10 , 11 , 12 ] # a.__setitem__(slice(1, 3, None), [10, 11, 12]) del a [ 1 : 4 ] # a.__delitem__(slice(1, 4, None)) The slicing features of Python are more powerful than many programmers realize. For example, the following variations of extended slicing are all supported and may be useful for working with multidimensional data structures such as matrices and arrays: 1 2 3 4 5 a = m [ 0 : 100 : 10 ] # Strided slice (step=10) b = m [ 1 : 10 , 3 : 20 ] # Multidimensional slice c = m [ 0 : 100 : 10 , 50 : 75 : 5 ] # Multiple dimensions with strides m [ 0 : 5 , 5 : 10 ] = n # Extended slice assignment del m [: 10 , 15 :] # Extended slice deletion","title":"Object Protocol and Data Abstraction"},{"location":"python/structure/structure/#iterator-protocol","text":"If an instance, obj , supports iteration, it provides a method, obj.iter() , that returns an iterator. An iterator iter , in turn, implements a single method, iter.next() , that returns the next object or raises StopIteration to signal the end of iteration. These methods are used by the implementation of the for statement as well as other operations that implicitly perform iteration. For example, the statement for x in s is carried out by performing these steps: 1 2 3 4 5 6 7 _iter = s . __iter__ () while True : try : x = _iter . __next__ () except StopIteration : break # Do statements in body of for loop Sample Iterator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class FRange : def __init__ ( self , start , stop , step ): self . start = start self . stop = stop self . step = step def __iter__ ( self ): x = self . start while x < self . stop : yield x x += self . step # Example use: nums = FRange ( 0.0 , 1.0 , 0.1 ) for x in nums : print ( x ) # 0.0, 0.1, 0.2, 0.3, ...","title":"Iterator Protocol"},{"location":"python/structure/structure/#attribute-access","text":"__getattribute__(self, name) : Returns the attribute self.name __getattr__(self, name) : Returns the attribute self.name if it\u2019s not found through __getattribute__() __setattr__(self, name, value) : Sets the attribute self.name = value __delattr__(self, name)","title":"Attribute Access"},{"location":"python/structure/structure/#function-protocol","text":"An object can emulate a function by providing the __call__() method. If an object, x , provides this method, it can be invoked like a function. That is, x(arg1, arg2, ...) invokes x.__call__(arg1, arg2, ...) . There are many built-in types that support function calls. For example, types implement __call__() to create new instances. Bound methods implement __call__() to pass the self argument to instance methods. Library functions such as functools.partial() also create objects that emulate functions.","title":"Function Protocol"},{"location":"python/structure/structure/#context-manager-protocol","text":"The with statement allows a sequence of statements to execute under the control of an instance known as a context manager. The general syntax is as follows: 1 2 with context [ as var ]: statements A context object shown here is expected to implement the","title":"Context Manager Protocol"},{"location":"python/structure/structure/#use-repr","text":"Just use repr it's good for debugging in the REPL.","title":"Use repr"},{"location":"python/structure/structure/#docs","text":"Docstring is stored in the __doc__ attribute. The documentation string is stored in the doc attribute of the function. It\u2019s often accessed by IDEs to provide interactive help. Functions can also be annotated with type hints. For example:","title":"Docs"},{"location":"python/structure/structure/#passing-arguments","text":"You can pass arguments like this: 1 2 3 4 5 6 7 8 def func ( x , y , z ): ... s = ( 1 , 2 , 3 ) # Pass a sequence as arguments result = func ( * s ) # Pass a mapping as keyword arguments d = { 'x' : 1 , 'y' : 2 , 'z' : 3 } result = func ( ** d )","title":"Passing Arguments"},{"location":"python/structure/structure/#tuple-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import NamedTuple class ParseResult ( NamedTuple ): name : str value : str def parse_value ( text ): ''' Split text of the form name=val into (name, val) ''' parts = text . split ( '=' , 1 ) return ParseResult ( parts [ 0 ] . strip (), parts [ 1 ] . strip ()) r = parse_value ( 'url=http://www.python.org' ) print ( r . name , r . value )","title":"Tuple Example"},{"location":"python/structure/structure/#avoid-using-global-statement","text":"It should be noted that use of the global statement is usually considered poor Python style. If you\u2019re writing code where a function needs to mutate state behind the scenes, consider using a class definition and modify state by mutating an instance or class variable instead. For example: 1 2 3 4 5 class Config : x = 42 def func (): Config . x = 13 Python allows nested function definitions. Here\u2019s an example:","title":"Avoid Using Global Statement"},{"location":"python/structure/structure/#inner-functions","text":"nonlocal cannot be used to refer to a global variable\u2014it must reference a local variable in an outer scope. Thus, if a function is assigning to a global, you should still use the global declaration. Use of nested functions and nonlocal declarations is not a common programming style. For example, inner functions have no outside visibility, which can complicate testing and debugging. Nevertheless, nested functions are sometimes useful for breaking recursion. Current limit: sys.getrecursionlimit() default is 1000 Set limit: sys.setrecursionlimit()","title":"Inner Functions"},{"location":"python/structure/structure/#lambda-functions","text":"1 2 3 4 5 6 x = 2 f = lambda y : x * y x = 3 g = lambda y : x * y print ( f ( 10 )) # --> prints 30 print ( g ( 10 )) # --> prints 30 This is called late binding. 1 2 3 4 x = 2 f = lambda y , x = x : x * y x = 3 g = lambda y , x = x : x * y","title":"Lambda Functions"},{"location":"python/structure/structure/#higher-order-functions","text":"Python supports the concept of higher-order functions. This means that functions can be passed as arguments to other functions, placed in data structures, and returned by a function as a result. Functions are said to be first-class objects, meaning there is no difference between how you might handle a function and any other kind of data.","title":"Higher-Order Functions"},{"location":"python/structure/structure/#function-as-callback-with-parameters","text":"1 2 3 4 after ( 10 , lambda : add ( 2 , 3 )) from functools import partial after ( 10 , partial ( add , 2 , 3 )) Since partials are fully evaluated, the callables created by partial() are objects that can be serialized into bytes, saved in files, and even transmitted across network connections (for example, using the pickle standard library module). This is not possible with a lambda function. Thus, in applications where functions are passed around, possibly to Python interpreters running in different processes or on different machines, you\u2019ll find partial() to be a bit more adaptable. As an aside, partial function application is closely related to a","title":"Function as Callback with Parameters"},{"location":"python/structure/structure/#decorators","text":"Shorthand of Decorators 1 func = decorate ( func ) 1 2 3 4 5 6 7 8 from functools import wraps def trace ( func ): @wraps ( func ) def call ( * args , ** kwargs ): print ( 'Calling' , func . __name__ ) return func ( * args , ** kwargs ) return call The @wraps() decorator copies various function metadata to the replacement function. In this case, metadata from the given function func() is copied to the returned wrapper function call() . Multiple Decorators 1 2 3 4 @decorator1 @decorator2 def func ( x ): pass The above code is equivalent to: 1 func = decorator1 ( decorator2 ( func ))","title":"Decorators"},{"location":"python/structure/structure/#function-inspections","text":"f.__name__ : Function name f.__qualname__ : Fully qualified name (if nested) f.__module__ : Name of module in which defined f.__doc__ : Documentation string f.__annotations__ : Type hints f.__globals__ : Dictionary that is the global namespace f.__closure__ : Closure variables (if any) f.__code__ : Underlying code object","title":"Function Inspections"},{"location":"python/structure/structure/#check-if-two-function-parameters-are-the-same","text":"1 2 3 4 5 6 7 8 import inspect def func ( x : int , y : float , debug = False ) -> float : pass sig = inspect . signature ( func ) assert inspect . signature ( func1 ) == inspect . signature ( func2 ) Attributes are not visible within the function body\u2014they are not local variables and do not appear as names in the execution environment. The main use of function attributes is to store extra metadata. Sometimes frameworks or various metaprogramming techniques utilize function tagging\u2014that is, attaching attributes to functions. One example is the @abstractmethod decorator that\u2019s used on methods within abstract base classes. 1 2 3 4 5 def func (): statements func . secure = 1 func . private = 1","title":"Check if Two Function Parameters are the Same"},{"location":"python/structure/structure/#frame-attributes","text":"f.f_back : Previous stack frame (toward the caller) f.f_code : Code object being executed f.f_locals : Dictionary of local variables ( locals() ) f.f_globals : Dictionary used for global variables ( globals() ) f.f_builtins : Dictionary used for built-in names f.f_lineno : Line number f.f_lasti : Current instruction. This is an index into the bytecode string of f_code . f.f_trace : Function called at the start of each source code line 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import inspect from collections import ChainMap def debug ( * varnames ): f = inspect . currentframe () . f_back # Previous stack vars = ChainMap ( f . f_locals , f . f_globals ) print ( f ' { f . f_code . co_filename } : { f . f_lineno } ' ) for name in varnames : print ( f ' { name } = { vars [ name ] !r} ' ) # Example use def func ( x , y ): z = x + y debug ( 'x' , 'y' ) # Shows x and y along with file/line","title":"Frame Attributes"},{"location":"python/structure/structure/#dynamic-code-execution","text":"1 exec ( str [, globals [, locals ]]) 1 2 3 4 5 6 7 globs = { 'x' : 7 , 'y' : 10 , 'birds' : [ 'Parrot' , 'Swallow' , 'Albatross' ] } locs = {} exec ( 'z = 3 * x + 4 * y' , globs , locs ) exec ( 'for b in birds: print(b)' , globs , locs ) 1 2 3 4 5 6 7 8 9 10 11 12 def make_init ( * names ): parms = ',' . join ( names ) code = f 'def __init__(self, { parms } ): \\n ' for name in names : code += f ' self. { name } = { name } \\n ' d = {} exec ( code , d ) return d [ '__init__' ] # Example use class Vector : __init__ = make_init ( 'x' , 'y' , 'z' )","title":"Dynamic Code Execution"},{"location":"python/structure/structure/#positional-and-named-arguments","text":"1 2 3 4 5 def func ( x , y , / ): pass func ( 1 , 2 ) # Ok func ( 1 , y = 2 ) # Error","title":"Positional and Named Arguments"},{"location":"python/structure/structure/#name-and-docstring","text":"__name__ __doc__","title":"Name and Docstring"},{"location":"python/structure/structure/#argument-passing","text":"Everything is passed by reference, but extra care is needed only for mutable types. Pass ready parameters to functions. 1 2 3 4 5 6 7 8 9 def func ( x , y , z ): ... s = ( 1 , 2 , 3 ) # Pass a sequence as arguments result = func ( * s ) # Pass a mapping as keyword arguments d = { 'x' : 1 , 'y' : 2 , 'z' : 3 } result = func ( ** d )","title":"Argument Passing"},{"location":"python/structure/structure/#namedtuple","text":"1 2 3 4 5 6 7 8 9 10 11 12 from typing import NamedTuple class ParseResult ( NamedTuple ): name : str value : str def parse_value ( text ): ''' Split text of the form name=val into (name, val) ''' parts = text . split ( '=' , 1 ) return ParseResult ( parts [ 0 ] . strip (), parts [ 1 ] . strip ())","title":"NamedTuple"},{"location":"python/structure/structure/#late-binding","text":"1 2 def func (): n += 1 # Error: UnboundLocalError 1 2 3 4 5 x = 42 def func (): print ( x ) # Fails. UnboundLocalError x = 13","title":"Late Binding"},{"location":"python/structure/structure/#async-function","text":"Use of await is only valid within an enclosing async function definition. It\u2019s also a required part of making async functions execute. If you leave off the await , you\u2019ll find that the code breaks. The requirement of using await hints at a general usage issue with asynchronous functions. Namely, their different evaluation model prevents them from being used in combination with other parts of Python. Specifically, it is never possible to write code like print(await twice(2)) \u2014at least not without an intervening await or async keyword. 1 2 3 4 5 6 async def twice ( x ): return 2 * x def main (): print ( twice ( 2 )) # Error. Doesn't execute the function print ( await twice ( 2 )) # Error. Can't use await here.","title":"Async Function"},{"location":"python/structure/structure/#yield-and-return","text":"1 2 3 4 5 6 def func (): try : next ( f ) except StopIteration as e : yield 37 return 42 1 2 3 4 5 6 7 8 def countdown ( n ): print ( 'Counting down from' , n ) try : while n > 0 : yield n n = n - 1 finally : print ( 'Only made it to' , n ) Generators are guaranteed to execute the finally block code even if the generator is not fully consumed\u2014it will execute when the abandoned generator is garbage-collected. Similarly, any cleanup code involving a context manager is also guaranteed to execute.","title":"yield and return"},{"location":"python/structure/structure/#yield-from","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def countup ( stop ): n = 1 while n <= stop : yield n n += 1 def countdown ( start ): n = start while n > 0 : yield n n -= 1 def up_and_down ( n ): yield from countup ( n ) yield from countdown ( n ) yield from is especially useful when writing code that must recursively iterate through nested iterables. 1 2 3 4 5 6 def flatten ( items ): for i in items : if isinstance ( i , list ): yield from flatten ( i ) else : yield i","title":"yield from"},{"location":"python/structure/structure/#avoiding-recursion-limit","text":"1 2 3 4 5 6 7 8 9 10 11 def flatten ( items ): stack = [ iter ( items )] while stack : try : item = next ( stack [ - 1 ]) if isinstance ( item , list ): stack . append ( iter ( item )) else : yield item except StopIteration : stack . pop ()","title":"Avoiding Recursion Limit"},{"location":"python/structure/structure/#sending-values-to-enhanced-generators-coroutines","text":"1 2 3 4 5 def receiver (): print ( 'Ready to receive' ) while True : n = yield print ( 'Got' , n ) 1 2 3 4 5 r = receiver () r . send ( None ) # Advances to the first yield print ( r . send ( 1 )) print ( r . send ( 2 )) print ( r . send ( 'Hello' ))","title":"Sending Values to Enhanced Generators (Coroutines)"},{"location":"python/structure/structure/#check-throw-and-close-method-in-internet","text":"","title":"Check throw() and close() Method in Internet"},{"location":"python/structure/structure/#enhanced-generators","text":"Enhanced generators are an odd programming construct. Unlike a simple generator which naturally feeds a for loop, there is no core language feature that drives an enhanced generator. Why, then, would you ever want a function that needs values to be sent to it? Is it purely academic? Historically, enhanced generators have been used in the context of concurrency libraries\u2014especially those based on asynchronous I/O. In that context, they\u2019re usually referred to as coroutines or generator-based coroutines . However, much of that functionality has been folded into the async and await features of Python. There is little practical reason to use yield for that specific use case. That said, there are still some practical applications. Like generators, an enhanced generator can be used to implement different kinds of evaluation and control flow. One example is the @contextmanager decorator found in the contextlib module. 1 2 3 4 5 6 7 8 class Manager : def __enter__ ( self ): return somevalue def __exit__ ( self , ty , val , tb ): if ty : # An exception occurred ... # Return True/ if handled. False otherwise With the @contextmanager generator, everything prior to the yield statement executes when the manager enters (via the enter() method). Everything after the yield executes when the manager exits (via the exit() method). If an error took place, it is reported as an exception on the yield statement. Here's a book on the internet where you can find more information about this topic.","title":"Enhanced Generators"},{"location":"python/structure/structure/#final-words-a-brief-history-of-generators-and-looking-forward","text":"Generators are one of Python\u2019s more interesting success stories. They are also part of a greater story concerning iteration. Iteration is one of the most common programming tasks of all. In early versions of Python, iteration was implemented via sequence indexing and the __getitem__() method. This later evolved into the current iteration protocol based on __iter__() and __next__() methods. Generators appeared shortly thereafter as a more convenient way to implement an iterator. In modern Python, there is almost no reason to ever implement an iterator using anything other than a generator. Even on iterable objects that you might define yourself, the __iter__() method itself is conveniently implemented in this way. In later versions of Python, generators took on a new role as they evolved enhanced features related to coroutines\u2014the send() and throw() methods. These were no longer limited to iteration but opened up possibilities for using generators in other contexts. Most notably, this formed the basis of many so-called async frameworks used for network programming and concurrency. However, as asynchronous programming has evolved, most of this has transformed into later features that use the async / await syntax. Thus, it\u2019s not so common to see generator functions used outside of the context of iteration\u2014their original purpose. In fact, if you find yourself defining a generator function and you\u2019re not sure why, it\u2019s worth questioning whether or not it\u2019s necessary.","title":"Final Words: A Brief History of Generators and Looking Forward"},{"location":"python/structure/structure/#function-introspection","text":"Here are some useful function introspection attributes: f.__name__ : Function name f.__qualname__ : Fully qualified name f.__module__ : Module name f.__doc__ : Docstring f.__annotations__ : Type hints f.__globals__ : Dictionary of global namespace f.__closure__ : Closure variables f.__code__ : Code object","title":"Function Introspection"}]}